
## Job type: db_from_s3

The **db_from_s3** job type loads data from AWS S3 into a target database. The
data formats supported, and the loading mechanism used, are dependent on the
target database type. For example, Redshift databases use the `COPY` command to
load structured data from S3.

The target database type, and hence the loading mechanism, are determined by
the type of database connector specified for the job.

The **db_from_s3** job type can be used in conjunction with the lava
**s3trigger** Lambda function to load data whenever a new data file is
placed in S3.

The following target database types are supported:

*   [AWS Redshift](#loading-data-to-redshift)

*   [Postgres RDS and Aurora Postgres](#loading-data-to-postgres-rds).

*   [Postgres](#loading-data-to-postgres)

*   [Aurora MySQL](#loading-data-to-aurora-mysql)

*   [SQLite3](#loading-data-to-sqlite3)

*   [Oracle](#loading-data-to-oracle)

### Payload

The payload is ignored.

### Parameters

|Parameter|Type|Required|Description|
|-|-|-|-------------------------------------------------------------|
|args|\*|\*|Dependent on the target database type.|
|bucket|String|Yes|The name of the S3 bucket from which the data is copied.|
|columns|List[String]|Sometimes|A list of SQL column specifications. Required if the target table must be created (e.g. if the table does not exist or the `mode` is `drop`). If specified, the copy process will explicitly include the columns specified. This is useful when only some of the columns in the target table are to be loaded with data.|
|db\_conn\_id|String|Yes|The [connection ID](#database-connectors) for a target database. The `type` associated with the connection specification will determine which loader is used.|
|jinja|Boolean|No|If `false`, disable Jinja rendering of the `bucket` and `key`. Default `true`.|
|key|String|Yes|The key in the S3 bucket from which the data is copied.|
|min_size|String \| Int|No|The loading process will *attempt* to avoid loading files below the specified size. The value is a *size* value as described in [Lava Worker Configuration](#lava-worker-configuration). The default is 0. See [below](#about-the-min_size-parameter) for more information.|
|mode|String|Yes|Update mode for the target table. See [Copy Modes](#copy-modes) for more information.|
|s3\_conn\_id|String|Conditional|The [connection ID](#connector-type-aws) for AWS S3. This is used to allow access to the source data in S3. Whether or not this is required is database dependent. Only one of either `s3_conn_id` or `s3_iam_role` is ever required.|
|s3\_iam\_role|String|Conditional|The IAM role name used to allow access to the source data in S3. Whether or not this is required is database dependent. Only one of either `s3_conn_id` or `s3_iam_role` is ever required.|
|schema|String|Yes|Target schema name. If the Jinja rendered value starts with `/`, the rest of the value is used as a [regular expression](#deriving-target-schema-and-table-name-from-the-s3-key) (regex) to derive the schema name from the Jinja rendered `key` value. In this case, the `columns` argument is not permitted and the target table must already exist.|
|skip_missing|Boolean|No|If `true`, no attempt is made to load missing files. The default is `false` which means attempting to load a non-existent file will result in an error.  This won't help in the situation where the file is removed from S3 between the check for existence and the database load operation. Note that the lava worker will require `s3:GetObject` access to the object if the value is set to `true`.|
|table|String|Yes|Target table name. If the Jinja rendered value starts with `/`, the rest of the value is used as a [regular expression](#deriving-target-schema-and-table-name-from-the-s3-key) (regex) to derive the table name from the `key` argument. In this case, the `columns` argument is not permitted and the target table must already exist.|
|vars|Map[String,\*]|No|A map of variables injected when `bucket`, `key`, `schema` and `table` are Jinja rendered.|

#### About the min_size Parameter

If the `min_size` parameter is specified, the loading process will *attempt* to
avoid loading files below the specified size. This is not always possible,
depending on the loading mechanism being used.

The fundamental use case for this parameter is to avoid attempting to load empty
data files. Most of the database loading mechanisms will happily *load* an empty
file and consider that to be a success. Some, such as the native S3 loading
process for Aurora Postgres, consider this to be an error and will abort the
transaction. This is particularly problematic when loading a number of files in
one operation, some of which may be empty. This can happen with data unloaded
from Redshift where the source table is skewed and some slice files are empty as
a result.

The file size is checked directly in S3 without downloading the object to the
lava worker. This means that for compressed files, the value applies to the file
before decompression. It also means that the lava worker must have read access
(`s3:GetObject`) to objects to obtain size information, even if the loading
process is performed directly from the database.

Unfortunately, an empty file will GZIP to a non-empty file of small but
uncertain length due to the embedding of the filename. To avoid empty GZIP
files, the value should be set to a small but non-zero value (e.g. 100).

For uncompressed data without headers, setting the value to `1` should avoid
empty files.

### Jinja Rendering of the Bucket, Key, Schema and Table

The `bucket`, `key`, `schema` and `table` parameters are rendered using
[Jinja](http://jinja.pocoo.org) prior to further use. The rendering is done in
the following order:

1.  The `bucket` and `key` parameters are rendered.
2.  The `schema` and `table` parameters are rendered.

Refer to [Jinja Rendering in Lava](#jinja-rendering-in-lava)
for more information.

The following variables are made available to the renderer.

|Name|Type|Description|
|-|-|-------------------------------------------------------------|
|globals|dict[str,\*]|The `globals` from the job specification updated with any globals received in the job dispatch.|
|job|dict[str,\*]|The [augmented job specification](#the-augmented-job-specification).|
|realm|dict[str,\*]|The realm specification.|
|start|datetime|The local time when the job run started.|
|state|dict[str,\*]|A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the [state](#the-state-table) table.|
|ustart|datetime|The UTC time when the job run started.|
|utils|dict[str,runnable]|A dictionary of [utility functions](#jinja-utility-functions) that can be used in the Jinja markup.|
|vars|dict[str,\*]|A dictionary of variables provided as the `vars` component of the job `parameters`.|

The following additional variables are made available when rendering the
`schema` and `table` parameters:

|Name|Type|Description|
|-|-|-------------------------------------------------------------|
|bucket|str|The Jinja rendered value of the `bucket` parameter.|
|key|str|The Jinja rendered value of the `key` parameter.|

This two-step process provides an additional mechanism for deriving the target
database object name from the S3 object name. It complements the regex based
mechanism described below.

!!! question "Why two mechanisms for deriving schema and table from the S3 key?"
    Well, the original regex mechanism was a legacy of previous code and is
    still supported for historical reasons. The newer Jinja based method is more
    flexible and consistent with lava generally. Try not to use them both at
    once. It probably won't end well.

### Deriving Target Schema and Table Name from the S3 Key

The regex options for `schema` and `table` provide an alternate mechanism to
allow the target table to be derived at run time from the S3 object name. This
allows a single job specification to be used for multiple tables provided an
appropriate S3 naming convention is used. This is particularly handy if the job
is dispatched by an [S3 object created
event](#dispatching-jobs-from-s3-events). In this configuration,
the `bucket` and `key` parameters would be supplied when the job is dispatched
as a result of an S3 bucket notification event.

For each of these arguments, if the value starts with `/`, the rest of the value
must be a regular expression with a single capture group. The regex is applied
to the S3 object key that triggered the event (minus any suffix) and the value
of the capture group is used as the schema or table name respectively. If the
regex option is used, the `columns` argument is not permitted and the target
schema and table must already exist.

By combining these mechanisms, it is possible to use a single job specification
to load any number of different tables into a given target database provided
the target schema and table name can be derived from the S3 object key using
regular expressions.

### Copy Modes

The `mode` argument  specifies how any existing data in the table should be
handled and how new data should be loaded.

|Mode|Description|
|-|-------------------------------------------------------------|
|abort|If the target table contains any data, then abort with an error. This prevents the loss of any existing data.|
|append|New data is appended to any existing data in the table.|
|drop|The table is dropped and recreated prior to loading. This requires the `columns` argument to be provided so lava knows how to create the table.|
|delete|Delete all existing data from the table.|
|switch|Perform A/B table switching. If the `table` argument specifies a target table of `mytable`, data will be alternately loaded into switch tables `mytable_a` and `mytable_b`. During the load phase, lava will find whichever switch table is empty and load the new data into it. It will then delete all data from the other table. All this is done inside a transaction. Once the transaction is committed, the empty table is truncated to minimise vacuuming requirements. If neither switch table is empty, the load is aborted. Generally, there will be a view on top of the switch tables presenting the union of the two.|
|truncate|Truncate the table before loading.|

### Event Driven Loading

A common usage pattern is to trigger a database load when a data file is created
in S3. Lava supports this via the [s3trigger lambda
function](#dispatching-jobs-from-s3-events).

To load a table in response to an [S3 bucket notification
event](#dispatching-jobs-from-s3-events) would require an entry
in the [s3triggers table](#the-s3triggers-table) like so:

```json
{
  "description": "A new custard club membership file appeared in S3",
  "bucket": "my-bucket",
  "prefix": "a/b",
  "enabled": true,
  "job_id": "demo/copy-mytable",
  "parameters": {
    "bucket": "{{bucket}}",
    "key": "{{key}}"
   }
}
```

The `bucket` and `key` parameters are then determined at run-time and merged
into the job specification.

When a new data file is placed in S3, the lava **s3trigger** Lambda function
will be invoked. It will look up the event source (bucket + object key) in the
[s3triggers table](#the-s3triggers-table) to determine that it
should invoke the `demo/copy-mytable` job. It will use Jinja to render the
parameters shown above from the bucket event notification details and then
dispatch the job with the customised `bucket` and `key` parameters replacing any
value in the job specification.

### Dev Mode Behaviour

Normally, the **db_from_s3** job will copy a log of events to S3 on the
conclusion of the job. In dev mode, this log is emitted locally at the end of
the job run.

### Moving Data from Redshift to Other Databases

Unfortunately, there is little consistency of data interchange formats
or methods across the AWS database services.

For example:

*   Data unloaded from Redshift cannot be reliably copied back to Redshift
    without special handling of date fields.

*   Data unloaded from Redshift as pipe separated values (the default), cannot
    be reliably loaded to Postgres, Postgres RDS or Aurora Postgres because of
    differences in escaping the pipe character. CSV format seems to work.

*   Redshift unloading works best with compression and parallel unloading
    enabled. Aurora MySQL supports a manifest but does not support loading of
    compressed data. Postgres RDS and Aurora Postgres don't support loading from
    a manifest. They do support loading compressed data but only if the content
    encoding is set correctly on the S3 objects. Redshift does not do this.

*   The syntax and capability of data loading commands varies widely from
    Redshift to Aurora MySQL to Aurora/RDS Postgres to standard Postgres.

Frankly, it's a mess.

Lava can only compensate for these problems to a limited degree. For example,
the [redshift_unload](#job-type-redshift_unload) job has
additional logic to work around the first problem above. The
[db_from_s3](#job-type-db_from_s3) job uses a range of
background trickery (such as manifest handling) to partially compensate for some
of the other limitations and inconsistencies.

Be careful out there!
