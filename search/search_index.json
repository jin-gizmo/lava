{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to Lava","text":"<p>Lava is an AWS based distributed scheduler and job runner.</p> <p>Lava has been used extensively in a large, business critical data warehouse / data lake environment to run tens of thousands of jobs a day on a 24x7 basis, with jobs lasting anywhere between a few seconds and 12 hours.</p> <p>More ...</p> <p>Lava was developed at Origin Energy as part of the Jindabyne initiative. While not part of our core IP, it proved valuable internally, and we're sharing it in the hope it's useful to others.</p> <p></p> <p>Kudos to Origin for fostering a culture that empowers its people to build complex technology solutions in-house.</p>"},{"location":"01-about-lava.html","title":"About Lava","text":"<p>Lava was developed at Origin Energy as part of the Jindabyne initiative. While not part of our core IP, it proved valuable internally, and we're sharing it in the hope it's useful to others.</p> <p></p> <p>Kudos to Origin for fostering a culture that empowers its people to build complex technology solutions in-house.</p> <p>Lava is an AWS based distributed scheduler and job runner. It is scalable, robust and built using native AWS components, wherever possible.</p> <p>Lava has been used extensively in a large, business critical data warehouse / data lake environment to run tens of thousands of jobs a day on a 24x7 basis, with jobs lasting anywhere between a few seconds and 12 hours.</p> <p>Typical jobs perform tasks such as:</p> <ul> <li> <p>AWS environment command and control</p> </li> <li> <p>Data ETL jobs, large and small</p> </li> <li> <p>Inter-company data exchanges.</p> </li> </ul> <p>Lava was built as a result of terminal frustration with some commercial and open source options. Too often, these proved to be hard to install, maintain or use in a complex multi-user, multi-environment context.</p>"},{"location":"01-about-lava.html#features","title":"Features","text":"<p>Lava features include:</p> <ul> <li> <p>Provision of an integrated orchestration and execution environment that is     readily scalable from a desktop installation to a large auto scaling fleet.</p> </li> <li> <p>Jobs can be scheduled, triggered by AWS and external events, or initiated by     other jobs.</p> </li> <li> <p>Ability to connect to a variety of database types, including Postgres, MySQL,     Oracle, Microsoft SQL Server and Redshift.</p> </li> <li> <p>Enhanced support for some AWS RDS and RDS Aurora databases.</p> </li> <li> <p>Built-in connectors to a handful of useful services, including databases,     file shares, Microsoft SharePoint, email servers and Slack.</p> </li> <li> <p>Ability to run SQL jobs as well as executable payloads, including docker     based payloads and native code bundles.</p> </li> <li> <p>Secure management of connection credentials to avoid the need to embed them     into job payloads.</p> </li> </ul>"},{"location":"01-about-lava.html#design-principles","title":"Design Principles","text":"<p>Lava is based on the following principles:</p> <ul> <li> <p>Most people want to do simple tasks so make doing these as simply as     scalability and robustness requirements allow.</p> </li> <li> <p>It should be quick and simple to deploy.</p> </li> <li> <p>AWS native components should be used wherever possible, instead of     installing specialised software components.</p> </li> <li> <p>Existing, tried and tested standard Linux operating system components     should be used wherever possible.</p> </li> <li> <p>Simple, robust, transparent operation.</p> </li> <li> <p>Scalable and reliable.</p> </li> <li> <p>Simple, visible configuration.</p> </li> <li> <p>Jobs are trusted within their operating environment.</p> </li> </ul> <p>These principles led to the following design decisions:</p> <ul> <li> <p>All configuration information is stored in AWS DynamoDB.</p> </li> <li> <p>Job payloads are stored in AWS S3 or AWS ECR and job outputs are stored in     S3.</p> </li> <li> <p>Execution workers are completely stateless and powered by a single Python   based code bundle that can be installed on a standard Linux instance at   boot time or run in a docker container.</p> </li> <li> <p>Linux cron is used for dispatching scheduled jobs.</p> </li> <li> <p>Event driven jobs can be dispatched by AWS facilities such as S3 bucket event     notifications and Amazon EventBridge.</p> </li> <li> <p>AWS SQS is used to dispatch jobs to workers.</p> </li> <li> <p>Logging uses AWS CloudWatch (logs and metrics), DynamoDB and S3.</p> </li> <li> <p>AWS IAM and KMS are used to manage much of the security.</p> </li> <li> <p>AWS SSM Parameter Store and/or AWS Secrets Manager are used to store job     specific parameters. Encrypted parameters store sensitive ones.</p> </li> <li> <p>Deployment of a lava environment and worker instances is done using     supplied CloudFormation templates.</p> </li> <li> <p>AWS ECR is supported for docker related aspects of lava.</p> </li> <li> <p>Lava worker nodes can use AWS EC2 auto scaling groups for robustness.     The supplied CloudFormation templates will set these up.</p> </li> <li> <p>Jobs can use SQS, SES and SNS (among other things) to send messages to     external components.</p> </li> <li> <p>AWS Lambda is used for some support functions.</p> </li> </ul>"},{"location":"01-about-lava.html#authors","title":"Authors","text":"<p>Murray Andrews, Chris Donoghue and Alex Boul.</p>"},{"location":"02-concepts.html","title":"Concepts","text":"<p>Lava is based on the following concepts.</p>"},{"location":"02-concepts.html#realms","title":"Realms","text":"<p>A lava realm is a completely self-contained lava deployment. A realm has its own dedicated configuration tables, jobs, dispatchers, workers, S3 payload storage and security controls.</p>"},{"location":"02-concepts.html#jobs","title":"Jobs","text":"<p>A job is a lava unit of work. Jobs are sent by a dispatcher to workers, either on demand, based on a cron style schedule, in response to an S3 bucket notification event, or in response to an Amazon EventBridge event. Each invocation of a job is a job run and is assigned a UUID. This UUID is used in the naming of job outputs in S3 and in event logging.  Various job types are supported, depending on the kind of processing required.</p> <p>In most cases, a job will run within seconds of being dispatched. When a job is run, it inherits the local timezone of the worker. For those job types that support environment variable parameters (e.g. the exe job type and pkg job type, this can be modified by setting the <code>TZ</code> environment variable. It is important to understand that the dispatcher and the worker may operate in different timezones. The dispatcher timezone controls when the job runs. The worker timezone controls the timezone in which the job runs.</p>"},{"location":"02-concepts.html#workers","title":"Workers","text":"<p>Workers are Linux based nodes that receive jobs from dispatchers via AWS SQS.</p> <p>A realm can have one or more worker fleets with each fleet having one or more worker nodes. All workers in a fleet must have the same capabilities but different fleets can have different capabilities.</p> <p>For example, it's possible to have one fleet of workers dedicated to short jobs that require minimal computational power and a second fleet dedicated to jobs requiring significant computation. Jobs are dispatched to whichever fleet is specified in the job configuration.</p> <p>Workers can be AWS EC2 nodes but do not have to be. Any Linux node with Python 3.9+ installed can be a worker.</p> <p>EC2 based worker fleets can be set to auto scale if required.</p>"},{"location":"02-concepts.html#handlers","title":"Handlers","text":"<p>The worker uses a simple plugin mechanism to handle jobs of different types. Each such plugin handles a given job type.</p> <p>Current handlers support actions such as running SQL, running executables and building the lava crontab on a dispatcher. It is relatively straightforward to add new handlers.</p>"},{"location":"02-concepts.html#dispatchers","title":"Dispatchers","text":"<p>Dispatchers send job run instructions to workers via AWS SQS. If a realm is to have scheduled jobs, then it must have at least one dispatcher.</p> <p>A realm can have multiple dispatchers. For example, a realm may need to schedule jobs in two different timezones (e.g. local and UTC) in which case there would be two dispatchers, each operating in its respective timezone. However, a given job must specify a single dispatcher.</p> <p>A dispatcher is just a special kind of worker that uses the onboard cron daemon to send dispatch messages. Like all workers, it is stateless and the crontab is built behind the scenes automatically by a special job type.</p> <p>Refer to The Lava Dispatch Process for more information.</p>"},{"location":"02-concepts.html#payloads","title":"Payloads","text":"<p>Each job has an associated payload. This is typically the code bundle that is to be executed for the job. Payloads are stored in an area of S3 dedicated to the realm.</p>"},{"location":"02-concepts.html#actions","title":"Actions","text":"<p>For each job, it is possible to specify actions that are to be performed when the job completes, fails or retries. Actions include things such as dispatching other jobs or sending messages via SQS, SNS, email etc.</p>"},{"location":"03-architecture.html","title":"Lava Architecture","text":"<p>Each lava realm has the following components:</p> <ul> <li> <p>A few DynamoDB tables.</p> </li> <li> <p>S3 locations for payloads     and job outputs.</p> </li> <li> <p>One or more SQS queues to feed jobs to     workers.</p> </li> <li> <p>One or more workers.</p> </li> <li> <p>Zero or more dispatchers.</p> </li> <li> <p>Auxiliary components, such as IAM     roles, SNS topics for notices etc.</p> </li> <li> <p>Optional Lambda functions     for triggering jobs from S3 events and assisting external programs to     request job dispatches.</p> </li> <li> <p>A few other AWS bits and pieces.</p> </li> </ul> <p>Most of these components are created by the CloudFormation templates provided with lava. See Installation for more information.</p> <p>A typical realm configuration is shown below.</p> <p></p>"},{"location":"03-architecture.html#lava-s3-locations","title":"Lava S3 Locations","text":"<p>Each lava realm has two designated S3 locations:</p> <ol> <li> <p>The payload area that contains the artefacts required to run a job (scripts     etc.).  This location is specified by the <code>s3_payloads</code> field in the realms     table.</p> </li> <li> <p>The temporary area that contains outputs produced during the run of the job.     This location is specified by the <code>s3_temp</code> field in the realms     table. This area can be used to transfer     artefacts from one job to another. It would normally have an S3 lifecycle     rule to remove objects after a short period of time (e.g. 1 or 2 days).</p> </li> </ol> <p>A bucket for each realm is created by the CloudFormation templates.</p>"},{"location":"03-architecture.html#sqs-queues","title":"SQS Queues","text":"<p>Dispatchers place messages for each job run on an SQS queue where it is picked up by a worker and executed.</p> <p>For a given <code>&lt;REALM&gt;</code> and <code>&lt;WORKER&gt;</code>, the SQS queue name must be <code>lava-&lt;REALM&gt;-&lt;WORKER&gt;</code>.</p> <p>When setting up the queue, it's important to give proper consideration to the visibility timeout and message retention period properties. The visibility  timeout must be long enough for workers to process jobs otherwise SQS will resubmit the message for another run before the run is finished.</p> <p>An SQS queue for each worker is created by the CloudFormation templates.</p>"},{"location":"03-architecture.html#lava-workers","title":"Lava Workers","text":"<p>A lava worker is any Linux node running the worker code bundle. The worker is multi-threaded and can be run interactively, in batch mode or as a daemon.</p> <p>One Linux instance can run multiple workers from the same or different worker fleets, if required. While it is possible to run workers from different realms on the one AWS EC2 instance, this is not recommended as it means the different realms would be sharing an IAM instance role.</p> <p>The CloudFormation templates create a launch template and auto scaling group for each worker fleet based on the lava EC2 AMI. The worker will install the lava code from S3 and run it on boot.</p>"},{"location":"03-architecture.html#lava-dispatchers","title":"Lava Dispatchers","text":"<p>A lava dispatcher is a Lava worker that runs lavasched jobs. Each realm must have at least one dispatcher if jobs are to run on a schedule. The lavasched job causes the crontab on the node to be rebuilt with the list of jobs and their associated schedules.</p> <p>While there can be multiple dispatchers in a realm (e.g. for load sharing or for multiple timezones), each job can only be assigned to a single dispatcher. This means that a worker that also serves as a dispatcher must be a singleton. It cannot be part of an auto scaling group which allows more than one node to run.</p>"},{"location":"03-architecture.html#auxiliary-components","title":"Auxiliary Components","text":""},{"location":"03-architecture.html#iam-roles-and-permissions","title":"IAM Roles and Permissions","text":"<p>The realm CloudFormation templates will create a base set of IAM components, both for the lava workers and for users needing to interact with the lava environment.  See Lava IAM Components.</p>"},{"location":"03-architecture.html#kms-keys","title":"KMS Keys","text":"<p>Each realm requires two AWS KMS keys:</p> <ol> <li> <p><code>lava-&lt;REALM&gt;-sys</code>: The system key that should be used for things such as     credentials when stored in the SSM parameter store. Workers need to be able     to decrypt using this key.</p> </li> <li> <p><code>lava-&lt;REALM&gt;-user</code>: The user key that should be used for encrypting data in     the S3 bucket for the realm. Workers need to be able to encrypt and decrypt     with this key. </p> </li> </ol> <p>These keys are created by the CloudFormation templates.</p>"},{"location":"03-architecture.html#ssm-parameters","title":"SSM Parameters","text":"<p>Lava connectors that require access to confidential parameters (e.g. passwords) expect to find those in the AWS SSM parameter store with names starting with <code>/lava/&lt;REALM&gt;/</code>. Secure string parameters should use the <code>lava-&lt;REALM&gt;-sys</code> KMS key.</p>"},{"location":"03-architecture.html#sns-topics","title":"SNS Topics","text":"<p>Lava job actions can send SNS messages at the completion of a job. While the standard worker IAM policy does not prevent access to other topics, the CloudFormation templates create a topic <code>lava-&lt;REALM&gt;-notices</code> for general purpose notices.</p>"},{"location":"04-dynamodb-tables.html","title":"DynamoDB Tables","text":"<p>Each lava realm uses a number DynamoDB tables to hold configuration and status information.</p> <p>The tables are created by the CloudFormation templates.</p> <p>See also Maintaining DynamoDB Table Entries.</p>"},{"location":"04-dynamodb-tables.html#the-realms-table","title":"The Realms Table","text":"<p>The realms table is named <code>lava.realms</code>. This is a global lava table and is the only object shared across realms.</p> Field Type Required Description config Map No An optional map of configuration values that will be applied to all workers in the realm. Refer to Lava Worker Configuration for more information. on_fail List[Map] No The default on_fail actions for jobs in the realm. on_success List[Map] No The default on_success actions for jobs in the realm. realm String Yes A unique identifier for the realm. Keep it simple. s3_key String Yes A KMS key identifier used when objects are written to S3 by a worker. Typically, either a key ARN or <code>alias/&lt;KEY-NAME&gt;</code>. s3_payloads String Yes A location in S3 where payloads are stored for the realm in the form <code>s3://&lt;BUCKET&gt;/&lt;PREFIX&gt;.</code> s3_temp String Yes A location in S3 where job outputs are stored for the realm in the form <code>s3://&lt;BUCKET&gt;/&lt;PREFIX&gt;.</code> X-* String No Any fields beginning with <code>x-</code> or <code>X-</code> are ignored by lava. These can be used as required for other purposes (e.g. CI/CD, versioning or other related purposes). A number of these fields are used as part of the boot process for EC2 based lava workers for configuration control. * * * Other fields in the realms table may be present to set defaults for other lava subcomponents. These are described in the relevant section."},{"location":"04-dynamodb-tables.html#the-jobs-table","title":"The Jobs Table","text":"<p>The jobs table for a given <code>&lt;REALM&gt;</code> is named <code>lava.&lt;REALM&gt;.jobs</code>. It contains information about jobs and their associated run information.</p> Field Type Required Description cw_metrics Boolean No If specified, enable/disable generation of CloudWatch custom metrics for this job. If present, overrides any value set at the worker/realm level. description String Not yet A short description of the job. This field will be mandatory in a future release. dispatcher String No An identifier specifying the dispatcher for the job. enabled Boolean|String No Whether or not the job is enabled. Defaults to <code>false</code>. String values are Jinja rendered, providing a means to dynamically enable / disable jobs at run-time. More information. event_log * No The specified value is Jinja rendered and recorded as part of the job run event information in the events table. More information. globals Map[String,*] No A map of named values that are made available for Jinja rendering of job actions and job parameters for those job types that use Jinja parameter rendering. Names beginning with <code>lava</code> (case insensitive) are reserved for lava's use. More information on parameters and globals. iteration_delay String No The delay between attempts to run the job in the form nnX where nn is a number and X is s (seconds) or m (minutes). Default is <code>0s</code>. The maximum allowed value is specified by the ITERATION_MAX_DELAY configuration parameter. See Job Retries for more information. iteration_limit Integer No The number of attempts that will be made to run the job. Default is 1. The maximum allowed value is specified by the ITERATION_MAX_LIMIT configuration parameter. This is unrelated to the SQS related <code>max_tries</code> parameter. See Job Retries for more information. job_id String Yes The unique job identifier for the realm. It is possible to have some grouping of jobs using a path like structure. e.g. <code>job_group/myjob_01</code>. max_run_delay String No The maximum allowed delay between when a job is dispatched and when it is run in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes), <code>h</code> (hours) or <code>d</code> (days). If this limit is exceeded, the job run is discarded with an error. If not specified, no limit is imposed other than the message retention period of the worker SQS job queue. max_tries Number No By default, if a lava worker fails mid-job, SQS will resubmit the dispatch request at the end of the visibility timeout. If <code>max_tries</code> is set to a positive integer value, then the dispatch message will be discarded when the SQS message <code>ApproximateReceiveCount</code> exceeds the specified value. Note that the minimum of the value of the <code>Maximum Receives</code> value for the worker SQS queue (if set) and any limit specified by the <code>--retries</code> worker option is still the upper limit. on_fail List[Map] No Job specific on_fail actions for the job. Overrides any realm level setting. on_retry List[Map] No Job specific on_retry actions for the job. Overrides any realm level setting. on_success List[Map] No Job specific on_success actions for the job. Overrides any realm level setting. owner String Not yet Name or email address of the job owner. This field will be mandatory in a future release. parameters Map[String,*] No A map of parameters that will be passed to the job. The parameter structure is dependent on the job type. payload * Yes The job payload. The type and format is job type dependent. Currently, a value is required even for job types that do not need it. In this case set the value to <code>null</code>. schedule String No A cron schedule that specifies when the job will run. Refer to the section Schedule Specifications for more information. If not specified, the job can be dispatched on demand but will not be scheduled. state Map[String,*] No A map of state items. For each item, the key is the <code>state_id</code> and the value is a default value. The default values are replaced at run-time by the current value of the specified state item in the state table, if it exists. type String Yes The name of the job handler to run. worker String Yes The name of a worker that can run the job. X-* String No Any fields beginning with <code>x-</code> or <code>X-</code> are ignored by lava. These can be used as required for other purposes (e.g. CI/CD, versioning or other related purposes). The lava job framework uses a number of these fields for various purposes. <p>Warning</p> <p>Currently, unknown fields in the job specification will result in a deprecation warning being written to the worker's log but the job is permitted to run. A future release will reject jobs that have unknown fields.</p>"},{"location":"04-dynamodb-tables.html#the-enabled-field","title":"The enabled Field","text":"<p>Prior to v8.0 (Incahuasi), the <code>enabled</code> field was a simple, static boolean value. If <code>false</code>, the job would be skipped. This behaviour is unchanged in v8+ if the value is boolean.</p> <p>If the value is a string, it is Jinja rendered. If the resulting value is the string <code>true</code> (case and surrounding whitespace are ignored), the job is enabled to run. Any other value will result in the job being skipped. This allows job execution to be conditional on run-time values. </p> <p>The following variables are made available to the Jinja renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>As an example, the following job specification fragment will only enable the job if the value of the global <code>a</code> is palindromic (which it is, in the example):</p> <pre><code>{\n  \"enabled\": \"{% set word = globals.a | lower %}{{ word == word | reverse }}\",\n  \"globals\": {\n    \"#\": \"Global a is a palindrome.\",\n    \"a\": \"Tattarrattat\"\n  }\n}\n</code></pre> <p>In this example, the job is only enabled if the value of the <code>x</code> field in state item <code>sid</code> is odd:</p> <pre><code>{\n  \"enabled\": \"{% set val = state['sid'].x | int %}{{ val % 2 == 1 }}\",\n  \"state\": {\n    \"sid\": \"-- set at run-time --\"\n  }\n}\n</code></pre> <p>As a more complex example, this job specification fragment only enables a job if it has not run successfully in the last 4 hours:</p> <pre><code>{\n  \"schedule\": \"30 * * * *\",\n  \"enabled\": \"{% set ts=utils.parsedate.parse(state['sid']) %}{{ ustart-ts &gt; utils.timedelta(hours=4) }}\",\n  \"on_success\": [\n    {\n      \"action\": \"state\",\n      \"state_id\": \"sid\",\n      \"value\": \"{{ ustart.isoformat() }}\"\n    }\n  ],\n  \"state\": {\n    \"sid\": \"2000-01-01T00:00:00Z\"\n  },\n  \"event_log\": \"Time since last run: {% set ts=utils.parsedate.parse(state['sid']) %}{{ ustart-ts }}\"\n\n}\n</code></pre> <p>Some points to note:</p> <ul> <li> <p>A state variable is declared to hold an ISO 8601 datetime containing the     last successful run-time for the job. This has an initial value defined in     the job specification.</p> </li> <li> <p>The <code>enabled</code> field checks the start time of the current run against the     previous run start time, to ensure the required period of time has passed     (4 hours in the example).</p> </li> <li> <p>Once the job runs, an <code>on_success</code> action creates a state item that records     the start time of the successful run.</p> </li> <li> <p>An <code>event_log</code> field records for posterity the time since the previous run.</p> </li> </ul>"},{"location":"04-dynamodb-tables.html#the-event_log-field","title":"The event_log Field","text":"<p>Jobs may receive critical configuration as part of the dispatch process via parameters, globals, and state items. It can be tricky to determine from the events table exactly what a job run was doing, particularly in the event that a job run fails.</p> <p>While it would be possible to record the entire augmented job specification, this is not safe in all cases as sensitive values may be exposed in the events table.</p> <p>The <code>event_log</code> field allows the job specification to specify that certain information should be recorded in the job run record in the events table. The value of the field is an arbitrary object that will be Jinja rendered and added to the events table before the first iteration of the job begins. this information is added in a new entry with a <code>logging</code> status in the <code>log</code> field of the event record.</p> <p>The following variables are made available to the Jinja renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>As an example, the following job specification fragment would record the value of a specific global.</p> <pre><code>{\n  \"event_log\": \"The value of global g1 is '{{ globals.g1 }}'\"\n}\n</code></pre> <p>This fragment would record two globals in a map format:</p> <pre><code>{\n  \"event_log\": {\n    \"g1\": \"{{ globals.g1 }}\",\n    \"g2\": \"{{ globals.g2 }}\"\n  }\n}\n</code></pre> <p>This fragment would record all of the globals (as a Python object converted to a string):</p> <pre><code>{\n  \"event_log\": \"{{ globals }}'\"\n}\n</code></pre> <p>Warning</p> <p>DO NOT be cavalier with this. Take care to avoid logging sensitive information.</p>"},{"location":"04-dynamodb-tables.html#the-connections-table","title":"The Connections Table","text":"<p>The connections table for a given <code>&lt;REALM&gt;</code> is named <code>lava.&lt;REALM&gt;.connections</code>. It contains information to assist job handlers make connections to external resources, typically databases.</p> Field Type Required Description conn_id String Yes Connection identifier. description String No A short description of the connection. enabled Boolean Yes Whether or not the connection is enabled. Defaults to <code>false</code> owner String Not yet Name or email address of the connection owner. This field will be mandatory in a future release. type String Yes The connection type. This is used to identify a connector plugin to establish the connection. X-* String No Any fields beginning with <code>x-</code> or <code>X-</code> are ignored by lava. These can be used as required for other purposes (e.g. CI/CD, versioning or other related purposes). * * * All other fields are connection type specific. For more information see the section on connectors."},{"location":"04-dynamodb-tables.html#the-events-table","title":"The Events Table","text":"<p>The events table for a given <code>&lt;REALM&gt;</code> is named <code>lava.&lt;REALM&gt;.events</code>. It is populated by lava workers as they start and finish jobs runs.</p> <p>Querying the table via the DynamoDB console can be a bit tedious so a utility <code>lava-events</code> is provided to assist with this. Get help thus:</p> <pre><code>lava-events --help\n</code></pre> <p>The lava GUI also provides the ability to query the events table.</p> Field Type Required Description hostname String Yes The hostname of the worker that ran the job. instance_id String No The AWS EC2 instance ID. Not present when the worker is not an EC2 instance. job_id String Yes The unique job identifier for the realm. log List[Map] Yes A list of events that have occurred for this run of the job. Each entry in the list is a map which will contain <code>info</code>, <code>status</code> and <code>ts_event</code> fields. The contents of the <code>info</code> field are job type dependent. run_id String Yes The UUID for the job run. This is used in the naming of job outputs. status String Yes The most recent <code>status</code> value for this job run. It will reflect the <code>status</code> value of the latest entry in the <code>log</code> list. ts_dispatch String Yes A timezone aware ISO 8601 format timestamp for the time the job was dispatched. ts_event String Yes A timezone aware ISO 8601 format timestamp for the most recent event for this job run. It will reflect the <code>ts_event</code> value for this job run. ttl Number Yes The epoch timestamp when the event record will expire. DynamoDB manages expiry automatically provided the TTL attribute for the table is set to <code>ttl</code>. tu_event String Yes A timezone naive ISO 8601 format timestamp for the UTC time for the most recent event for this job run. It will reflect the <code>tu_event</code> value for this job run. worker_id String No If the worker is an AWS EC2 instance, the instance ID. worker String Yes The name of the worker that ran the job."},{"location":"04-dynamodb-tables.html#the-s3triggers-table","title":"The S3triggers Table","text":"<p>The s3triggers table for a given <code>&lt;REALM&gt;</code> is named <code>lava.&lt;REALM&gt;.s3triggers</code>. It is used to map S3 bucket events to jobs. When an S3 event occurs with a bucket and object prefix matching an entry in the table, the corresponding lava job is dispatched. See Triggering Jobs from S3 Events for more information.</p> Field Type Required Description bucket String Yes Bucket name. delay String No Dispatch message sending delay in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds) or <code>m</code> (minutes). The maximum allowed value is 15 minutes. description String No A short description of the trigger. enabled Boolean Yes Whether or not the s3trigger is enabled. globals Map[String,*] No A map of named values that are included in the dispatch request. These are Jinja rendered. Names beginning with <code>lava</code> (case insensitive) are reserved for lava's use. More information on parameters and globals. jinja Boolean No If <code>false</code>, disable Jinja rendering of the parameters and globals. Defaults to <code>true</code>. job_id String | List[String] Yes Job identifier for the lava job that will be dispatched, or a list of job identifiers. Each is Jinja rendered before use. owner String Not yet Name or email address of the trigger owner. This field will be mandatory in a future release. parameters Map[String,*] No A map of parameters for the job that will be included in the dispatch. These will be Jinja rendered. prefix String Yes Object prefix. Do not include a trailing <code>/</code> or matches will fail. To indicate the root of the bucket, use a prefix value of <code>*</code>. trigger_id String Yes A unique identifier within the realm for the trigger entry. X-* String No Any fields beginning with <code>x-</code> or <code>X-</code> are ignored by lava. These can be used as required for other purposes (e.g. CI/CD, versioning or other related purposes). <p>In addition to the fields described above, entries in the S3 triggers tab may also contain fields starting with <code>if_</code> and <code>if_not_</code>. These cause a test to be applied to the S3 object event, The dispatch is only performed if the test passes in the case of <code>if_</code> fields, or fails in the case of <code>if_not_</code> fields. The available <code>if_</code> tests are described below. There is a corresponding <code>if_not_</code> test for each.</p> Field Type Required Description if_fnmatch String | List[String] No Perform a glob style match against the S3 object key. If a list of patterns is provided, returns true if any of the patterns match the key. The matching rules defined by Python's fnmatch apply. if_size_gt Integer | String No Check that the S3 object is larger than the specified size. Values can be specified as an integer or a string in the form <code>nnX</code>, where <code>n</code> is a number and <code>X</code> is an optional unit such as 'K', 'KB', 'KiB', 'MiB' etc. Default for <code>X</code> is bytes. if_event_type String No Check that the S3 event type matches the specified value (e.g. <code>ObjectCreated:Put</code>). Glob style patterns are accepted (e.g. <code>ObjectCreated:*</code>)."},{"location":"04-dynamodb-tables.html#the-state-table","title":"The State Table","text":"<p>The state table for a given <code>&lt;REALM&gt;</code> is named <code>lava.&lt;REALM&gt;.state</code>. It is used to allow lava jobs to save state information for limited periods of time that can be accessed by authorised external actors or other lava jobs.</p> <p>Creation and reading of entries in the state table are managed by the lava state manager. Other tools should not be used for this purpose.</p> Field Type Required Description state_id String Yes The unique identifier for the state item. publisher String No An arbitrary identifier for the entity posting the event item. Not used by lava itself. timestamp String No An ISO 8601 format timestamp for the state item creation. Not used by lava itself. ttl Number Yes The epoch timestamp when the state record will expire. DynamoDB manages expiry automatically provided the TTL attribute for the table is set to <code>ttl</code>. The default and maximum time-to-live for entries in the table can be controlled by worker configuration parameters. type String Yes The state value type. This tells the lava worker how to decode the value. See State Item Types. value * Yes The state value. The structure depends on the state type."},{"location":"04-dynamodb-tables.html#state-item-types","title":"State Item Types","text":"<p>Each state record has a specified <code>type</code> that tells the worker how to decode the <code>value</code>.  Within lava itself, this is largely transparent as the worker handles all the necessary encoding and decoding.</p> <p>The following types are supported:</p> Type Description json The value is stored as a JSON encoded object. This is the default as it provides the most fidelity in the encoding / decoding process. Lava does this automatically within its own universe. External actors should use the lava state API or the lava state utility rather than attempt to reproduce this process natively. raw The value is stored as a DynamoDB object. This can sometimes do unhelpful type conversions on numbers. secure This uses the same value encoding mechanism as <code>json</code> with the addition of KMS encryption. Once again, external actors should use the lava state API or the lava state utility rather than attempt to reproduce this process natively. KMS encryption imposes a maximum size limit of 4096 bytes on the JSON encoded state item value."},{"location":"05-job-dispatch.html","title":"The Lava Dispatch Process","text":"<p>The underlying mechanism for the lava job dispatch process is very simple. A JSON formatted message containing a job ID is sent via AWS SQS to a worker. The SQS queue for any worker is named <code>lava-&lt;REALM&gt;-&lt;WORKER&gt;</code>.</p> <p>The worker retrieves the message, extracts the job details from the jobs table and then runs the job if it is enabled and the worker is the one named in the job specification.</p> <p>There are several mechanisms that can initiate the sending of the SQS dispatch message:</p> <ul> <li> <p>Scheduled dispatch</p> </li> <li> <p>Job initiated dispatch</p> </li> <li> <p>AWS S3 event triggered dispatch</p> </li> <li> <p>Amazon EventBridge triggered dispatch</p> </li> <li> <p>The Dispatch Helper</p> </li> <li> <p>Direct dispatch.</p> </li> </ul>"},{"location":"05-job-dispatch.html#scheduled-dispatch","title":"Scheduled Dispatch","text":"<p>Dispatch events can be scheduled by a dispatcher. A dispatcher is just a worker that runs the lavasched job type. This job type builds a crontab containing invocations of the <code>lava-dispatcher</code> utility to dispatch jobs in accordance with the schedules specified in the job specifications.</p> <p>A realm can have multiple dispatchers. For example, a realm may need to schedule jobs in multiple timezones (e.g. local and UTC) in which case there would be two dispatchers, each operating in its respective timezone. However, a given job must specify a single dispatcher.</p> <p>The lavasched job must also be dispatched periodically to create and refresh the crontab. Clearly, there is a chicken and egg problem here in that an initial dispatch of the lavasched job is required to create the first crontab.</p> <p>There are two ways to achieve this, manually or with a worker jump-start.</p>"},{"location":"05-job-dispatch.html#initialising-the-scheduler","title":"Initialising the Scheduler","text":"<p>The lavasched job can be dispatched manually whenever required. When dispatching a lavasched job, the <code>dispatcher</code> parameter must be provided.</p> <pre><code># This command will force the dispatching worker to (re-)build its crontab.\n# It will include dispatches for jobs aimed at &lt;DISPATCHER&gt;\n\nlava-dispatcher --realm &lt;REALM&gt; --worker &lt;DISPATCH-WORKER&gt; \\\n    &lt;LAVASCHED_JOB_ID&gt; --param dispatcher=&lt;DISPATCHER&gt;\n</code></pre> <p>The problem with this approach is for dispatchers that are started spontaneously (e.g. by an auto scaler). A manual intervention is then required to dispatch the first lavasched job or else not much lava will flow.</p>"},{"location":"05-job-dispatch.html#jump-starting-the-scheduler","title":"Jump-starting the Scheduler","text":"<p>The lava worker has a <code>--jump-start</code> option. When the worker starts, this option forces it to search the jobs table for any enabled lavasched jobs for which it is the designated worker and dispatch them immediately.</p> <p>This option is safe to use on any worker but does require a full scan of the jobs table.</p>"},{"location":"05-job-dispatch.html#scheduling-the-scheduler","title":"Scheduling the Scheduler","text":"<p>Of course, the lavasched jobs should also have their own schedule specified to refresh the crontab as changes are made in the jobs table. It is recommended to schedule the lavasched jobs to run every 10 or 15 minutes. The job makes a reasonable effort to avoid updating the crontab when nothing has changed. It is also careful to avoid disturbing entries in the crontab that don't belong to lava.</p>"},{"location":"05-job-dispatch.html#matching-jobs-to-dispatchers","title":"Matching Jobs to Dispatchers","text":"<p>This is a typical job specification for a lavasched job. Note the two different <code>dispatcher</code> items that serve related but separate purposes.</p> <pre><code>{\n  \"description\": \"Rebuild the crontab for the dispatcher\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"...\",\n  \"owner\": \"...\",\n  \"parameters\": {\n    \"dispatcher\": \"Sydney\",\n    \"env\": {\n      \"CRON_TZ\": \"Australia/Sydney\",\n      \"PATH\": \"/usr/local/bin:/bin:/usr/bin\"\n    }\n  },\n  \"payload\": \"--\",\n  \"schedule\": \"0-59/15 * * * *\",\n  \"type\": \"lavasched\",\n  \"worker\": \"core\"\n}\n</code></pre> <p>Info</p> <p>This PATH specification in the environment clause above is critical when using the lava AMI. If not specified, cron will use the default system version of Python rather than the preferred version installed in <code>/usr/local/bin</code> and the dispatcher may fail. Silently.</p> <p>Each lavasched job specification must contain both</p> <ol> <li> <p>A <code>dispatcher</code> field in the job specification.</p> <p>This indicates which dispatcher will dispatch the lavasched job itself.</p> </li> <li> <p>A <code>dispatcher</code> parameter in the job specifications <code>parameter</code> object.</p> <p>This indicates which jobs will be included in the crontab built by the job. It is matched against the <code>dispatcher</code> field for all jobs in the table.</p> </li> </ol>"},{"location":"05-job-dispatch.html#suspending-the-scheduler","title":"Suspending the Scheduler","text":"<p>If there is a need to temporarily disable scheduled job dispatch from a given dispatcher, change both of the <code>dispatcher</code> values in the lavasched job specification to a value that will not match any other job. Once the crontab updates, no jobs other than the lavasched job will be run. This is much simpler than changing all of the other jobs and still allows the scheduled dispatch process to be re-enabled when required.</p> <p>For example:</p> <pre><code>{\n  \"description\": \"Rebuild the crontab for the dispatcher\",\n  \"dispatcher\": \"**Disabled** Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"...\",\n  \"owner\": \"...\",\n  \"parameters\": {\n    \"dispatcher\": \"**Disabled** Sydney\",\n    \"env\": {\n      \"CRON_TZ\": \"Australia/Sydney\",\n      \"PATH\": \"/usr/local/bin:/bin:/usr/bin\",\n    }\n  },\n  \"payload\": \"--\",\n  \"schedule\": \"0-59/15 * * * *\",\n  \"type\": \"lavasched\",\n  \"worker\": \"core\"\n}\n</code></pre>"},{"location":"05-job-dispatch.html#schedule-specifications","title":"Schedule Specifications","text":"<p>Each job that is to be dispatched on a schedule must have a <code>schedule</code> field in the job specification that is used to derive the time component of the dispatcher crontab entries.</p> <p>The value of this field is either:</p> <ul> <li> <p>A string containing a conventional crontab timing specification.</p> </li> <li> <p>A lava scheduling object as     described below.</p> </li> <li> <p>A list of an arbitrary mixture of the above.</p> </li> </ul> <p>This means that one job can have multiple crontab entries to allow more complex scheduling permutations without the need to duplicate the entire job specification.</p> <p>Info</p> <p>The syntax of crontab strings must be compatible with the dispatch worker's cron implementation. It is strongly recommended to avoid the non-standard extensions to cron found on some systems. Crontab Guru is a useful syntax helper / checker. Having said that, some of the <code>@</code> forms can be useful shortcuts. The <code>@reboot</code> form is a bit special in that it will be dispatched only when the dispatcher node reboots.</p>"},{"location":"05-job-dispatch.html#lava-scheduling-objects","title":"Lava Scheduling Objects","text":"<p>A lava scheduling object is a map with the following fields:</p> Field Type Required Description crontab String Yes A conventional crontab timing specification. from String No An ISO 8601 datetime string specifying when the schedule object becomes active. Default is midnight, 01/01/0001 to String No An ISO 8601 datetime string specifying when the schedule object ceases to active. Default is midnight, 31/12/9999 <p>The <code>from</code> and <code>to</code> fields allow specific schedules to be active only within defined time periods. If <code>from</code> is less than <code>to</code>, the schedule is active only between those two times. If <code>from</code> is greater than <code>to</code>, the schedule is active only outside those two times.</p> <p></p> <p>The <code>from</code> and <code>to</code> fields are each timezone aware. If no timezone is specified, the local timezone of the worker running the lavasched job is assumed.</p>"},{"location":"05-job-dispatch.html#examples","title":"Examples","text":"<p>The simplest form of schedule is a simple crontab string:</p> <pre><code>{\n  ...\n  \"schedule\": \"0 12 * * Mon\",\n  ...\n}\n</code></pre> <p>This one will run the job at midday on weekdays and 2pm on weekends.</p> <pre><code>{\n  ...\n  \"schedule\": [\n    \"0 12 * * 1-5\",\n    \"0 14 * * 0,6\"\n  ],\n  ...\n}\n</code></pre> <p>This one will run the job daily at midday before 30 June 2019 and daily at 2pm after that. Note that in this example, UTC is specified in the date entries.</p> <pre><code>{\n  ...\n  \"schedule\": [\n    {\n      \"crontab\": \"0 12 * * *\",\n      \"to\": \"2019-06-30T00:00:00Z\"\n    },\n    {\n      \"crontab\": \"0 14 * * *\",\n      \"from\": \"2019-06-30T00:00:00Z\"\n    }\n  ],\n  ...\n}\n</code></pre> <p>This one will run the job daily at midday, except during February 2019 when it is not active. Note that <code>from</code> is greater than <code>to</code> in this case and the lack of timezone means the local timezone of the worker running the lavasched job will be used.</p> <pre><code>{\n  ...\n  \"schedule\": {\n    \"crontab\": \"0 12 * * *\",\n    \"from\": \"2019-03-01T00:00:00\",\n    \"to:\": \"2019-02-01T00:00:00\"\n  },\n  ...\n}\n</code></pre>"},{"location":"05-job-dispatch.html#job-initiated-dispatch","title":"Job Initiated Dispatch","text":"<p>Lava has two mechanisms by which a job can dispatch other jobs.</p>"},{"location":"05-job-dispatch.html#the-dispatch-job-type","title":"The Dispatch Job Type","text":"<p>The dispatch job type initiates the dispatch of other jobs. This could be used, for example, as a step in a job chain.</p>"},{"location":"05-job-dispatch.html#the-dispatch-job-action","title":"The Dispatch Job Action","text":"<p>Job actions are conditionally executed when a job succeeds or fails. One of the available action types is the dispatch action.</p>"},{"location":"05-job-dispatch.html#the-dispatch-helper","title":"The Dispatch Helper","text":"<p>The dispatch helper is an AWS Lambda function that provides a simplified mechanism for an external component to dispatch a lava job. It accepts simple, dispatch requests and constructs the necessary messages within the lava environment to complete the dispatch process.</p> <p>The dispatch helper can accept dispatch requests via the following mechanisms:</p> <ul> <li> <p>SNS</p> </li> <li> <p>SQS</p> </li> <li> <p>Amazon EventBridge</p> </li> <li> <p>direct invocation.</p> </li> </ul> <p>The standard CloudFormation template will setup an SNS topic, <code>lava-&lt;REALM&gt;-dispatch</code> and subscribe the dispatch helper lambda function to it. Other SNS topics or SQS queues can be created and subscribed manually as required.</p> <p>Two request formats are accepted:</p> <ol> <li> <p>CLI-style format</p> </li> <li> <p>JSON format.</p> </li> </ol> <p></p>"},{"location":"05-job-dispatch.html#cli-style-dispatch-requests","title":"CLI-Style Dispatch Requests","text":"<p>The dispatch request message sent to the dispatch helper must be in the following format:</p> <pre><code># One or more lines like the following. Shell style comments and blank lines\n# are ignored. Shell style lexing is applied.\n\n&lt;JOB_ID&gt; [-d DURATION] [-g &lt;GLOBAL&gt;=&lt;VALUE&gt;] ... [-p &lt;PARAM&gt;=&lt;VALUE&gt; ] ... \n\n# ... or ...\n\n&lt;JOB_ID&gt; [--delay DURATION] [--global &lt;GLOBAL&gt;=&lt;VALUE&gt;] ... [--param &lt;PARAM&gt;=&lt;VALUE&gt; ] ...\n</code></pre> <p>For example:</p> <pre><code># Dispatch the job \"my-job\" with no parameters.\nmy-job\n\n# Dispatch the job \"my-job\" with some additional parameters.\nmy-job -p timeout=20m -p flow=Pahoehoe -g planet=Mars -g name='Alba Mons'\n\n# Dispatch the job \"my-job\" with a delay of 3 minutes\nmy-job -d 3m\n</code></pre> <p>This is one way to send a dispatch message via the helper:</p> <pre><code># Get the topic ARN then ...\naws sns publish \\\n    --topic-arn \"arn:aws:sns:us-west-2:0123456789012:lava-&lt;REALM&gt;-dispatch\" \\\n    --message \"my-job -p timeout=20m -p flow=Pahoehoe -g planet=Mars -g name='Alba Mons'\"\n</code></pre>"},{"location":"05-job-dispatch.html#parameter-and-global-specifications","title":"Parameter and Global Specifications","text":"<p>When specifying parameters and globals, the name can be a simple name or a dot separated hierarchical name. The dispatch helper will convert the names into a matching JSON structure that is included in the dispatch message. When the lava worker receives the dispatch message, it will merge this structure into the corresponding parameter or globals structure extracted from the jobs table.</p> <p>For example, consider the following message sent to the dispatch helper:</p> <pre><code>my-job -p timeout=1h -p vars.location=Isabela -p vars.name=\"Sierra Negra\" -g country=Equador\n</code></pre> <p>The dispatch message sent by the dispatch helper will contain the following.</p> <pre><code>{\n    \"globals\": {\n        \"country\": \"Equador\"\n    },\n    \"parameters\": {\n        \"timeout\": \"1h\",\n        \"vars\": {\n            \"location\": \"Isabela\",\n            \"name\": \"Sierra Negra\"\n        }\n    }\n}\n</code></pre> <p>If the job specification in the jobs table contains the following:</p> <pre><code>{\n    \"globals\": {\n        \"country\": \"Replaced at run time\",\n        \"ocean\": \"Atlantic\"\n    },\n    \"parameters\": {\n        \"action\": \"run away\",\n        \"timeout\": \"20m\",\n        \"vars\": {\n            \"whatever\": \"This will be replaced\"\n        }\n    }\n}\n</code></pre> <p>then the final job specification used by the lava worker will be:</p> <pre><code>{\n    \"globals\": {\n        \"country\": \"Equador\",\n        \"ocean\": \"Atlantic\"\n    },\n    \"parameters\": {\n        \"action\": \"run away\",\n        \"timeout\": \"1h\",\n        \"vars\": {\n            \"location\": \"Isabela\",\n            \"name\": \"Sierra Negra\"\n        }\n    }\n}\n</code></pre> <p>Notice that the parameters and globals from the dispatch helper override similarly named parameters and globals in the job specification, including, in this case, the entire <code>vars</code> map in the parameters.</p>"},{"location":"05-job-dispatch.html#json-dispatch-requests","title":"JSON Dispatch Requests","text":"<p>JSON formatted dispatch requests must be in the following format:</p> <pre><code>{\n  \"job_id\": \"...\",\n  \"globals\": {\n    \"g1\": \"...\",\n    \"g2\": \"...\"\n  },\n  \"parameters\": {\n    \"p1\": \"...\",\n    \"p2\": \"...\"\n  },\n  \"delay\": \"&lt;DURATION&gt;\"\n}\n</code></pre> <p>The <code>job_id</code> element is mandatory. All other elements are optional. The values for individual <code>globals</code> and <code>parameters</code> can be any supported JSON type (not just strings as shown above).</p> <p>The dispatch request can be sent to the dispatch help lambda as either:</p> <ul> <li> <p>The body of an SQS or SNS message</p> </li> <li> <p>The payload of a direct lambda invocation</p> </li> <li> <p>The content of an event message produced by Amazon EventBridge.</p> </li> </ul> <p>For example, the following sends a dispatch request directly to the dispatch helper lambda using the AWS CLI.</p> <pre><code># Send a dispatch request directly to the dispatch helper for realm &lt;REALM&gt;\n\naws lambda invoke --cli-binary-format raw-in-base64-out \\\n    --function-name lava-&lt;REALM&gt;-dispatch \\\n    --invocation-type Event \\\n    --payload '{\"job_id\": \"my-job-id\", \"globals\": {\"g1\": \"GLOB1\"}}' \\\n    /dev/stdout\n</code></pre> <p>See also Dispatching Jobs from Amazon EventBridge.</p>"},{"location":"05-job-dispatch.html#dispatching-jobs-from-s3-events","title":"Dispatching Jobs from S3 Events","text":"<p>Lava jobs can be dispatched in response to S3 events. This is done using a realm specific Lambda function s3trigger which, optionally, gets deployed as a function named <code>lava-&lt;REALM&gt;-s3trigger</code> when the main realm CloudFormation stack is deployed.</p> <p>S3 event messages can be sent to the s3trigger lambda as either:</p> <ul> <li> <p>S3 bucket notification events delivered directly from S3, via SNS or via     SQS.</p> </li> <li> <p>Amazon EventBridge     messages.</p> </li> </ul> <p>The following diagram shows the various event notification options supported by lava.</p> <p></p> <p>Note that the CloudFormation stack will deploy the function, the s3triggers table and required IAM components but it will not add any entries in the s3triggers table, nor will it add any S3 subscriptions to the lambda function. These steps must be done manually.</p> <p>The s3trigger function works thus:</p> <ol> <li> <p>An S3 bucket notification event or Amazon EventBridge event record is passed     to the Lambda function.</p> </li> <li> <p>The function looks up the s3triggers table     to find entries that match the bucket name and object key. There may be     multiple matching entries.</p> </li> <li> <p>If there is no matching s3triggers entry, the final path component of the     prefix is removed and the search for matching entries is repeated. This     process is repeated until either an entry is found, or all path components     have been exhausted.</p> </li> <li> <p>Any matching s3trigger entries with the <code>enabled</code> field set to <code>false</code> are     discarded.</p> </li> <li> <p>For each remaining s3trigger entry, any <code>if_*</code> and <code>if_not_*</code> conditions are     evaluated. If these checks pass, the job referenced in the s3trigger entry     is dispatched.</p> </li> </ol> <p>Info</p> <p>Like lava jobs, s3trigger entries must be explicitly enabled in order to be active.</p> <p>For example, if the bucket name is <code>mybucket</code> and the object key for the S3 event notification is <code>a/b/c</code>, the  s3triggers table will be progressively queried for the following table entries until one is found.</p> <ul> <li>bucket=<code>mybucket</code>, prefix=<code>a/b/c</code></li> <li>bucket=<code>mybucket</code>, prefix=<code>a/b</code></li> <li>bucket=<code>mybucket</code>, prefix=<code>a</code></li> <li>bucket=<code>mybucket</code>, prefix=<code>*</code></li> </ul> <p>The final query refers to an s3triggers entry for the entire bucket (i.e. an empty prefix). This use of <code>*</code> to represent an empty prefix is a consequence of DynamoDB's inability to handle empty strings.</p> <p>Note</p> <p>DynamoDB has now been updated to handle empty strings but lava retains the requirement to use a <code>*</code> as described above.</p> <p>Query results are cached for a short period of time to reduce traffic on the s3triggers and jobs tables. The cache duration can be modified by setting the S3TRIGGER_CACHE_TTL configuration variable. The scope of the cache is limited to each run-time instance of the lambda function. This works because AWS Lambda will reuse run-time instances where possible.</p>"},{"location":"05-job-dispatch.html#rendering-of-dispatch-parameters","title":"Rendering of Dispatch Parameters","text":"<p>The s3trigger entry contains the job_id for the job to be dispatched and may also contain a map of parameters for the job and a map of globals that will be included in the dispatch.</p> <p>These parameters and globals will be rendered using Jinja unless the s3trigger entry has a <code>jinja</code> field set to <code>false</code>. This rendering process allows information related to the S3 event, such as the bucket name and object key, to be passed to the lava job at run time.</p> <p>The parameters must be legal values for the job type being dispatched. They will be merged in with any parameters in the job specification itself.</p> <p>The globals are agnostic of job type and can be provided to any job. They will be merged in with any globals in the job specification itself and made available to the Jinja rendering process of any job that uses this mechanism.</p> <p>The following variables are made available to the renderer.</p> Name Type Description bucket str The bucket name. key str The S3 object key. event dict[str,*] The raw S3 event record. The format of this can vary significantly, depending on whether the event message was delivered directly from S3 or via SQS, SNS or EventBridge. Don't use it unless absolutely necessary. info dict[str,*] A canonical extract of the most useful elements from the S3 event record. It is consistent in structure and content across the different event record delivery mechanisms and should be used instead of <code>event</code>.  Normal Jinja syntax can be used to extract components of interest. A sample is shown below. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>The <code>info</code> object contains the following elements:</p> Name Type Description aws_region str The AWS region for the S3 operation. bucket str The bucket name. event_time datetime A timezone aware datetime for the S3 operation. event_type str The type of S3 operation that caused the event. See About S3 Event Types below. key str The S3 object key. size int The size in bytes of the object. source_ip IPv4Address | IPv6Address The source IP address of the S3 operation. See ipaddress in the Python standard library documentation. <p>A typical <code>info</code> (Python) object looks like this:</p> <pre><code>{\n    'bucket': 'my-bucket',\n    'key': 'HappyFace.jpg',\n    'size': 1024,\n    'event_time': datetime.datetime(2022, 1, 30, 16, 18, 17, 123, tzinfo=tzutc()),\n    'source_ip': IPv4Address('10.200.240.5'),\n    'event_type': 'ObjectCreated:Put',\n    'aws_region': 'ap-southeast-2'\n}\n</code></pre> <p>An s3triggers (JSON) entry may then look like this.</p> <pre><code>{\n    \"description\": \"Something interesting just happened.\",\n    \"bucket\": \"...\",\n    \"prefix\": \"...\",\n    \"enabled\": true,\n    \"globals\": {\n        \"bucket\": \"{{bucket}}\",\n        \"key\": \"{{key}}\",\n        \"account-id\": \"{{bucket.split(':')[4]}}\"\n    },\n    \"if_fnmatch\": \"*.zip\",\n    \"if_not_fnmatch\": \"*ignore*\",\n    \"if_size_gt\": 0,\n    \"job_id\": \"my_exe_job\",\n    \"parameters\": {\n        \"env\": {\n            \"BUCKET\": \"{{bucket}}\",\n            \"KEY\": \"{{key}}\",\n            \"IP\": \"{{info.source_ip}}\",\n            \"EVENT_TIME\": \"{{info.event_time.isoformat()}}\"\n            }\n        },\n    \"trigger_id\": \"unique_trigger_id\"\n}\n</code></pre>"},{"location":"05-job-dispatch.html#about-s3-event-types","title":"About S3 Event Types","text":"<p>AWS is very inconsistent in values for event types between EventBridge messages and the S3 bucket notification configuration mechanisms. Lava can't easily fix that without compounding the problem. Sorry.</p> <p>For example, when an S3 object is created, the S3 bucket notification configuration event will have an <code>eventName</code> field something like <code>ObjectCreated:Put</code>. The EventBridge message for the same action will have a <code>detail-type</code> field of <code>Object Created</code> and a <code>detail.reason</code> value of <code>PutObject</code>. The other S3 event types vary even more than this.</p> <p>Note</p> <p>This is spectacularly unnecessary and annoying. The only saving grace is that it's rarely necessary to use this field in lava. Filtering can be done by the AWS service itself in most cases (S3 or EventBridge).</p> <p>This is how lava populates the <code>info.event_type</code> value when Jinja rendering an s3trigger specification:</p> Source Value for event_type S3 The <code>eventName</code> field is used as-is. EventBridge The <code>detail-type</code> and <code>detail.reason</code> fields are joined with a colon and all whitespace removed. e.g. Putting a file in S3 will yield <code>ObjectCreated:PutObject</code>. <p>See the EventBridge message structure documentation for more information.</p>"},{"location":"05-job-dispatch.html#s3-event-deduplication","title":"S3 Event Deduplication","text":"<p>AWS does not guarantee that an individual S3 event will generate a single event notification. Duplicate event messages are very rare, except when AWS S3 bucket replication is configured which seems to generate multiple event notifications more often.</p> <p>Duplicate event notifications tend to occur within a small number of seconds of each other and can cause problems for lava jobs as two instances of a job operating on the same S3 object will be dispatched at almost the same time.</p> <p>The lava s3trigger lambda provides a level of support for reducing the risk of message duplication by, optionally, caching S3 object data for received notifications and discarding duplicates. The following attributes are compared to determine if an event notification is a duplicate:</p> <ul> <li> <p>bucket name</p> </li> <li> <p>object key</p> </li> <li> <p>object size</p> </li> <li> <p>event type.</p> </li> </ul> <p>The effectiveness of this mechanism is pretty good but it is limited by these factors:</p> <ul> <li> <p>The cache is time limited.</p> </li> <li> <p>The cache is size limited.</p> </li> <li> <p>As the cache is within the lambda itself, it relies on duplicate messages     being received by the same warm invocation instance of the s3trigger lambda.</p> </li> </ul> <p>The caching process is disabled by default and the cache configuration is configurable via the S3TRIGGER_DEDUP_CACHE_SIZE and S3TRIGGER_DEDUP_TTL parameters.</p> <p>If a more robust deduplication mechanism is required, it needs to be implemented outside lava (e.g. using a FIFO SQS queue to feed messages to s3trigger).</p> <p>Warning</p> <p>Be aware that the duplicate S3 event messages can have different message ID fields so some external logic would be required to explicitly set the SQS message deduplication ID if a FIFO queue is used. Are we having fun yet?</p>"},{"location":"05-job-dispatch.html#testing-s3-triggers","title":"Testing S3 Triggers","text":"<p>Testing an S3 trigger requires that the s3trigger Lambda function is invoked with a well-formed AWS S3 bucket notification event. There are essentially three ways to do this:</p> <ol> <li> <p>Configure S3 or EventBridge to send the notifications appropriately and then     drop the object of interest in S3. Repeating the test involves copying the     object back on top of itself.</p> <p>While this will work fine, it can be cumbersome to do manually, particularly if multiple objects are involved. It can also have unintended side effects if it triggers other unrelated actions.</p> </li> <li> <p>Construct an Amazon EventBridge message in the appropriate format and use the     AWS CLI to submit the message on the default event bus.</p> </li> <li> <p>Generate artificial bucket notification events and use those to invoke the     lambda directly.</p> <p>Lava comes with a simple shell script <code>etc/s3lambda.sh</code> to do this. Run <code>etc/s3lambda.sh -h</code> to get help.</p> </li> </ol>"},{"location":"05-job-dispatch.html#configuring-s3-bucket-notification-events","title":"Configuring S3 Bucket Notification Events","text":"<p>The s3trigger lambda function can receive S3 bucket notification events via any of the following mechanisms:</p> <ol> <li> <p>Direct subscription of the lambda function to the source bucket.</p> </li> <li> <p>Via SQS, where the source bucket has been configured to send event     notifications to an SQS queue.</p> </li> <li> <p>Via SNS, where the source bucket has been configured to send event     notifications to an SNS topic.</p> </li> </ol> <p>Tip</p> <p>In each case, it is strongly recommended to configure the trigger from the AWS Lambda console, not from the SNS/SQS/S3 console to avoid arcane IAM permission issues.</p>"},{"location":"05-job-dispatch.html#configuring-amazon-eventbridge-for-s3-events","title":"Configuring Amazon EventBridge for S3 Events","text":"<p>The process is basically this:</p> <ol> <li> <p>Configure the bucket to send notifications to Amazon EventBridge for all     events. All events on that bucket will be sent to the default event bus.</p> </li> <li> <p>Create a rule on the default event bus to capture events of interest.</p> <ul> <li>The rule name must be in the form <code>lava.&lt;REALM&gt;.*</code>.</li> <li>The event pattern should select S3 objects of interest.</li> <li>The target list must include the lambda <code>lava-&lt;REALM&gt;-s3trigger</code>.</li> <li>The rule input (i.e. what gets sent to the lambda) must be set to     Matched event.</li> </ul> </li> </ol> <p>An event pattern will look something like this:</p> <pre><code>{\n    \"source\": [\"aws.s3\"],\n    \"detail-type\": [\"Object Created\"],\n    \"detail\": {\n        \"bucket\": {\n            \"name\": [\"my-bucket\"]\n        },\n        \"object\": {\n            \"key\": [\n                { \"prefix\": \"an-interesting-prefix\" }\n            ]\n        }\n    }\n}\n</code></pre> <p>Tip</p> <p>The lava job framework provides built-in support for creating EventBridge rules suitable for triggering jobs from S3 events.</p> <p>More information:</p> <ul> <li> <p>Amazon S3 Event Notifications using EventBridge</p> </li> <li> <p>Amazon EventBridge event patterns</p> </li> </ul>"},{"location":"05-job-dispatch.html#dispatching-jobs-from-amazon-eventbridge","title":"Dispatching Jobs from Amazon EventBridge","text":"<p>Amazon EventBridge is an increasingly pervasive event management and distribution tool within AWS environments. It gathers events from a number of sources, selects events of interest based on user defined criteria and sends those events to one or more targets for response / processing.</p> <p>Events in an EventBridge event bus can be used to trigger the dispatch of lava jobs using the lava dispatch helper.</p> <p>The lava dispatch helper is a lambda function that that can receive dispatch requests via SQS and SNS. It can also handle appropriately formatted dispatch requests sent from EventBridge directly to the dispatch helper lambda function.</p> <p>The manual setup process is:</p> <ol> <li> <p>Go to the Amazon EventBridge console and create a new event rule.</p> </li> <li> <p>Fill in the event pattern / event bus settings as required.</p> </li> <li> <p>Under Select Targets, select the realm dispatch lambda function and     enable Input Transformer. This requires specification of an     Input Path and an Input Template.</p> </li> <li> <p>The Input Path selects fields from incoming event messages and makes     them available for injection into the message sent to the dispatch helper     lambda function. Use it to select whatever fields from the input event are     needed in the lava dispatch request.</p> </li> <li> <p>The Input Template is the message that actually gets sent to the     dispatch lambda. The message must be a JSON formatted object that complies     with the JSON dispatch request format.</p> </li> </ol> <p>This process is automated when EventBridge rules are created using the lava job framework.</p>"},{"location":"05-job-dispatch.html#example","title":"Example","text":"<p>As an example, consider a requirement to dispatch a lava job in response to an AWS EC2 auto scaling event, with the EC2 instance ID and auto scaling group name passed as globals to the lava job.</p> <p>The auto scaling event message in the event bus looks like this:</p> <pre><code>{\n  \"version\": \"0\",\n  \"id\": \"...\",\n  \"detail-type\": \"EC2 Instance Launch Successful\",\n  \"source\": \"aws.autoscaling\",\n  \"account\": \"123456789012\",\n  \"time\": \"2020-10-11T11:23:51Z\",\n  \"region\": \"ap-southeast-2\",\n  \"resources\": [\n    \"arn:aws:autoscaling:ap-southeast2:123456789012:autoScalingGroup:...\",\n    \"arn:aws:ec2:ap-southeast-2:123456789012:instance/i-0cefe067f7c6ee173\"\n  ],\n  \"detail\": {\n    \"StatusCode\": \"InProgress\",\n    \"AutoScalingGroupName\": \"myAutoScalingGroup\",\n    \"ActivityId\": \"...\",\n    \"Details\": {\n      \"Availability Zone\": \"ap-southeast-2a\",\n      \"Subnet ID\": \"subnet-745bae91\"\n    },\n    \"RequestId\": \"...\",\n    \"EndTime\": \"2020-10-11T11:23:51Z\",\n    \"EC2InstanceId\": \"i-0cefe067f7c6ee173\",\n    \"StartTime\": \"2020-10-11T11:16:43Z\",\n    \"Cause\": \"...\"\n  }\n}\n</code></pre> <p>As the lava job requires the name of the auto scaling group and the affected EC2 instance ID as job globals, the Input Path would look like this:</p> <pre><code>{\n  \"asg_name\": \"$.detail.AutoScalingGroupName\",\n  \"instance\": \"$.detail.EC2InstanceId\"\n}\n</code></pre> <p>The Input Template uses <code>&lt;input_path_var&gt;</code> to select values from the Input Path. The Input Template that constructs the dispatch request looks like this:</p> <pre><code>{\n  \"job_id\": \"my-job-id\",\n  \"globals\": {\n      \"instance\": \"&lt;instance&gt;\",\n      \"asg_name\": \"&lt;asg_name&gt;\"\n  }\n}\n</code></pre>"},{"location":"05-job-dispatch.html#testing-eventbridge-dispatch","title":"Testing EventBridge Dispatch","text":"<p>Testing of the configuration involves EventBridge receiving and processing an event that meets the selection criteria for the event rule. If it is not easy to have this occur naturally, it is straightforward to hand-craft a message and use the AWS CLI to send it.</p> <p>First create a JSON formatted file (e.g. <code>events.json</code>) containing the event(s). This only requires enough details to meet the EventBridge rule filtering criteria and to support the information required to create the message for the target.</p> <p>For example:</p> <pre><code>[\n  {\n    \"Source\": \"...\",\n    \"Detail\": \"{ \\\"AutoScalingGroupName\\\": \\\"...\\\", \\\"EC2InstanceId\\\": \\\"...\\\" }\",\n    \"Resources\": [\n      \"resource1\",\n      \"resource2\"\n    ],\n    \"DetailType\": \"myDetailType\"\n  }\n]\n</code></pre> <p>Note that <code>Detail</code> is a string containing a JSON encoded object.  Send the message, thus:</p> <pre><code>aws events put-events --entries file://events.json\n</code></pre>"},{"location":"05-job-dispatch.html#direct-dispatch","title":"Direct Dispatch","text":"<p>The mechanism underlying the dispatch process is the placement of an appropriately formatted message onto a specific worker fleet's SQS queue. Lava provides a number interfaces for this purpose.</p> <ul> <li> <p>The lava-dispatcher CLI utility.</p> </li> <li> <p>A Python interface.</p> </li> <li> <p>The lava GUI.</p> </li> </ul> <p>Warning</p> <p>Do not attempt to create your own dispatch messages for posting directly on the SQS worker queue. It will end in tears. Trust me on this.</p>"},{"location":"05-job-dispatch.html#python-dispatch-interface","title":"Python Dispatch Interface","text":"<p>The recommended way to generate a direct dispatch in Python is to use the lava libraries. Refer to the API documentation for more information.</p> <pre><code>from lava.lavacore import dispatch\n\nrun_id = dispatch(realm='...', job_id='...')\n</code></pre>"},{"location":"05-job-dispatch.html#handling-of-parameters-and-globals-during-dispatch","title":"Handling of Parameters and Globals During Dispatch","text":"<p>In addition to specifying the job to be dispatched, dispatch messages may also contain job parameter values and named global values. Values specified in the dispatch message will override any similarly named parameter or global in the job specification.</p> <p>This provides a mechanism for job specifications to be more generic, with specific values being supplied at run-time.</p>"},{"location":"05-job-dispatch.html#job-parameters","title":"Job Parameters","text":"<p>Job parameters are job type specific and must match the requirements of the job type. The parameters for a job are determined by the following order of precedence:</p> <ol> <li> <p>Parameters defined in the dispatch message.</p> </li> <li> <p>Parameters defined in the job specification.</p> </li> </ol>"},{"location":"05-job-dispatch.html#globals","title":"Globals","text":"<p>Globals are not job type specific and can be provided to any job type. Moreover, globals are passed to child jobs by the chain dispatch and  dag job types. Globals are also passed to downstream jobs initiated as a result of a dispatch post-job action.</p> <p>This then provides a mechanism for a job to receive generic global values when dispatched (e.g. by an S3 event) which are then made available to child/downstream jobs, irrespective of job type. Global values are made available to the Jinja rendering process for any job type that supports this capability. Refer to individual job types for more information.</p> <p>Global values for a job are determined by the following order of precedence  (highest to lowest):</p> <ol> <li> <p>Globals in the dispatch message.</p> </li> <li> <p>Globals in the dispatching job, if any.</p> </li> <li> <p>Globals in the job specification.</p> </li> </ol>"},{"location":"05-job-dispatch.html#globals-owned-by-lava","title":"Globals Owned by Lava","text":"<p>Global names within a job specification may not start with <code>lava</code> (case insensitive). This prefix is reserved for lava's use.</p> <p>Lava jobs have a concept of master job and parent job. For a simple, stand-alone job, the master job, the parent job and the current job are all one and the same. In a hierarchy of jobs formed when jobs start other jobs via chain jobs, dispatch jobs, dag jobs, or dispatch post-job actions, the master job is the initial job at the top of the hierarchy. The parent job is the job that caused the current job to run.</p> <p>For a single level hierarchy resulting from a chain job, the master and parent are the same. In a multi-level hierarchy, they will differ, as shown below.</p> <p></p> <p>Lava adds its own globals under <code>globals.lava</code> that capture information relating to the master and parent jobs.</p> Name Type Description foreach_index int For children of foreach jobs, this is the loop iteration counter, starting at zero. iteration int The iteration (run attempt) number for the job. See Job Retries for more information. master_job_id str The <code>job_id</code> of the master job. master_start datetime The local start time of the master job, including timezone. master_ustart datetime The UTC start time of the master job, including timezone. parent_job_id str The <code>job_id</code> of the parent job. parent_start datetime The local start time of the parent job, including timezone. parent_ustart datetime The UTC start time of the parent job, including timezone. <p>All jobs started from the same master job have access to the <code>job_id</code> of the master job and the local and UTC timestamps for when it started. These can be useful for constructing Jinja render values that are guaranteed to be consistent across all jobs started from a common master (e.g. for use in filenames).</p> <pre><code>{# This might be useful in a filename #}\n{{ globals.lava.master_ustart.strftime('%Y-%m-%d') }}\n</code></pre>"},{"location":"05-job-dispatch.html#use-with-event-triggered-dispatch","title":"Use with Event Triggered Dispatch","text":"<p>The s3triggers table permits the inclusion of globals which will be passed on to any jobs when dispatched. These will override similarly named globals in the job specification.</p>"},{"location":"06-jobs-job-types.html","title":"Jobs and Job Types","text":"<p>A lava job is a static definition represented as an entry in the jobs table. Each invocation of a job is referred to as a run which has a unique <code>run_id</code>.</p> <p>The <code>run_id</code> is used in construction of prefixes for job outputs in S3 as well as in event records in the events table.</p> <p>Various job types are supported, depending on the kind of processing required. Each job type has its own requirements for the <code>parameters</code> field in the job specification.</p> <p>Jobs can be run in either normal mode or in dev mode by specifying the <code>--dev</code> argument when invoking the lava worker. In dev mode, the behaviour of the job may by slightly altered to assist with job development and debugging without changing the fundamental action of the job. The specifics are job type dependent.</p> <p>If the DEBUG configuration variable is set to <code>true</code>, some additional information is added to the event records for failed jobs.</p>"},{"location":"06-jobs-job-types.html#job-families","title":"Job Families","text":"<p>Jobs types can be roughly grouped into the following families.</p> <ul> <li> <p>Orchestration jobs that control the operation of other jobs:</p> <ul> <li>chain</li> <li>dag</li> <li>dispatch</li> <li>foreach</li> </ul> </li> <li> <p>Database jobs that interact with a database to run SQL or load / unload     data:</p> <ul> <li>db_from_s3</li> <li>redshift_unload</li> <li>sql</li> <li>sqlc</li> <li>sqli</li> <li>sqlv</li> </ul> <p>See also Choosing an SQL Job Type.</p> </li> <li> <p>Executable jobs that run a code bundle external to lava itself:</p> <ul> <li>cmd</li> <li>docker</li> <li>exe</li> <li>pkg</li> </ul> </li> <li> <p>Integration jobs that interact with an external system of some kind:</p> <ul> <li>sharepoint_get_doc</li> <li>sharepoint_get_list</li> <li>sharepoint_get_multi_doc</li> <li>sharepoint_put_doc</li> <li>sharepoint_put_list</li> <li>smb_get</li> <li>smb_put</li> </ul> </li> <li> <p>Lava internal jobs that lava uses for it's own operations:</p> <ul> <li>lavasched</li> </ul> </li> <li> <p>Miscellaneous jobs that don't fit the above:</p> <ul> <li>log</li> </ul> </li> </ul>"},{"location":"06-jobs-job-types.html#job-payloads","title":"Job Payloads","text":"<p>Some job types have some kind of associated payload. Payloads are specified by the mandatory <code>payload</code> field in the job specification.</p> <p>Payloads are job type specific and can be of the following types:</p> <ul> <li> <p>S3 based payloads: The <code>payload</code> field will point to     location(s) in S3 that the lava worker will download at run-time. These are     typically things such as code bundles, SQL scripts etc.</p> </li> <li> <p>Inline payloads: The <code>payload</code> field will contain the actual payload.     These may be things like single CLI commands, small in-line SQL scripts,     docker repository names etc.</p> </li> <li> <p>No payload: Some job types (e.g. log) don't require a     payload and hence the value of the <code>payload</code> field is ignored. (As an     accident of history, the field is still required but it can be set to     anything. A value of <code>--</code> or <code>null</code> is common usage.)</p> </li> </ul>"},{"location":"06-jobs-job-types.html#s3-payloads","title":"S3 Payloads","text":"<p>Prior to version 7.1.0 (Pichincha), the S3 payload downloader was the v1 downloader.</p> <p>As of version 7.1.0 (Pichincha), there is an additional v2 downloader.</p> <p>Only one version of the downloader is active on any lava worker.</p> <p>Info</p> <p>As of version 8.1.0 (K\u012blauea), the v2 downloader is the default and the v1 downloader is deprecated.</p> <p>The v1 downloader can be enabled by setting the <code>PAYLOAD_DOWNLOADER</code> worker configuration parameter to <code>v1</code>. See Lava Worker Configuration for more details.</p>"},{"location":"06-jobs-job-types.html#the-v1-payload-downloader","title":"The v1 Payload Downloader","text":"<p>The v1 S3 payload downloader requires the <code>payload</code> field in the job specification to be a string that specifies a location in S3 relative to the <code>s3_payloads</code> area defined in the realms table. It can be either:</p> <ul> <li> <p>an S3 object key, in which case a single S3 object is downloaded; or</p> </li> <li> <p>an S3 prefix ending in <code>/</code>, in which case all objects under that prefix will   be downloaded and made available to the job in lexicographic order.</p> </li> </ul> <p>Info</p> <p>The download process when a prefix is specified does not recurse down the object hierarchy in S3. All objects to be downloaded must sit directly under that prefix. The job will abort with an error if there are any sub-prefixes.</p> <p>A typical job specification might contain something like this:</p> <pre><code>{\n  \"payload\": \"app/whatever/query-some-stuff.sql\"\n}\n</code></pre> <p>... or this:</p> <pre><code>{\n  \"payload\": \"app/whatever/lots-of-sqls/\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#the-v2-payload-downloader","title":"The v2 Payload Downloader","text":"<p>If the  <code>payload</code> field in the job specification is a string, the v2 S3 payload downloader behaves the same as the  v1 downloader, with one minor exception. If the string is a prefix containing sub-prefixes, the v1 downloader aborts with an error whereas the v2 downloader silently ignores the sub-prefixes.</p> <p>In v2, the <code>payload</code> field may also be a list of strings. These are processed in the specified order, using the same mechanism as the v1 downloader and the combined list of downloaded objects is made available to the job. The download process does not recurse into sub-prefixes which are silently ignored.</p> <p>Info</p> <p>Lava places the files resulting from each item in the list in a separate private directory to avoid name clashes. There is no way a payload file can know the location of any other payload file that isn't part of the same list item.</p>"},{"location":"06-jobs-job-types.html#job-retries","title":"Job Retries","text":"<p>By default, lava will make one attempt to run a job once dispatched and the job will then either succeed or fail.</p> <p>As of v6.1.0 (Volc\u00e1n Pinta), lava supports a job retry mechanism that can retry a failed job one or more times. This process is controlled by the following fields in the job specification:</p> <ul> <li> <p><code>iteration_limit</code>: specifies maximum number of attempts that will be made to     run the job successfully before giving up</p> </li> <li> <p><code>iteration_delay</code>: The delay between run attempts.</p> </li> </ul> <p>The job will be re-run until either it succeeds or the <code>iteration_limit</code> is reached. Lava makes an additional globals.lava.iteration global available for use with run-time Jinja rendering of job and action fields.</p> <p>A single worker thread remains committed to the job for the entirety of the retry process so it is important to use the retry mechanism sensibly.</p> <p>Info</p> <p>The entire duration of the retry process must fit within the queue visibility timeout of the worker SQS queue or else SQS itself will resubmit the dispatch message, with the same run ID, while the first job is still running. This is not ideal.</p>"},{"location":"06-jobs-job-types.html#job-actions-for-retries","title":"Job Actions for Retries","text":"<p>Lava makes the on_retry job action available for situations where a job fails but will be retried in another iteration.</p> <p>For example, if a job has an <code>iteration_count</code> of 2, and both attempts fail, the <code>on_retry</code> actions will be executed after the first iteration and the <code>on_fail</code> actions will be executed after the second and final iteration.</p>"},{"location":"06-jobs-job-types.html#lava-job-retries-iterations-vs-sqs-resubmissions","title":"Lava Job Retries (Iterations) vs SQS Resubmissions","text":"<p>The job specification contains both <code>iterations_*</code> related fields and a <code>max_tries</code> fields.</p> <p>The <code>iterations_*</code> related fields control internal job retry within the lava worker.</p> <p>The <code>max_tries</code> field controls how many times SQS is permitted to resubmit the same dispatch message due to SQS queue visibility timeouts before it is discarded by the lava worker.</p>"},{"location":"06-jobs-job-types.html#choosing-an-sql-job-type","title":"Choosing an SQL Job Type","text":"<p>Lava provides the following SQL jobs types:</p> <ul> <li> <p>sql</p> </li> <li> <p>sqlc</p> </li> <li> <p>sqli</p> </li> <li> <p>sqlv.</p> </li> </ul> <p>All of these will run blobs of SQL against a relational database using a lava database connector to connect to the target database. Each job type has unique characteristics that make it more or less suited for a given context.</p> <p>The following table compares the job types as well as the stand-alone lava-sql CLI utility:</p> Feature sql sqli sqlc sqlv lava-sql Runs inside the lava worker * * Runs outside the lava worker * * * Uniform, lava provided interface * * * * DB specific client * Timeout supported * * Can be killed on lava side * * * SQL in-line in job-spec * SQL in S3 * * * Lava transaction support * * * Jinja payload rendering * * * * Run multiple SQL statements * * * * * Suitable for large jobs * ** ** ** Lava CSV formatting control * * * * HTML output ? * Output column headers * * * * * Client side copy support ? Runs as a stand-alone utility * * <p><code>?</code> means support depends on database client capabilities.</p>"},{"location":"06-jobs-job-types.html#job-type-chain","title":"Job type: chain","text":"<p>The chain job type runs a list of other jobs sequentially. Unlike the dispatch job type, this is a synchronous operation. All of the jobs in the chain must be set to run on the same worker and will all run under the <code>run_id</code> of the parent job.</p> <p>It is possible to commence processing of a chain at an arbitrary point in the list. This is useful when it's necessary to resume a failed chain at a mid-point.</p> <p>Any globals available in the chain job will also be passed in the dispatch requests to the jobs in the chain.</p>"},{"location":"06-jobs-job-types.html#payload","title":"Payload","text":"<p>The <code>payload</code> is either a comma separated list of <code>job_id</code>s or an actual list of <code>job_id</code>s.</p>"},{"location":"06-jobs-job-types.html#parameters","title":"Parameters","text":"Parameter Type Required Description can_fail String or List[String] No A glob style pattern, or list of patterns specifying child jobs that are allowed to fail. See Allowing Child Jobs to Fail. job_prefix String No Prepend the specified value to each job ID in the payload, including any job specified by the <code>start</code> parameter. start String No The <code>job_id</code> of the starting point in the chain. The chain will commence with the first job with the given ID."},{"location":"06-jobs-job-types.html#allowing-child-jobs-to-fail","title":"Allowing Child Jobs to Fail","text":"<p>By default, the chain is aborted when any job in the list fails unless the <code>can_fail</code> parameter is specified, however jobs that are not enabled will be skipped and the chain will continue.</p> <p>The <code>can_fail</code> parameter can be set to a glob style pattern, or list of patterns.  A failed child job with a <code>job_id</code> that matches any of the patterns will not cause the parent chain to fail. In this situation, it is important that the child job handles its own on_fail actions, as the parent will not. Tough love.</p> <p>This tolerance of failure does not include configuration errors on child jobs, such as malformed job specifications, jobs sent to the wrong worker etc. These will still cause the entire chain to fail.</p> <p>Keep calm</p> <p>Before anyone gets all bitter and twisted about can vs may in <code>can_fail</code>...  both are essentially equivalent in this crazy, modern world. Look it up.</p>"},{"location":"06-jobs-job-types.html#handling-of-globals","title":"Handling of Globals","text":"<p>The chain job type merges its globals into those of the child jobs in the chain. A value specified in the parent chain job will override a similarly named value in the child.</p> <p>The chain job will also add lava specific globals under <code>globals.lava</code>. These lava owned globals allow all jobs in a chain, even a  multi-level chain, to access some common global values.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour","title":"Dev Mode Behaviour","text":"<p>The chain job behaviour is unchanged for dev mode. However, dev mode is propagated to jobs in the chain.</p>"},{"location":"06-jobs-job-types.html#examples","title":"Examples","text":"<p>A basic chain:</p> <pre><code>{\n  \"description\": \"Chain, chain, chain ... Chain of tools\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/chain-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {},\n  \"payload\": [\n    \"demo/job_01\",\n    \"demo/job_02\"\n  ],\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"chain\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>The chain with a different starting point:</p> <pre><code>{\n  \"description\": \"Chain, chain, chain ... Chain of tools\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/chain-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"start\": \"demo/job_02\"\n  },\n  \"payload\": [\n    \"demo/job_01\",\n    \"demo/job_02\"\n  ],\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"chain\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>With a common job prefix:</p> <pre><code>{\n  \"description\": \"Chain, chain, chain ... Chain of tools\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/chain-01\",\n  \"job_prefix\": \"demo/\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"start\": \"job_02\"\n  },\n  \"payload\": [\n    \"job_01\",\n    \"job_02\"\n  ],\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"chain\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>An uncaring parent chain job that doesn't care if any of its children fail:</p> <pre><code>{\n  \"description\": \"Chain, chain, chain ... Chain of tools\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/chain-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"can_fail\": \"*\"\n  },\n  \"payload\": [\n    \"demo/job_01\",\n    \"demo/job_02\"\n  ],\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"chain\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>A parent chain job that plays favourites:</p> <pre><code>{\n  \"description\": \"Chain, chain, chain ... Chain of tools\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/chain-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"can_fail\": [\n      \"*/job_[0-9][0-9]\"\n    ]\n  },\n  \"payload\": [\n    \"demo/job_01\",\n    \"demo/job_02\",\n    \"demo/black_sheep\"\n  ],\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"chain\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-cmd","title":"Job type: cmd","text":"<p>The cmd job type runs a single Linux command.</p>"},{"location":"06-jobs-job-types.html#payload_1","title":"Payload","text":"<p>The payload is the command string. This will be parsed using standard Linux shell lexical analysis to determine the executable and arguments. Additional arguments can also be specified with the <code>args</code> parameter.</p>"},{"location":"06-jobs-job-types.html#environment","title":"Environment","text":"<p>Stdin will be redirected from <code>/dev/null</code>. Stdout and stderr are captured and, if non-empty, placed into the realm <code>s3_temp</code> area with the following prefixes:</p> <ul> <li> <p>stdout: <code>&lt;s3_temp&gt;/&lt;job_id&gt;/&lt;run_id&gt;/stdout</code></p> </li> <li> <p>stderr: <code>&lt;s3_temp&gt;/&lt;job_id&gt;/&lt;run_id&gt;/stderr</code></p> </li> </ul> <p>The following variables are placed into the environment for the command.</p> Variable Description LAVA_JOB_ID The <code>job_id</code>. LAVA_OWNER The value of the <code>owner</code> field from the job specification. LAVA_REALM The realm name. LAVA_RUN_ID The <code>run_id</code> UUID. LAVA_S3_KEY The identifier for the KMS key needed to write data into the S3 temporary area. LAVA_S3_TMP The private S3 temporary area for this job run. The command is allowed to put data here. LAVA_WORKER The worker name."},{"location":"06-jobs-job-types.html#parameters_1","title":"Parameters","text":"Parameter Type Required Description args List[String] No A list of additional arguments for the command. env Map[String,String] No A map of additional environment variables for the command. jinja Boolean No If <code>false</code>, disable Jinja rendering of the payload. Default <code>true</code>. timeout String No By default, cmd jobs are killed after 10 minutes. This parameter can override that with values in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours). Note that this must be less than the visibility timeout on the worker SQS queue. vars Map[String,*] No A map of variables injected when the command arguments and environment are Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-payload-and-environment","title":"Jinja Rendering of the Payload and Environment","text":"<p>The collected arguments for the command and any environment values defined in the job specification are individually rendered using Jinja prior to execution.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_1","title":"Dev Mode Behaviour","text":"<p>Normally, the cmd job will copy stdout and stderr to S3 on the conclusion of the job. In dev mode, stdout and stderr are emitted locally during the job run instead of being copied to S3.</p>"},{"location":"06-jobs-job-types.html#examples_1","title":"Examples","text":"<p>The following example will list the contents of the S3 payloads area for this realm.</p> <pre><code>{\n  \"description\": \"List S3\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/list_s3\",\n  \"owner\": \"demo@somewhere.com\",\n  \"payload\": \"aws s3 ls {{realm.s3_payloads}}/ --recursive\",\n  \"type\": \"cmd\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This does the same thing but with the arguments supplied a little differently.</p> <pre><code>{\n  \"description\": \"List S3\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/list_s3\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"args\": [\n      \"--recursive\"\n    ]\n  },\n  \"payload\": \"aws s3 ls {{realm.s3_payloads}}/\",\n  \"type\": \"cmd\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-dag","title":"Job type: dag","text":"<p>The dag (Directed Acyclic Graph) job type runs a set of child jobs in the order defined by a dependency matrix.</p> <p>Ordering controlled by explicit dependencies will be honoured in that a child job which is a predecessor of another child job will always run to completion before the successor job starts.  Except for this, there is no guaranteed run order for the child jobs. Jobs that are not dependent on one another may run in any order, or in parallel.</p> <p>All of the jobs in the DAG must be set to run on the same worker and will all run under the <code>run_id</code> of the parent job. The jobs will be run in a dedicated transient thread pool that is separate from the main lava worker threads.</p> <p>Any globals available in the dag job will also be passed to the child jobs.</p>"},{"location":"06-jobs-job-types.html#payload_2","title":"Payload","text":"<p>The <code>payload</code> is a map containing elements in the form:</p> <pre><code>{\n  \"payload\": {\n    \"child_job_id_1\": \"predecessor_job_id\",\n    \"child_job_id_2\": [ List of predecessor job IDs ],\n    \"child_job_id_3\": null,\n    \"child_job_id_4\": []\n  }\n}\n</code></pre> <p>The keys are job IDs and values must be one of the following:</p> <ol> <li> <p>The ID of a predecessor job as a string.</p> </li> <li> <p>A list of predecessor job IDs.</p> </li> <li> <p>A <code>null</code> indicating the job must be run but has no predecessor requirements.</p> </li> <li> <p>An empty list, which also indicates the job must be run but has no     predecessor requirements.</p> </li> </ol> <p>Info</p> <p>Every job listed in the payload map, in either a key, or as an element in a predecessor list, will be run exactly once by the dag job.</p> <p>It is not necessary to include a separate key for a job if it is also present in a predecessor list and has no predecessors of its own, but it's harmless to do so.</p> <p>Note</p> <p>The lava-dag-gen utility generates a dag job payload from a dependency matrix. It can be used standalone or with the lava job framework.</p>"},{"location":"06-jobs-job-types.html#parameters_2","title":"Parameters","text":"Parameter Type Required Description can_fail String or List[String] No A glob style pattern, or list of patterns specifying child jobs that are allowed to fail. See Allowing Child Jobs to Fail. job_prefix String No Prepend the specified value to each job ID in the payload. workers Integer No The number of worker threads to use for running child jobs. The default is specified by the DAG_WORKERS worker configuration parameter and the maximum allowed value is specified by the DAG_MAX_WORKERS configuration parameter. <p>Tip</p> <p>Don't get carried away setting the <code>workers</code> parameter too high. It will impact memory consumption.</p>"},{"location":"06-jobs-job-types.html#allowing-child-jobs-to-fail_1","title":"Allowing Child Jobs to Fail","text":"<p>By default, the dag job is aborted when any child fails, however child jobs running in parallel with the failed job will generally run to completion.</p> <p>The <code>can_fail</code> parameter can be set to a glob style pattern, or list of patterns.  A failed child job with a <code>job_id</code> that matches any of the patterns will not cause the parent dag job, or dependent jobs, to fail. In this situation, it is important that the child job handles its own on_fail actions, as the parent will not. Tough love.</p> <p>This tolerance of failure does not include configuration errors on child jobs, such as malformed job specifications, jobs sent to the wrong worker etc. These will still cause the entire dag job to fail.</p>"},{"location":"06-jobs-job-types.html#handling-of-globals_1","title":"Handling of Globals","text":"<p>The dag job type merges its globals into those of the child jobs.  A value specified in the parent dag job will override a similarly named value in the child.</p> <p>The dag job will also add lava specific globals under <code>globals.lava</code>. These lava owned globals allow all child jobs in the dag to access some common global values.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_2","title":"Dev Mode Behaviour","text":"<p>The dag job behaviour is unchanged for dev mode. However, dev mode is propagated to child jobs.</p>"},{"location":"06-jobs-job-types.html#examples_2","title":"Examples","text":"<pre><code>{\n  \"description\": \"Daggy job\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/dag-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"workers\": 2\n  },\n  \"payload\": {\n    \"demo/job_01\": \"demo/job_02\",\n    \"demo/job_02\": [ \"demo/job_03\", \"demo/job_04\" ],\n    \"demo/job_05\": null\n  },\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"dag\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This version uses <code>job_prefix</code> and is functionally identical to the one above:</p> <pre><code>{\n  \"description\": \"Daggy job\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/dag-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"job_prefix\": \"demo/\",\n    \"workers\": 2\n  },\n  \"payload\": {\n    \"job_01\": \"job_02\",\n    \"job_02\": [ \"job_03\", \"job_04\" ],\n    \"job_05\": null\n  },\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"dag\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>An uncaring parent dag job that doesn't care if any of its children fail:</p> <pre><code>{\n  \"description\": \"Daggy job\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/dag-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"can_fail\": \"*\",\n    \"workers\": 2\n  },\n  \"payload\": {\n    \"demo/job_01\": \"demo/job_02\",\n    \"demo/job_02\": [ \"demo/job_03\", \"demo/job_04\" ],\n    \"demo/job_05\": null\n  },\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"dag\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>A parent dag job that plays favourites:</p> <pre><code>{\n  \"description\": \"Daggy job\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/dag-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"can_fail\": [\n      \"*/job_[24]\"\n    ],\n    \"workers\": 2\n  },\n  \"payload\": {\n    \"demo/job_01\": \"demo/job_02\",\n    \"demo/job_02\": [ \"demo/job_03\", \"demo/job_04\" ],\n    \"demo/job_05\": null\n  },\n  \"schedule\": \"0 0 * * *\",\n  \"type\": \"dag\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-db_from_s3","title":"Job type: db_from_s3","text":"<p>The db_from_s3 job type loads data from AWS S3 into a target database. The data formats supported, and the loading mechanism used, are dependent on the target database type. For example, Redshift databases use the <code>COPY</code> command to load structured data from S3.</p> <p>The target database type, and hence the loading mechanism, are determined by the type of database connector specified for the job.</p> <p>The db_from_s3 job type can be used in conjunction with the lava s3trigger Lambda function to load data whenever a new data file is placed in S3.</p> <p>The following target database types are supported:</p> <ul> <li> <p>AWS Redshift</p> </li> <li> <p>Postgres RDS and Aurora Postgres.</p> </li> <li> <p>Postgres</p> </li> <li> <p>Aurora MySQL</p> </li> <li> <p>SQLite3</p> </li> <li> <p>Oracle</p> </li> </ul>"},{"location":"06-jobs-job-types.html#payload_3","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_3","title":"Parameters","text":"Parameter Type Required Description args * * Dependent on the target database type. bucket String Yes The name of the S3 bucket from which the data is copied. columns List[String] Sometimes A list of SQL column specifications. Required if the target table must be created (e.g. if the table does not exist or the <code>mode</code> is <code>drop</code>). If specified, the copy process will explicitly include the columns specified. This is useful when only some of the columns in the target table are to be loaded with data. db_conn_id String Yes The connection ID for a target database. The <code>type</code> associated with the connection specification will determine which loader is used. jinja Boolean No If <code>false</code>, disable Jinja rendering of the <code>bucket</code> and <code>key</code>. Default <code>true</code>. key String Yes The key in the S3 bucket from which the data is copied. min_size String | Int No The loading process will attempt to avoid loading files below the specified size. The value is a size value as described in Lava Worker Configuration. The default is 0. See below for more information. mode String Yes Update mode for the target table. See Copy Modes for more information. s3_conn_id String Conditional The connection ID for AWS S3. This is used to allow access to the source data in S3. Whether or not this is required is database dependent. Only one of either <code>s3_conn_id</code> or <code>s3_iam_role</code> is ever required. s3_iam_role String Conditional The IAM role name used to allow access to the source data in S3. Whether or not this is required is database dependent. Only one of either <code>s3_conn_id</code> or <code>s3_iam_role</code> is ever required. schema String Yes Target schema name. If the Jinja rendered value starts with <code>/</code>, the rest of the value is used as a regular expression (regex) to derive the schema name from the Jinja rendered <code>key</code> value. In this case, the <code>columns</code> argument is not permitted and the target table must already exist. skip_missing Boolean No If <code>true</code>, no attempt is made to load missing files. The default is <code>false</code> which means attempting to load a non-existent file will result in an error.  This won't help in the situation where the file is removed from S3 between the check for existence and the database load operation. Note that the lava worker will require <code>s3:GetObject</code> access to the object if the value is set to <code>true</code>. table String Yes Target table name. If the Jinja rendered value starts with <code>/</code>, the rest of the value is used as a regular expression (regex) to derive the table name from the <code>key</code> argument. In this case, the <code>columns</code> argument is not permitted and the target table must already exist. vars Map[String,*] No A map of variables injected when <code>bucket</code>, <code>key</code>, <code>schema</code> and <code>table</code> are Jinja rendered."},{"location":"06-jobs-job-types.html#about-the-min_size-parameter","title":"About the min_size Parameter","text":"<p>If the <code>min_size</code> parameter is specified, the loading process will attempt to avoid loading files below the specified size. This is not always possible, depending on the loading mechanism being used.</p> <p>The fundamental use case for this parameter is to avoid attempting to load empty data files. Most of the database loading mechanisms will happily load an empty file and consider that to be a success. Some, such as the native S3 loading process for Aurora Postgres, consider this to be an error and will abort the transaction. This is particularly problematic when loading a number of files in one operation, some of which may be empty. This can happen with data unloaded from Redshift where the source table is skewed and some slice files are empty as a result.</p> <p>The file size is checked directly in S3 without downloading the object to the lava worker. This means that for compressed files, the value applies to the file before decompression. It also means that the lava worker must have read access (<code>s3:GetObject</code>) to objects to obtain size information, even if the loading process is performed directly from the database.</p> <p>Unfortunately, an empty file will GZIP to a non-empty file of small but uncertain length due to the embedding of the filename. To avoid empty GZIP files, the value should be set to a small but non-zero value (e.g. 100).</p> <p>For uncompressed data without headers, setting the value to <code>1</code> should avoid empty files.</p>"},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-bucket-key-schema-and-table","title":"Jinja Rendering of the Bucket, Key, Schema and Table","text":"<p>The <code>bucket</code>, <code>key</code>, <code>schema</code> and <code>table</code> parameters are rendered using Jinja prior to further use. The rendering is done in the following order:</p> <ol> <li>The <code>bucket</code> and <code>key</code> parameters are rendered.</li> <li>The <code>schema</code> and <code>table</code> parameters are rendered.</li> </ol> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>. <p>The following additional variables are made available when rendering the <code>schema</code> and <code>table</code> parameters:</p> Name Type Description bucket str The Jinja rendered value of the <code>bucket</code> parameter. key str The Jinja rendered value of the <code>key</code> parameter. <p>This two-step process provides an additional mechanism for deriving the target database object name from the S3 object name. It complements the regex based mechanism described below.</p> <p>Why two mechanisms for deriving schema and table from the S3 key?</p> <p>Well, the original regex mechanism was a legacy of previous code and is still supported for historical reasons. The newer Jinja based method is more flexible and consistent with lava generally. Try not to use them both at once. It probably won't end well.</p>"},{"location":"06-jobs-job-types.html#deriving-target-schema-and-table-name-from-the-s3-key","title":"Deriving Target Schema and Table Name from the S3 Key","text":"<p>The regex options for <code>schema</code> and <code>table</code> provide an alternate mechanism to allow the target table to be derived at run time from the S3 object name. This allows a single job specification to be used for multiple tables provided an appropriate S3 naming convention is used. This is particularly handy if the job is dispatched by an S3 object created event. In this configuration, the <code>bucket</code> and <code>key</code> parameters would be supplied when the job is dispatched as a result of an S3 bucket notification event.</p> <p>For each of these arguments, if the value starts with <code>/</code>, the rest of the value must be a regular expression with a single capture group. The regex is applied to the S3 object key that triggered the event (minus any suffix) and the value of the capture group is used as the schema or table name respectively. If the regex option is used, the <code>columns</code> argument is not permitted and the target schema and table must already exist.</p> <p>By combining these mechanisms, it is possible to use a single job specification to load any number of different tables into a given target database provided the target schema and table name can be derived from the S3 object key using regular expressions.</p>"},{"location":"06-jobs-job-types.html#copy-modes","title":"Copy Modes","text":"<p>The <code>mode</code> argument  specifies how any existing data in the table should be handled and how new data should be loaded.</p> Mode Description abort If the target table contains any data, then abort with an error. This prevents the loss of any existing data. append New data is appended to any existing data in the table. drop The table is dropped and recreated prior to loading. This requires the <code>columns</code> argument to be provided so lava knows how to create the table. delete Delete all existing data from the table. switch Perform A/B table switching. If the <code>table</code> argument specifies a target table of <code>mytable</code>, data will be alternately loaded into switch tables <code>mytable_a</code> and <code>mytable_b</code>. During the load phase, lava will find whichever switch table is empty and load the new data into it. It will then delete all data from the other table. All this is done inside a transaction. Once the transaction is committed, the empty table is truncated to minimise vacuuming requirements. If neither switch table is empty, the load is aborted. Generally, there will be a view on top of the switch tables presenting the union of the two. truncate Truncate the table before loading."},{"location":"06-jobs-job-types.html#event-driven-loading","title":"Event Driven Loading","text":"<p>A common usage pattern is to trigger a database load when a data file is created in S3. Lava supports this via the s3trigger lambda function.</p> <p>To load a table in response to an S3 bucket notification event would require an entry in the s3triggers table like so:</p> <pre><code>{\n  \"description\": \"A new custard club membership file appeared in S3\",\n  \"bucket\": \"my-bucket\",\n  \"prefix\": \"a/b\",\n  \"enabled\": true,\n  \"job_id\": \"demo/copy-mytable\",\n  \"parameters\": {\n    \"bucket\": \"{{bucket}}\",\n    \"key\": \"{{key}}\"\n   }\n}\n</code></pre> <p>The <code>bucket</code> and <code>key</code> parameters are then determined at run-time and merged into the job specification.</p> <p>When a new data file is placed in S3, the lava s3trigger Lambda function will be invoked. It will look up the event source (bucket + object key) in the s3triggers table to determine that it should invoke the <code>demo/copy-mytable</code> job. It will use Jinja to render the parameters shown above from the bucket event notification details and then dispatch the job with the customised <code>bucket</code> and <code>key</code> parameters replacing any value in the job specification.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_3","title":"Dev Mode Behaviour","text":"<p>Normally, the db_from_s3 job will copy a log of events to S3 on the conclusion of the job. In dev mode, this log is emitted locally at the end of the job run.</p>"},{"location":"06-jobs-job-types.html#moving-data-from-redshift-to-other-databases","title":"Moving Data from Redshift to Other Databases","text":"<p>Unfortunately, there is little consistency of data interchange formats or methods across the AWS database services.</p> <p>For example:</p> <ul> <li> <p>Data unloaded from Redshift cannot be reliably copied back to Redshift     without special handling of date fields.</p> </li> <li> <p>Data unloaded from Redshift as pipe separated values (the default), cannot     be reliably loaded to Postgres, Postgres RDS or Aurora Postgres because of     differences in escaping the pipe character. CSV format seems to work.</p> </li> <li> <p>Redshift unloading works best with compression and parallel unloading     enabled. Aurora MySQL supports a manifest but does not support loading of     compressed data. Postgres RDS and Aurora Postgres don't support loading from     a manifest. They do support loading compressed data but only if the content     encoding is set correctly on the S3 objects. Redshift does not do this.</p> </li> <li> <p>The syntax and capability of data loading commands varies widely from     Redshift to Aurora MySQL to Aurora/RDS Postgres to standard Postgres.</p> </li> </ul> <p>Frankly, it's a mess.</p> <p>Lava can only compensate for these problems to a limited degree. For example, the redshift_unload job has additional logic to work around the first problem above. The db_from_s3 job uses a range of background trickery (such as manifest handling) to partially compensate for some of the other limitations and inconsistencies.</p> <p>Be careful out there!</p>"},{"location":"06-jobs-job-types.html#loading-data-to-aurora-mysql","title":"Loading Data to Aurora MySQL","text":"<p>Loading to Amazon Aurora MySQL is done using the native LOAD DATA FROM S3 facility.</p> <p>There are a number of preliminary setup steps required on the database itself to enable this mechanism. Once enabled, it can load uncompressed data. Loading of compressed data is not supported.</p>"},{"location":"06-jobs-job-types.html#aurora-mysql-specific-parameters","title":"Aurora MySQL Specific Parameters","text":"<p>The <code>db_conn_id</code> parameter must point to a connection with a <code>type</code> of mysql-aurora.</p> <p>The loading operation relies on pre-configuration on the cluster itself to provide access to S3. The <code>s3_conn_id</code> and <code>s3_iam_role</code> parameters are not used.</p> <p>Note</p> <p>The syntax of the LOAD DATA FROM S3 command is very different from its Postgres and Redshift counterparts. Lava attempts to reduce the apparent differences by adopting some of the Redshift / Postgres conventions in the <code>args</code> parameter. Whether this is helpful or dangerous folly is in the eye of the beholder.</p> <p>The <code>args</code> parameter must contain a list of  Aurora MySQL LOAD DATA command options. See also the standard MySQL LOAD DATA reference. Only the following options are supported:</p> <ul> <li>FILE</li> <li>IGNORE</li> <li>MANIFEST</li> <li>PARTITION</li> <li>PREFIX</li> <li>REPLACE</li> </ul> <p>In addition, the following non-standard options are supported:</p> Option Description DELIMITER Takes an argument of the form <code>'string'</code> specifying the column delimiter. ENCODING Takes an argument specifying the character set of the data. ESCAPE Takes an argument of the form <code>'char'</code> specifying the character used to escape quote characters. HEADER Takes an optional integer argument specifying the number of header lines to skip. The default value is <code>1</code>. QUOTE Takes arguments of the form <code>[OPTIONAL] 'char'</code> specifying the character used to quote data fields. If the <code>OPTIONAL</code> keyword is present, only fields with a string type are quoted. If omitted, all fields are quoted. TERMINATOR Takes an argument in the form <code>DOS | UNIX | 'char'</code>. The <code>DOS</code> and <code>UNIX</code> options set the line terminator appropriate for data sourced from those systems."},{"location":"06-jobs-job-types.html#aurora-mysql-examples","title":"Aurora MySQL Examples","text":"<p>The following example shows loading of a data file to a table in <code>switch</code> mode. The tables <code>membership_a</code> and <code>membership_b</code> will be created with the given column specifications if they don't already exist.</p> <pre><code>{\n  \"description\": \"Copy data to the table 'custardclub.membership'\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/copy-mytable\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"db_conn_id\": \"mysql-aurora-conn-01\",\n    \"bucket\": \"my-bucket\",\n    \"key\": \"a/b/data.csv\",\n    \"schema\": \"custardclub\",\n    \"table\": \"membership\",\n    \"mode\": \"switch\",\n    \"args\": [\n      \"DELIMITER ','\",\n      \"HEADER\",\n      \"ENCODING utf8\",\n      \"QUOTE OPTIONAL '\\\"'\",\n      \"TERMINATOR UNIX\"\n    ],\n    \"columns\": [\n      \"CustardClubNo INTEGER PRIMARY KEY NOT NULL\",\n      \"GivenName VARCHAR(20)\",\n      \"FamilyName VARCHAR(30)\",\n      \"CustardBidPrice FLOAT(2)\",\n      \"CustardJedi BOOLEAN\",\n      \"LastCustard DATE\",\n      \"CustardQuota INTEGER\"\n    ]\n  },\n  \"payload\": \"--\",\n  \"type\": \"db_from_s3\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#loading-data-to-aurora-postgres","title":"Loading Data to Aurora Postgres","text":"<p>See Loading Data to Postgres RDS.</p>"},{"location":"06-jobs-job-types.html#loading-data-to-oracle","title":"Loading Data to Oracle","text":"<p>Loading to Oracle is done by lava performing direct data insertion. The files to be loaded are copied from S3 to the lava worker node, decompressed on the fly if required, and then inserted row by row into the database by lava.</p> <p>Warning</p> <p>If using <code>TRUNCATE</code> or <code>DROP</code> mode, a <code>COMMIT</code> is done after the <code>TRUNCATE</code> / <code>DROP</code> but prior to loading of new data.</p>"},{"location":"06-jobs-job-types.html#oracle-specific-parameters","title":"Oracle Specific Parameters","text":"<p>The <code>db_conn_id</code> parameter must point to a connection with a <code>type</code> of oracle.</p> <p>The <code>args</code> parameter supports the following options. The CSV format related options are linked directly to the Python <code>csv</code> module formatting parameters.</p> Option Description DELIMITER 'c' As for the Python <code>csv</code> module formatting parameters. DOUBLEQUOTE Controls how instances of <code>QUOTECHAR</code> appearing inside a field should themselves be quoted. If the <code>DOUBLEQUOTE</code> option is present, the character is doubled. If not present, the <code>ESCAPECHAR</code> is used as a prefix to the <code>QUOTECHAR</code>. ESCAPECHAR 'c' As for the Python <code>csv</code> module formatting parameters. GZIP Source data is compressed using gzip. The worker will decompress it on the fly for insertion. HEADER Ignore the first line in each data file. MANIFEST The S3 object will be treated as a Redshift compatible manifest containing a list of actual data files to load. QUOTECHAR 'c' As for the Python <code>csv</code> module formatting parameters. QUOTING 'style' As for the Python <code>csv</code> module QUOTE_* parameters (without the <code>QUOTE_</code> prefix). Default is <code>minimal</code> (i.e. <code>QUOTE_MINIMAL</code>). Case is not significant."},{"location":"06-jobs-job-types.html#loading-data-to-postgres","title":"Loading Data to Postgres","text":"<p>Loading to Postgres is done using a client side COPY. The files to be loaded are copied from S3 to the lava worker node, decompressed if required, and then copied to the database via the client.</p> <p>While this will work for Postgres RDS/Aurora, it will be much slower than a direct S3 copy as described in Loading Data to Postgres RDS.</p> <p>Moreover, if multiple files are to be loaded (e.g. when using a manifest), the files are processed sequentially. This can make the whole operation rather slow. If large amounts of data need to be loaded, it is recommended to implement a custom loading process.</p> <p>Loading of each individual file is subject to a timeout which is set via the PG_COPY_TIMEOUT configuration variable. This can be set at the worker or realm level.</p> <p>Warning</p> <p>If using <code>TRUNCATE</code> or <code>DROP</code> mode, a <code>COMMIT</code> is done after the <code>TRUNCATE</code> / <code>DROP</code> but prior to loading of new data.</p>"},{"location":"06-jobs-job-types.html#postgres-specific-parameters","title":"Postgres Specific Parameters","text":"<p>The <code>db_conn_id</code> parameter must point to a connection with a <code>type</code> of postgres or psql.</p> <p>Use with postgres-aurora or postgres-rds will work but is not recommended.</p> <p>The worker uses its own credentials to copy the files from S3 to local storage. The <code>s3_conn_id</code> and <code>s3_iam_role</code> parameters are not used.</p> <p>The <code>args</code> parameter must contain a list Postgres COPY options. </p> <p>The following options are supported:</p> <ul> <li>DELIMITER</li> <li>ENCODING</li> <li>ESCAPE</li> <li>FORCE_NOT_NULL (see below)</li> <li>FORCE_NULL (see below)</li> <li>FORMAT</li> <li>FREEZE</li> <li>HEADER</li> <li>NULL</li> <li>OIDS</li> <li>QUOTE</li> </ul> <p>In addition, the following non-standard options are supported:</p> Option Description FORCE_NOT_NULL As for <code>FORCE_NULL</code>. FORCE_NULL This standard option accepts a non-standard <code>*</code> argument which lava will replace with a list of all columns in the target table. GZIP Source data is compressed using gzip. The worker will decompress it prior to loading. MANIFEST The S3 object will be treated as a Redshift compatible manifest containing a list of actual data files to load."},{"location":"06-jobs-job-types.html#loading-data-to-postgres-rds","title":"Loading Data to Postgres RDS","text":"<p>Loading to Postgres RDS and Aurora Postgres is done using the native aws_s3.table_import_from_s3 facility. This is supported in the following versions:</p> <ul> <li>AWS RDS PostgreSQL versions 11.1+</li> <li>AWS RDS Aurora PostgreSQL-compatible versions 10.7+</li> </ul> <p>There are a number of preliminary setup steps required on the database itself to enable this mechanism. Once enabled, it can load uncompressed data and gzip compressed data.</p>"},{"location":"06-jobs-job-types.html#postgres-rds-specific-parameters","title":"Postgres RDS Specific Parameters","text":"<p>The <code>db_conn_id</code> parameter must point to a connection with a <code>type</code> of either postgres-aurora or postgres-rds.</p> <p>If the <code>s3_conn_id</code> parameter is specified, the relevant connector will be used to provide credentials to access S3. If the parameter is not specified, the database must have an attached IAM role that provides the required access to S3. As this is implicit, there is no need to specify the <code>s3_conn_id</code> parameter.</p> <p>The <code>args</code> parameter must contain a list of Postgres COPY command options appropriate for the data file.</p> <p>The following options are supported:</p> <ul> <li>DELIMITER</li> <li>ENCODING</li> <li>ESCAPE</li> <li>FORCE_NOT_NULL (see below)</li> <li>FORCE_NULL (see below)</li> <li>FORMAT</li> <li>FREEZE</li> <li>HEADER</li> <li>NULL</li> <li>OIDS</li> <li>QUOTE</li> </ul> <p>In addition, the following non-standard options are supported:</p> Option Description FORCE_NOT_NULL As for <code>FORCE_NULL</code>. FORCE_NULL This standard option accepts a non-standard <code>*</code> argument which lava will replace with a list of all columns in the target table. GZIP The Postgres copy operation can load gzip compressed data but the S3 object being loaded must have its <code>Content-Encoding</code> metadata set to <code>gzip</code>. If this option is specified, lava will attempt to set the content encoding appropriately if necessary. Be aware that this is, in fact, an object copy operation with new metadata so the worker will require appropriate IAM permissions to do that. MANIFEST The S3 object will be treated as a Redshift compatible manifest containing a list of actual data files to load. <p>Info</p> <p>When configuring a bucket event notification for s3trigger to dispatch a <code>db_from_s3</code> job using GZIP compression, do not  trigger on All object create events<code>as this may cause two events to be sent and the job will run twice. Restrict the event type to</code>ObjectCreated:Put` only. This is due to the hidden copy operation lava must perform to set the correct content encoding on the source S3 object.</p>"},{"location":"06-jobs-job-types.html#enabling-the-postgres-rds-copy-extension","title":"Enabling the Postgres RDS Copy Extension","text":"<p>To enable the aws_s3.table_import_from_s3 copy extension follow the AWS instructions.</p> <p>It is also necessary to allow the database users who will be performing copy operations to access the extension, thus:</p> <pre><code>GRANT USAGE ON SCHEMA aws_s3 TO &lt;USER&gt;;\nGRANT EXECUTE ON FUNCTION aws_s3.table_import_from_s3\n    (\n        table_name text,\n        column_list text,\n        options text,\n        bucket text,\n        file_path text,\n        region text,\n        access_key text,\n        secret_key text,\n        session_token text\n        )\n    TO &lt;USER&gt;;\n</code></pre>"},{"location":"06-jobs-job-types.html#postgres-rdsaurora-examples","title":"Postgres RDS/Aurora Examples","text":"<p>The following example shows loading of a data file to a table in <code>switch</code> mode. The tables <code>membership_a</code> and <code>membership_b</code> will be created with the given column specifications if they don't already exist.</p> <pre><code>{\n  \"description\": \"Copy data to the table 'custardclub.membership'\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/copy-mytable\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"db_conn_id\": \"pg-rds-conn-01\",\n    \"s3_conn_id\": \"s3-conn-01\",\n    \"bucket\": \"my-bucket\",\n    \"key\": \"a/b/data.csv\",\n    \"schema\": \"custardclub\",\n    \"table\": \"membership\",\n    \"mode\": \"truncate\",\n    \"args\": [\n      \"FORMAT CSV\",\n      \"HEADER\"\n    ],\n    \"columns\": [\n      \"CustardClubNo INTEGER PRIMARY KEY NOT NULL\",\n      \"GivenName VARCHAR(20)\",\n      \"FamilyName VARCHAR(30)\",\n      \"CustardBidPrice FLOAT(2)\",\n      \"CustardJedi BOOLEAN\",\n      \"LastCustard DATE\",\n      \"CustardQuota INTEGER\"\n    ]\n  },\n  \"payload\": \"--\",\n  \"type\": \"db_from_s3\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#loading-data-to-redshift","title":"Loading Data to Redshift","text":"<p>Loading to Redshift is done using the native Redshift COPY command. Data files contain structured, tabular data. All of the formats supported by the COPY command can be used, including:</p> <ul> <li>CSV files</li> <li>JSON files</li> <li>Compressed versions of the above.</li> </ul> <p>For Redshift, it is generally better to use <code>truncate</code> mode rather than <code>delete</code>.</p>"},{"location":"06-jobs-job-types.html#redshift-specific-parameters","title":"Redshift Specific Parameters","text":"<p>The <code>db_conn_id</code> parameter must point to a connection with a <code>type</code> of redshift or redshift-serverless.</p> <p>The <code>args</code> parameter must contain a list of Redshift COPY options appropriate for the data file. Most of the parameters supported by Redshift COPY, including <code>MANIFEST</code>, can be used.</p>"},{"location":"06-jobs-job-types.html#redshift-examples","title":"Redshift Examples","text":"<p>The following example shows loading of a data file to a table in <code>append</code> mode. The table will be created with the given column specifications if it doesn't already exist.</p> <pre><code>{\n  \"description\": \"Copy data to the table 'custardclub.membership'\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/copy-mytable\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"db_conn_id\": \"redshift-conn-01\",\n    \"s3_conn_id\": \"s3-conn-01\",\n    \"bucket\": \"my-bucket\",\n    \"key\": \"a/b/data.csv.bz2\",\n    \"schema\": \"custardclub\",\n    \"table\": \"membership\",\n    \"mode\": \"truncate\",\n    \"args\": [\n      \"BLANKSASNULL\",\n      \"BZIP2\",\n      \"CSV\",\n      \"EMPTYASNULL\",\n      \"IGNOREHEADER 1\",\n      \"MAXERROR 0\",\n    ],\n    \"columns\": [\n      \"CustardClubNo INTEGER PRIMARY KEY NOT NULL\",\n      \"GivenName VARCHAR(20)\",\n      \"FamilyName VARCHAR(30)\",\n      \"CustardBidPrice FLOAT(2)\",\n      \"CustardJedi BOOLEAN\",\n      \"LastCustard DATE\",\n      \"CustardQuota INTEGER\"\n    ]\n  },\n  \"payload\": \"--\",\n  \"type\": \"db_from_s3\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#loading-data-to-sqlite3","title":"Loading Data to SQLite3","text":"<p>Loading to SQLite is done by lava performing direct data insertion. The files to be loaded are copied from S3 to the lava worker node, decompressed on the fly if required, and then inserted row by row into the database by lava.</p> <p>Warning</p> <p>If using <code>TRUNCATE</code> or <code>DROP</code> mode, a <code>COMMIT</code> is done after the <code>TRUNCATE</code> / <code>DROP</code> but prior to loading of new data.</p>"},{"location":"06-jobs-job-types.html#sqlite3-specific-parameters","title":"SQLite3 Specific Parameters","text":"<p>The <code>db_conn_id</code> parameter must point to a connection with a <code>type</code> of sqlite3.</p> <p>The <code>schema</code> parameter must be set to <code>main</code> as SQLite3 has a very limited notion of schemas.</p> <p>The <code>args</code> parameter supports the following options. The CSV format related options are linked directly to the Python <code>csv</code> module formatting parameters.</p> Option Description DELIMITER 'c' As for the Python <code>csv</code> module formatting parameters. DOUBLEQUOTE Controls how instances of <code>QUOTECHAR</code> appearing inside a field should themselves be quoted. If the <code>DOUBLEQUOTE</code> option is present, the character is doubled. If not present, the <code>ESCAPECHAR</code> is used as a prefix to the <code>QUOTECHAR</code>. ESCAPECHAR 'c' As for the Python <code>csv</code> module formatting parameters. GZIP Source data is compressed using gzip. The worker will decompress it on the fly for insertion. HEADER Ignore the first line in each data file. MANIFEST The S3 object will be treated as a Redshift compatible manifest containing a list of actual data files to load. QUOTECHAR 'c' As for the Python <code>csv</code> module formatting parameters. QUOTING 'style' As for the Python <code>csv</code> module QUOTE_* parameters (without the <code>QUOTE_</code> prefix). Default is <code>minimal</code> (i.e. <code>QUOTE_MINIMAL</code>). Case is not significant."},{"location":"06-jobs-job-types.html#job-type-dispatch","title":"Job type: dispatch","text":"<p>The dispatch job type initiates a dispatch of other jobs.</p> <p>Unlike the chain job type, this is an asynchronous operation in that the dispatch is initiated but the job run does not wait for the dispatched jobs to actually start. The dispatched jobs will not necessarily run on the same worker and will each have their own <code>run_id</code>.</p> <p>Any globals available in the dispatch job will also be passed in the dispatch requests to the jobs being dispatched.</p>"},{"location":"06-jobs-job-types.html#payload_4","title":"Payload","text":"<p>The payload is a comma separated list, or an actual list of job IDs to dispatch.</p>"},{"location":"06-jobs-job-types.html#parameters_4","title":"Parameters","text":"Parameter Type Required Description delay String No Dispatch message sending delay in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds) or <code>m</code> (minutes). The maximum allowed value is 15 minutes. jinja Boolean No If <code>false</code>, disable Jinja rendering of the payload. Default <code>true</code>. job_prefix String No Prepend the specified value to each job ID in the payload. parameters Map[String,*] No A map of parameters that will be passed to the job being dispatched. This is Jinja rendered."},{"location":"06-jobs-job-types.html#handling-of-globals_2","title":"Handling of Globals","text":"<p>The dispatch job type merges its globals into those of the child jobs being dispatched. A value specified in the parent dispatch job will override a similarly named value in the child.</p> <p>In addition, the dispatch job will also add lava specific globals under <code>globals.lava</code>. These lava owned globals allow all jobs dispatched as a result of the current job to access some common global values.</p>"},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-payload","title":"Jinja Rendering of the Payload","text":"<p>The <code>parameters</code> parameter is Jinja rendered.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_4","title":"Dev Mode Behaviour","text":"<p>The dispatch job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_3","title":"Examples","text":"<p>The following example dispatches two downstream jobs with no delay.</p> <pre><code>{\n  \"description\": \"Dispatch two downstream jobs\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/dispatch2\",\n  \"owner\": \"demo@somewhere.com\",\n  \"payload\": \"demo/job_01, demo/job_02\",\n  \"type\": \"dispatch\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one dispatches a downstream job with a 5 minute delay and some additional parameters being passed to the dispatched job. Jinja rendering is disabled.</p> <pre><code>{\n  \"description\": \"Dispatch a downstream job\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/dispatch1\",\n  \"owner\": \"demo@somewhere.com\",\n  \"payload\": \"demo/job_01, demo/job_02\",\n  \"parameters\": {\n    \"delay\": \"5m\",\n    \"jinja\": false,\n    \"parameters\": {\n      \"param01\": \"Hello\",\n      \"param02\": \"world\"\n    }\n  },\n  \"type\": \"dispatch\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-docker","title":"Job type: docker","text":"<p>The docker job type launches a docker container on the worker node and runs the nominated command. Refer to the chapter on docker for more information.</p> <p>Lava will connect to a docker registry, pull the specified image from a repository and use it to create and run a container with the specified command. If no command is specified, the default entry point for the container will be used.</p> <p>The logs from the container will be captured and, if non-empty, placed into the realm <code>s3_temp</code> area as <code>&lt;s3_temp&gt;/&lt;job_id&gt;/&lt;run_id&gt;/logs</code>.</p> <p>If the command returns a zero exit status the job is considered to have succeeded. A non-zero exit status indicates failure.</p>"},{"location":"06-jobs-job-types.html#payload_5","title":"Payload","text":"<p>The payload is the container repository and, optionally, tag in the form <code>repository[:tag]</code>. If the tag is not specified, a tag of <code>latest</code> is used.</p>"},{"location":"06-jobs-job-types.html#environment_1","title":"Environment","text":"<p>The following variables are placed into the environment for the container.</p> Variable Description LAVA_JOB_ID The <code>job_id</code>. LAVA_OWNER The value of the <code>owner</code> field from the job specification. LAVA_REALM The realm name. LAVA_RUN_ID The <code>run_id</code> UUID. LAVA_S3_KEY The identifier for the KMS key needed to write data into the S3 temporary area. LAVA_S3_PAYLOAD The payload location for this job. LAVA_S3_TMP The private S3 temporary area for this job run. The executables are allowed to put data here. LAVA_TMP The temporary directory on the host worker node is mapped into the container at this location. LAVA_WORKER The worker name."},{"location":"06-jobs-job-types.html#docker-runtime-configuration","title":"Docker Runtime Configuration","text":"<p>The container is run with the following run-time attributes:</p> <ol> <li> <p>Environment variables as described above.</p> </li> <li> <p>The local temporary area on the worker node is mapped inside the container     and its location is made available as the <code>LAVA_TMP</code> environment variable.     The container can read and write this directory as for other lava jobs. This     can be useful for chain jobs as they     share the same temporary directory.</p> </li> <li> <p>The container runs with the user ID (uid) set to the effective uid (euid)     of the lava worker unless overridden by the <code>host_config</code> in the job     specification. Don't do this unless you know what you're doing.</p> </li> <li> <p>The effective group ID (egid) of the lava worker is added to the list of     group IDs for the container unless overridden by the <code>host_config</code> in the     job specification. Don't do this unless you know what you're doing.</p> </li> <li> <p>Unless otherwise specified in the <code>host_config</code> job parameter, the default     host configuration (memory, CPU, ports etc) is used.</p> </li> <li> <p>Stderr and stdout are captured by the lava worker.</p> </li> <li> <p>The container will use any proxy configuration present in the docker client     configuration file.</p> </li> <li> <p>Unless otherwise specified in the <code>host_config</code>, the hostname of the     container is set to <code>lava-&lt;REALM&gt;-&lt;WORKER&gt;-&lt;RUN_ID&gt;</code>.</p> </li> <li> <p>Unless otherwise specified in the <code>host_config</code> job parameter, the working     directory is set to the temporary directory shared between host and     container. This mimics the behaviour of other executable job types. The     lava worker will clean up this area on job completion.</p> </li> </ol>"},{"location":"06-jobs-job-types.html#parameters_5","title":"Parameters","text":"Parameter Type Required Description args List[String] No A list of additional arguments for the command. Ignored if no command is specified. command String No The command to run in the container. This will be parsed using standard Linux shell lexical analysis to determine the executable and arguments. If not specified, the default entry point for the container is used. connections Map[String,String] No A dictionary with keys that are connection labels and the values are conn_id docker String No The <code>conn_id</code> for connecting to docker. If not specified, a value must be specified for the entire realm in the realms table. env Map[String,String] No A map of additional environment variables for the container. host_config Map[String,*] No A map of container host configuration parameters. jinja Boolean No If <code>false</code>, disable Jinja rendering of the <code>args</code>. Default <code>true</code>. timeout String No By default, containers run by docker jobs are killed 10 minutes after the container starts to run. This parameter can override that with values in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours). Note that the timeout must be less than the visibility timeout on the worker SQS queue minus the time to pull the image and start the container. vars Map[String,*] No A map of variables injected when the command arguments and environment are rendered."},{"location":"06-jobs-job-types.html#connecting-to-docker","title":"Connecting to Docker","text":"<p>Lava needs to be able to connect to a docker daemon to create containers and a docker registry to obtain docker images. Like all connections in lava, this is managed through the connection manager.</p> <p>When running a docker job, the process to obtain the specified image is:</p> <ol> <li> <p>Look for a <code>docker</code> connection ID in the job parameters. If not found there,     look for a <code>docker</code> connection ID in the realm specification from the     realms table.</p> </li> <li> <p>Fetch the daemon and registry connection details from the     connections table.</p> </li> <li> <p>Connect to the docker daemon.</p> </li> <li> <p>Login to the docker registry and pull the required image from the     specified registry.</p> </li> </ol> <p>Refer to the section on the docker connector for more information.</p>"},{"location":"06-jobs-job-types.html#container-host-configuration","title":"Container Host Configuration","text":"<p>The <code>host_config</code> parameter in the docker job specification is a map that allows a number of configuration parameters for the container to be specified. The definition and types of these parameters correspond to like named parameters of the containers.run() function of the Docker SDK for Python.</p> <p>The following parameters are permitted:</p> <ul> <li>blkio_weight</li> <li>blkio_weight_device</li> <li>cap_add</li> <li>cap_drop</li> <li>cpu_count</li> <li>cpu_percent</li> <li>cpu_period</li> <li>cpu_quota</li> <li>cpu_shares</li> <li>cpuset_cpus</li> <li>cpuset_mems</li> <li>device_read_bps</li> <li>device_read_iops</li> <li>device_write_bps</li> <li>device_write_iops</li> <li>dns</li> <li>dns_opt</li> <li>dns_search</li> <li>domainname</li> <li>extra_hosts</li> <li>hostname</li> <li>group_add</li> <li>mem_limit</li> <li>mem_swappiness</li> <li>memswap_limit</li> <li>nano_cpus</li> <li>network_disabled</li> <li>network_mode</li> <li>ports</li> <li>publish_all_ports</li> <li>shm_size</li> <li>user</li> <li>working_dir</li> </ul> <p>So, for example, to allow a container to use CPUs 0 and 1, disable networking in the container and set the working directory to <code>/tmp/lava</code> add the following:</p> <pre><code>{\n  \"parameters\": {\n    \"host_config\": {\n      \"cpuset_cpus\": \"0,1\",\n      \"network_disabled\": true,\n      \"working_dir\": \"/tmp/lava\"\n    }\n  }\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-arguments-and-environment","title":"Jinja Rendering of the Arguments and Environment","text":"<p>The collected arguments for the command and any environment values defined in the job specification are individually rendered using Jinja prior to execution.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#connection-handling","title":"Connection Handling","text":"<p>Connections are handled similarly to the exe job type. In the case of a docker job, the connection handler executables are mapped into the container and are invoked in the same way.</p> <p>Python executables running in a docker container based on one of the docker images for lava can use the connection manager directly as described in the relevant section in the chapter on developing lava jobs.  It is important to ensure that <code>/usr/local/lib/lava</code> is on the <code>PYTHONPATH</code> for the executable in the container.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_5","title":"Dev Mode Behaviour","text":"<p>Normally, the docker job will copy the container logs to S3 on the conclusion of the job. In dev mode, the container logs are emitted locally after the job run instead of being copied to S3.</p>"},{"location":"06-jobs-job-types.html#examples_4","title":"Examples","text":"<p>The following example will run a shell to invoke the Linux date command in one of the standard lava docker images. The connection ID for the docker registry containing the image is <code>ecr</code>.</p> <pre><code>{\n  \"description\": \"Run a simple command in a container\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/simple-docker\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"args\": [\n      \"-c\",\n      \"date\"\n    ],\n    \"command\": \"/bin/sh\",\n    \"docker\": \"ecr\",\n    \"timeout\": \"10s\"\n  },\n  \"payload\": \"jin-gizmo/lava/amzn2023/base\",\n  \"type\": \"docker\",\n  \"worker\": \"core\"\n}\n</code></pre> <p>This example does exactly the same thing with the docker image and command specified in a slightly different way.</p> <pre><code>{\n  \"description\": \"Run a simple command in a container\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/simple-docker\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"command\": \"/bin/sh -c date\",\n    \"docker\": \"ecr\",\n    \"timeout\": \"10s\"\n  },\n  \"payload\": \"dist/lava/centos8/full:latest\",\n  \"type\": \"docker\",\n  \"worker\": \"core\"\n}\n</code></pre> <p>And again:</p> <pre><code>{\n  \"description\": \"Run a simple command in a container\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/simple-docker\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"command\": \"/bin/sh -c {{vars.cmd}}\",\n    \"docker\": \"ecr\",\n    \"timeout\": \"10s\",\n    \"vars\": {\n        \"cmd\": \"date\"\n    }\n  },\n  \"payload\": \"dist/lava/centos8/full:latest\",\n  \"type\": \"docker\",\n  \"worker\": \"core\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-exe","title":"Job type: exe","text":"<p>The exe job type downloads one or more executables from the S3 payload area and runs them.</p>"},{"location":"06-jobs-job-types.html#payload_6","title":"Payload","text":"<p>The payload is a location in S3 relative to the <code>s3_payloads</code> area specified in the realms table. It can be either an object key, in which case a single executable is downloaded, or a prefix ending in <code>/</code>, in which case all executables under that prefix will be downloaded and run in lexicographic order.</p> <p>See S3 Payloads for more information.</p> <p>The payload value will be parsed using standard Linux shell lexical analysis to determine the S3 object location and, optionally, any arguments. Additional arguments can also be specified with the <code>args</code> parameter.</p> <p>Info</p> <p>The <code>exe</code> job type requires the payload to be a single string. It does not support the list mechanism provided by the v2 Payload Downloader.</p>"},{"location":"06-jobs-job-types.html#environment_2","title":"Environment","text":"<p>Stdin will be redirected from <code>/dev/null</code>. Stdout and stderr are captured and, if non-empty, placed into the realm <code>s3_temp</code> area with the following prefixes:</p> <ul> <li> <p>stdout: <code>&lt;s3_temp&gt;/&lt;job_id&gt;/&lt;run_id&gt;/stdout</code></p> </li> <li> <p>stderr: <code>&lt;s3_temp&gt;/&lt;job_id&gt;/&lt;run_id&gt;/stderr</code></p> </li> </ul> <p>The following variables are placed into the environment for the command.</p> Variable Description LAVA_JOB_ID The <code>job_id</code>. LAVA_OWNER The value of the <code>owner</code> field from the job specification. LAVA_REALM The realm name. LAVA_RUN_ID The <code>run_id</code> UUID. LAVA_S3_KEY The identifier for the KMS key needed to write data into the S3 temporary area. LAVA_S3_PAYLOAD The payload location for this job. LAVA_S3_TMP The private S3 temporary area for this job run. The executables are allowed to put data here. LAVA_WORKER The worker name. PYTHONPATH The PYTHONPATH has the lava code directory appended. This allows Python based executables to directly import lava modules. This is particularly handy for accessing the lava connection manager from within a Python program. <p>Warning</p> <p>If the payload is a script (e.g. bash or Python) then Lava relies on the hashbang line at the beginning of the file. If the file has been edited on a DOS system then it may have DOS style CR-LF line endings which will cause the script interpreter to be unrecognised and the job will fail.</p>"},{"location":"06-jobs-job-types.html#parameters_6","title":"Parameters","text":"Parameter Type Required Description args List[String] No A list of additional arguments for the executable(s). connections Map[String,String] No A dictionary with keys that are connection labels and the values are conn_id env Map[String,String] No A map of additional environment variables for the command. jinja Boolean No If <code>false</code>, disable Jinja rendering of the <code>args</code>. Default <code>true</code>. timeout String No By default, executables run by exe jobs are killed after 10 minutes. This parameter can override that with values in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours). Note that the total timeout for the entire job (<code>timeout</code> multiplied by number of payload elements) must be less than the visibility timeout on the worker SQS queue. vars Map[String,*] No A map of variables injected when the command arguments and environment are Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-arguments-and-environment_1","title":"Jinja Rendering of the Arguments and Environment","text":"<p>The collected arguments for the executable(s) and any environment values defined in the job specification are individually rendered using Jinja prior to execution.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#connection-handling_1","title":"Connection Handling","text":"<p>Lava provides a mechanism for exe jobs to utilise connections defined in the connections table. It does this by creating a small executable that gives effect to the connection and then placing the name of that executable into an environment variable where it can be accessed by the job payload.</p> <p>For example, consider a <code>parameters</code> field for the job like the following,</p> <pre><code>{\n  \"parameters\": {\n    \"connections\": {\n      \"pgres_db\": \"conn_id_for_postgres\"\n    }\n  }\n}\n</code></pre> <p>Lava will create a small executable that automates connection to the relevant Postgres database using the Postgres command line client (psql) and place the name of that executable in the environment variable <code>LAVA_CONN_PGRES_DB</code>. A second environment variable, <code>LAVA_CONNID_PGRESS_DB</code> will contain the connection ID itself, <code>conn_id_for_postgres</code> in this case.</p> <p>If the payload is a shell script, for example, it would use this in the following way:</p> <pre><code># Use $LAVA_CONN_PGRES_DB anywhere you could use the psql CLI.\n# The authentication is automated behind the scenes.\n\n$LAVA_CONN_PGRES_DB &lt;&lt;!\nSELECT * FROM pg_user\nWHERE usename = 'fred';\n!\n</code></pre> <p>If the payload is a Python script, the lava connection manager can be used directly to provide a native Python DBAPI 2.0 connection object, thus:</p> <pre><code>import os\nfrom lava.connection import get_pysql_connection\n\nconn = get_pysql_connection(\n    conn_id=os.environ['LAVA_CONNID_PGRESS_DB'],\n    realm=os.environ['LAVA_REALM']\n)\n\n# Now use conn object in the usual way.\n</code></pre> <p>All of the database connectors work in the same way. Non-database connectors have a similar interface but the behaviour and usage depends on the underlying nature of the thing to which connection is required.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_6","title":"Dev Mode Behaviour","text":"<p>Normally, the exe job will copy stdout and stderr to S3 on the conclusion of the job. In dev mode, stdout and stderr are emitted locally during the job run instead of being copied to S3.</p>"},{"location":"06-jobs-job-types.html#examples_5","title":"Examples","text":"<p>The following example will download and run a shell script with a non-standard timeout of 20 minutes.</p> <pre><code>{\n  \"description\": \"Test pgconnector from exe\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/shell\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"timeout\": \"20m\"\n  },\n  \"payload\": \"demo/shell_script.sh\",\n  \"type\": \"exe\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one will download all files under the <code>demo/scripts/</code> prefix from the payloads area and run them. Note that the download is not recursive.</p> <pre><code>{\n  \"description\": \"Test pgconnector from exe\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/multi_exe\",\n  \"owner\": \"demo@somewhere.com\",\n  \"payload\": \"demo/scripts/\",\n  \"type\": \"exe\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one will run a Python script and provide it with connection handles to a Postgres database and a MySQL database. The command needed to run a Postgres database client with auto-connect will be in the environment variable <code>LAVA_CONN_PGRES_DB</code>. The command needed to run a MySQL database client with auto-connect will be in the environment variable <code>LAVA_CONN_MYSQL_DB</code>.</p> <pre><code>{\n  \"description\": \"Run pgconnector from exe\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/py_exe\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"connections\": {\n      \"mysql_db\": \"conn_id_for_mysql\",\n      \"pgres_db\": \"conn_id_for_postgres\"\n    }\n  },\n  \"payload\": \"demo/conect_to_db.py\",\n  \"type\": \"exe\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one will run an executable with some arguments.</p> <pre><code>{\n  \"description\": \"How to pass arguments to the exe\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/exe_with_args\",\n  \"owner\": \"demo@somewhere.com\",\n  \"payload\": \"demo/my-exe -y --log-level info\",\n  \"type\": \"exe\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one will do exactly the same thing with the arguments specified in a different way.</p> <pre><code>{\n  \"description\": \"How to pass arguments to the exe\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/exe_with_args\",\n  \"owner\": \"demo@somewhere.com\",\n  \"payload\": \"demo/my-exe -y\",\n  \"parameters\": {\n    \"args\": [\n      \"--log-level\",\n      \"info\"\n    ]\n  },\n  \"type\": \"exe\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>And again.</p> <pre><code>{\n  \"description\": \"How to pass arguments to the exe\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/exe_with_args\",\n  \"owner\": \"demo@somewhere.com\",\n  \"payload\": \"demo/my-exe -y --log-level {{vars.level}}\",\n  \"parameters\": {\n    \"vars\": {\n      \"level\": \"info\"\n    }\n  },\n  \"type\": \"exe\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-foreach","title":"Job type: foreach","text":"<p>The foreach job type runs a single specified job in a loop.</p> <p>Like the chain job type, this is a synchronous operation. The parent foreach job and the child job must be set to run on the same worker and the parent and all child iterations will run under the <code>run_id</code> of the parent job.</p> <p>Any globals available in the foreach job will also be passed to each iteration of the child job, with additional, iteration specific, globals injected for each iteration.  A loop index counter (<code>globals.lava.foreach_index</code>) is also made available to child iterations in the globals owned by lava.</p>"},{"location":"06-jobs-job-types.html#payload_7","title":"Payload","text":"<p>The <code>payload</code> is the <code>job_id</code> of the job to be iterated.</p>"},{"location":"06-jobs-job-types.html#parameters_7","title":"Parameters","text":"Parameter Type Required Description can_fail Boolean No A boolean that indicates if individual iterations are permitted to fail without causing the entire job to fail. Defaults to <code>false</code>. See Allowing Foreach Child Jobs to Fail. foreach Map[String,*] Yes This parameter specifies the mechanism used to generate named values for each iteration. It is effectively a Python-like iterator yielding a dictionary of values for each iteration that is merged into the globals for the child job. See Foreach Value Generators. limit Integer No An integer. Attempting to run a foreach job with more than this many loop iterations will fail without any job being run at all. Defaults to the realm level value as specified by the FOREACH_LIMIT configuration parameter. jinja Boolean No If true (the default), enable Jinja rendering. What gets rendered in the <code>foreach</code> generator specification is controlled by the generator itself."},{"location":"06-jobs-job-types.html#foreach-value-generators","title":"Foreach Value Generators","text":"<p>The <code>foreach</code> parameter is a map that defines the mechanism used to generate a set of named values for each loop iteration. These named values are merged into the globals passed to the child job.</p> <p>Value generation is as lazy as possible to minimise overheads and memory usage.</p> <p>Info</p> <p>It is not permitted for any value names returned by a <code>foreach</code> generator to begin with <code>lava</code> (case independent).</p> <p>Apart from the common <code>type</code> field, the other fields in the <code>foreach</code> parameter depend on the <code>type</code>. Field values are not Jinja rendered unless indicated otherwise below.</p>"},{"location":"06-jobs-job-types.html#foreach-type-csv","title":"Foreach type: CSV","text":"<p>The csv generator reads data from a CSV file. The file must have a header to provide the names of values generated for each <code>foreach</code> iteration.</p> Field Type Required Description type String Yes The foreach generator type: <code>csv</code>. filename String Yes The name of the CSV file, which can be local or in S3 (<code>s3://...</code>). This field is Jinja rendered. <p>For example, given this CSV source file ...</p> <pre><code>a,b\na0,b0\na1,b1\n</code></pre> <p>... the globals of the child job would have <code>globals.a=a0</code> on the first iteration and <code>globals.a=a1</code> on the second.</p>"},{"location":"06-jobs-job-types.html#foreach-type-inline","title":"Foreach type: inline","text":"<p>The inline generator contains the iteration values within the <code>foreach</code> parameter itself.</p> Field Type Required Description type String Yes The foreach generator type: <code>inline</code>. values List[Map[String,*]] Yes A list of maps containing iteration values. <p>For example, given this (partial) job specification ...</p> <pre><code>{\n  \"job_id\": \"example\",\n  \"type\": \"foreach\",\n  \"payload\": \"child_job_id\",\n  \"parameters\": {\n    \"foreach\": {\n      \"type\": \"inline\",\n      \"values\": [\n        {\"a\": \"a0\", \"b\": \"b0\" },\n        {\"a\": \"a1\", \"b\": \"b1\" }\n      ]\n    }\n  }\n}\n</code></pre> <p>... the globals of the child job would have <code>globals.a=a0</code> on the first iteration and <code>globals.a=a1</code> on the second.</p> <p>Note</p> <p>It is not required that each value map in the list has the same keys, although it would be an unusual use-case.</p>"},{"location":"06-jobs-job-types.html#foreach-type-jsonl","title":"Foreach type: jsonl","text":"<p>The jsonl generator reads JSON encoded objects from a file, one object per line.</p> Field Type Required Description type String Yes The foreach generator type: <code>jsonl</code>. filename String Yes The name of the JSONL file, which can be local or in S3 (<code>s3://...</code>). This field is Jinja rendered. <p>For example, given this JSONL source file ...</p> <pre><code>{\"a\": \"a0\", \"b\": \"b0\" }\n{\"a\": \"a1\", \"b\": \"b1\" }\n</code></pre> <p>... the globals of the child job would have <code>globals.a=a0</code> on the first iteration and <code>globals.a=a1</code> on the second.</p> <p>Note</p> <p>It is not required that each JSON line has the same keys, although it would be an unusual use-case.</p>"},{"location":"06-jobs-job-types.html#foreach-type-query","title":"Foreach type: query","text":"<p>The query generator obtains iteration values from a database query. </p> Field Type Required Description type String Yes The foreach generator type: <code>query</code>. conn_id String Yes The connection ID for a target database. query String Yes The query to provide iteration values. Each row provides one set of iteration values. The field names are derived from the column names in the query. Take care to specify useful column names in the query when using database functions. <p>But but but ...</p> <p>No, the query text is not Jinja rendered ... and it won't be.</p> <p>For example, given this (partial) job specification ...</p> <pre><code>{\n  \"job_id\": \"example\",\n  \"type\": \"foreach\",\n  \"payload\": \"child_job_id\",\n  \"parameters\": {\n    \"foreach\": {\n      \"type\": \"query\",\n      \"conn_id\": \"my_db_conn_id\",\n      \"query\": \"SELECT a, COUNT(*) AS b FROM some.table GROUP by 1 LIMIT 5\"\n    }\n  }\n}\n</code></pre> <p>... would generate up to 5 iterations, each containing an <code>a</code> and <code>b</code> global.</p> <p>Note</p> <p>Be careful about producing too many rows or including unnecessary values. The <code>limit</code> parameter will force the job to abort if more than that many rows are produced. Use database views to effect here.</p>"},{"location":"06-jobs-job-types.html#foreach-type-range","title":"Foreach type: range","text":"<p>The range generator behaves in the same way as the Python range() mechanism.</p> Field Type Required Description type String Yes The foreach generator type: <code>range</code>. name String No If specified, the current value of the range counter is made available in a global with this name. This value may be different from <code>lava.globals.foreach_index</code> which always counts up from <code>0</code>. start Integer No The starting index for the range. Default is <code>0</code>. stop Integer Yes The value of the <code>stop</code> parameter. step Integer No The value of the <code>step</code> parameter. Default is <code>1</code>. <p>For example, given this (partial) job specification ...</p> <pre><code>{\n  \"job_id\": \"example\",\n  \"type\": \"foreach\",\n  \"payload\": \"child_job_id\",\n  \"parameters\": {\n    \"foreach\": {\n      \"type\": \"range\",\n      \"name\": \"a\",\n      \"stop\": 2\n    }\n  }\n}\n</code></pre> <p>... the globals of the child job would have <code>globals.a=0</code> on the first iteration and <code>globals.a=1</code> on the second.</p>"},{"location":"06-jobs-job-types.html#foreach-type-s3list","title":"Foreach type: s3list","text":"<p>The s3list generator produces a list of objects in an AWS S3 bucket.</p> Field Type Required Description type String Yes The foreach generator type: <code>s3list</code>. bucket String Yes The S3 bucket name. This field is Jinja rendered. prefix String No The S3 prefix to list. Defaults to the bucket root. This field is Jinja rendered. glob String|List[String] No A glob style pattern, or list of patterns. Only S3 objects with names matching any of the patterns are returned. <p>A (partial) job specification might look like so:</p> <pre><code>{\n  \"job_id\": \"example\",\n  \"type\": \"foreach\",\n  \"payload\": \"child_job_id\",\n  \"parameters\": {\n    \"foreach\": {\n      \"type\": \"s3list\",\n      \"bucket\": \"my-bucket\",\n      \"prefix\": \"a/prefix/\",\n      \"glob\": [\n        \"*.csv\",\n        \"*.jsonl\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Each element returned by the generator to be added to the child globals is of the form:</p> <pre><code>{\n    's3obj': {\n        'Bucket': 'my-bucket',\n        'Key': 'a/prefix/somewhere/in/s3.jsonl',\n        'LastModified': datetime.datetime(2024, 1, 1, 6, 2, 59, tzinfo=tzutc()),\n        'ETag': '\"be0c0123456789abcd0123456678916a\"',\n        'Size': 197,\n        'StorageClass': 'STANDARD'\n    }\n}\n</code></pre> <p>The value in the dictionary shown above is the object returned in the <code>Contents</code> list in the response to the boto3 S3 client list_objects_v2 function, with the addition of the bucket name.</p> <p>In the example shown above, the child job could access the full S3 object name as:</p> <pre><code>s3://{{ globals.s3obj.Bucket }}/{{ globals.s3obj.Key }}\n</code></pre>"},{"location":"06-jobs-job-types.html#allowing-foreach-child-jobs-to-fail","title":"Allowing Foreach Child Jobs to Fail","text":"<p>By default, the foreach loop is aborted when any iteration fails unless the <code>can_fail</code> parameter is set to <code>true</code>. In this case, the iteration process will continue and the master foreach job will succeed even if child iterations fail. In this situation, it is important that the child job handles its own on_fail actions, as the parent will not. Suck it up kid.</p> <p>This tolerance of failure does not include configuration errors in the child job, such as a malformed job specification, child job sent to the wrong worker etc. This will still cause the entire foreach job to fail.</p> <p>Keep Calm</p> <p>Before anyone gets all bitter and twisted about can vs may in <code>can_fail</code>...  both are essentially equivalent in this crazy, modern world. Look it up.</p>"},{"location":"06-jobs-job-types.html#handling-of-globals_3","title":"Handling of Globals","text":"<p>The foreach job type merges its globals into those of the child job for each iteration. A value specified in the parent foreach job will override a similarly named value in the child.</p> <p>The globals generated by the foreach iteration generator will override any existing global with the same name.</p> <p>The foreach job will also add lava specific globals under <code>globals.lava</code>. These lava owned globals allow all child iterations, to access some common global values. An attempt by the foreach iteration generator to override any of these will cause the job to fail.</p> <p>The <code>global.lava.foreach_index</code> global is special. It is a counter, starting at zero for the first loop iteration, and incremented by one for each iteration.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_7","title":"Dev Mode Behaviour","text":"<p>The foreach job behaviour is unchanged for dev mode. However, dev mode is propagated to child jobs.</p>"},{"location":"06-jobs-job-types.html#job-type-lavasched","title":"Job type: lavasched","text":"<p>The lavasched job type is a special lava internal job type. It must be run only on workers that are also dispatchers for the realm to create the crontab on that node to dispatch jobs.</p> <p>Each crontab entry schedules the dispatch of a job by running <code>lava-dispatcher</code> to send an SQS message to the appropriate worker SQS queue.</p> <p>The lavasched jobs must themselves be scheduled to update the crontab periodically to accommodate changes in the jobs table. Refer to the section on Scheduled Dispatch for more information on how to initialise this process.</p> <p>Refer to the section Schedule Specifications for more information.</p>"},{"location":"06-jobs-job-types.html#payload_8","title":"Payload","text":"<p>The payload is ignored for lavasched jobs.</p>"},{"location":"06-jobs-job-types.html#parameters_8","title":"Parameters","text":"Parameter Type Required Description args List[String] No A list of additional arguments that will be added to the invocation of <code>lava-dispatcher</code> in the crontab entries. dispatcher String or List[String] Yes Name of dispatcher, or a list of names, specifying the dispatchers for which schedules should be built. env Map[String,String] No A map of environment variables that will be added into the crontab. Of these, the most useful is <code>CRON_TZ</code>, which controls the timezone used by cron for this dispatcher. Note that this only controls the timezone for the dispatch process, not for the job run itself which will be the local timezone of the worker. See also Cron and PATH."},{"location":"06-jobs-job-types.html#cron-and-path","title":"Cron and PATH","text":"<p>Scheduled dispatches are effected by running the Python based <code>lava-dispatcher</code> utility via cron(8). It is critical that the <code>PATH</code> for cron yields a lava compatible version of Python.</p> <p>Cron typically has a default <code>PATH</code> set at a system level that points to the system default version of Python. This may be an older version that is not lava compatible. This is the case for the lava AMI.</p> <p>The best way to avoid problems is to explicitly specify the desired <code>PATH</code> in the <code>env</code> parameter for the job. e.g.</p> <pre><code>{\n  \"parameters\": {\n    \"env\": {\n      \"PATH\": \"/usr/local/bin:/bin:/usr/bin\"\n  }\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_8","title":"Dev Mode Behaviour","text":"<p>The lavasched job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_6","title":"Examples","text":"<p>The following example will cause the crontab to be rebuilt for the <code>localtime</code> dispatcher every hour. It will include jobs that have a <code>dispatcher</code> field of <code>localtime</code> - including this lavasched job. The dispatch schedule will operate with respect to the local time set on the worker instance.</p> <pre><code>{\n  \"description\": \"Rebuild the dispatcher crontab\",\n  \"dispatcher\": \"localtime\",\n  \"enabled\": true,\n  \"job_id\": \"demo/schedule-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"dispatcher\": \"localtime\",\n    \"env\": {\n      \"PATH\": \"/usr/local/bin:/bin:/usr/bin\"\n    }\n  },\n  \"payload\": \"--\",\n  \"schedule\": \"0 * * * *\",\n  \"type\": \"lavasched\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one does the same but sets the logging level on the <code>lava-dispatcher</code> invocations to <code>warning</code>.</p> <pre><code>{\n  \"description\": \"Rebuild the Sydney dispatcher crontab\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/schedule-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"args\": [ \"--level\", \"warning\" ],\n    \"dispatcher\": \"Sydney\",\n    \"env\": {\n      \"PATH\": \"/usr/local/bin:/bin:/usr/bin\"\n    }\n  },\n  \"payload\": \"--\",\n  \"schedule\": \"0 * * * *\",\n  \"type\": \"lavasched\",\n  \"worker\": \"dispatch_syd\"\n}\n</code></pre> <p>This one will cause the dispatcher schedule to operate on Perth time. Note that the jobs themselves will be dispatched on a Perth schedule but they will run under whatever timezone setting the worker has.</p> <pre><code>{\n  \"description\": \"Rebuild the Perth dispatcher crontab\",\n  \"dispatcher\": \"Perth\",\n  \"enabled\": true,\n  \"job_id\": \"demo/schedule-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"dispatcher\": \"Perth\",\n    \"env\": {\n      \"CRON_TZ\": \"Australia/Perth\",\n      \"PATH\": \"/usr/local/bin:/bin:/usr/bin\"\n    }\n  },\n  \"payload\": \"--\",\n  \"schedule\": \"0 * * * *\",\n  \"type\": \"lavasched\",\n  \"worker\": \"dispatch_perth\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-log","title":"Job type: log","text":"<p>The log job type simply logs the job specification to the syslog. Payload and parameters are ignored.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_9","title":"Dev Mode Behaviour","text":"<p>The log job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#job-type-pkg","title":"Job type: pkg","text":"<p>The pkg job type downloads one or more code packages from the S3 payload area, unpacks them and runs the nominated executable within the package using the same invocation mechanism as the exe job type.</p> <p>Supported package types are:</p> <ul> <li> <p>Tar files, including any compressed variants supported by the standard Linux     tar command.</p> </li> <li> <p>Zip files.</p> </li> </ul>"},{"location":"06-jobs-job-types.html#payload_9","title":"Payload","text":"<p>The payload is a location in S3 relative to the <code>s3_payloads</code> area specified in the realms table. It can be either an object key, in which case a single package is downloaded, or a prefix ending in <code>/</code>, in which case all packages under that prefix will be downloaded and run in lexicographic order.</p> <p>See S3 Payloads for more information.</p> <p>Note</p> <p>The <code>pkg</code> job type also allows the payload to be a list of S3 locations if the v2 Payload Downloader is enabled.</p>"},{"location":"06-jobs-job-types.html#environment_3","title":"Environment","text":"<p>Stdin will be redirected from <code>/dev/null</code>. Stdout and Stderr are captured and, if non-empty, placed into the realm <code>s3_temp</code> area with the following prefixes:</p> <ul> <li> <p>stdout: <code>&lt;s3_temp&gt;/&lt;job_id&gt;/&lt;run_id&gt;/stdout</code></p> </li> <li> <p>stderr: <code>&lt;s3_temp&gt;/&lt;job_id&gt;/&lt;run_id&gt;/stderr</code></p> </li> </ul> <p>The following variables are placed into the environment for the command.</p> Variable Description LAVA_JOB_ID The <code>job_id</code>. LAVA_OWNER The value of the <code>owner</code> field from the job specification. LAVA_REALM The realm name. LAVA_RUN_ID The <code>run_id</code> UUID. LAVA_S3_KEY The identifier for the KMS key needed to write data into the S3 temporary area. LAVA_S3_PAYLOAD The payload location for this job. LAVA_S3_TMP The private S3 temporary area for this job run. The executables are allowed to put data here. LAVA_WORKER The worker name. PYTHONPATH The PYTHONPATH has the lava code directory appended. This allows Python based executables to directly import lava modules. This is particularly handy for accessing the lava connection manager from within a Python program. <p>Warning</p> <p>If the entry point in the payload is a script (e.g. bash or Python) then Lava relies on the hashbang line at the beginning of the file. If the file has been edited on a DOS system then it may have DOS style CR-LF line endings which will cause the script interpreter to be unrecognised and the job will fail.</p>"},{"location":"06-jobs-job-types.html#parameters_9","title":"Parameters","text":"Parameter Type Required Description args List[String] No A list of additional arguments for the main executable(s). command String Yes The name of the entry point executable in the bundle, relative to the root of the bundle. This will be parsed using standard Linux shell lexical analysis to determine the executable and arguments. Additional arguments can also be specified with the <code>args</code> parameter. connections Map[String,String] No A dictionary with keys that are connection labels and the values are conn_id env Map[String,String] No A map of additional environment variables for the command. jinja Boolean No If <code>false</code>, disable Jinja rendering of the <code>args</code>. Default <code>true</code>. timeout String No By default, executables run by exe jobs are killed after 10 minutes. This parameter can override that with values in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours). Note that the total timeout for the entire job (<code>timeout</code> multiplied by number of payload elements) must be less than the visibility timeout on the worker SQS queue. vars Map[String,*] No A map of variables injected when the arguments and environment are Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-arguments-and-environment_2","title":"Jinja Rendering of the Arguments and Environment","text":"<p>The collected arguments for the executable(s) and any environment values defined in the job specification are individually rendered using Jinja prior to execution.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#connection-handling_2","title":"Connection Handling","text":"<p>Connections are handled exactly as for the exe job type.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_10","title":"Dev Mode Behaviour","text":"<p>Normally, the pkg job will copy stdout and stderr to S3 on the conclusion of the job. In dev mode, stdout and stderr are emitted locally during the job run instead of being copied to S3.</p>"},{"location":"06-jobs-job-types.html#examples_7","title":"Examples","text":"<p>The following example will download a zip file, unpack it and run the main executable:</p> <pre><code>{\n  \"description\": \"Run a package of code.\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/my-pkg\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"command\": \"main-script.sh --log-level warning\"\n  },\n  \"payload\": \"demo/my-pkg.zip\",\n  \"type\": \"pkg\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-redshift_unload","title":"Job type: redshift_unload","text":"<p>The redshift_unload job type performs an UNLOAD operation on an AWS Redshift cluster.</p>"},{"location":"06-jobs-job-types.html#payload_10","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_10","title":"Parameters","text":"Parameter Type Required Description args List[String] Yes A list of options for the <code>UNLOAD</code> command. All of the Redshift supported parameters except for the authorisation parameters are supported. The options are pre-processed before passing to Redshift as described below. bucket String Yes The name of the S3 bucket to which the data is unloaded. conn_id String Yes The connection ID for a Redshift provisioned cluster or a Redshift Serverless workgroup. dateformat String No A Redshift datetime format string that will be applied to DATE fields when unloading. The safest value is <code>YYYY-MM-DD</code>. More information on date formats. insecure Boolean No Disable bucket security checks. Default is false. prefix String Yes The prefix in the S3 bucket to which the data is unloaded. This value is Jinja rendered prior to use. relation String or List[String] Yes The name of the table or view to be unloaded (without schema) or a list of names. s3_conn_id String No The connection ID for AWS S3. This is used in the <code>UNLOAD</code> command to allow Redshift access to S3. Either <code>s3_conn_id</code> or <code>s3_iam_role</code> is required. s3_iam_role String No The IAM role name to use in the <code>UNLOAD</code> command to allow Redshift access to S3. Either <code>s3_conn_id</code> or <code>s3_iam_role</code> is required. schema String Yes The name of the source schema for the object to be unloaded. start String No Name of the relation to start with when unloading a list of relations. If not specified, start at the beginning of the list. This is useful when <code>stop_on_fail</code> is false and an unload fails as it allows the unloads to be resumed at the point of failure once the issue is rectified. stop_on_fail Boolean No If true, stop when any unload fails otherwise keep moving through the unload list. Default is true. The event record for the job will indicate which unloads succeeded and which failed. vars Map[String,*] No A map of variables injected when the S3 target prefix is Jinja rendered. where String No An optional <code>WHERE</code> condition for the <code>UNLOAD</code> queries. Do not include the <code>WHERE</code> keyword. Note that the same condition will be added to each unload if multiple relations are specified."},{"location":"06-jobs-job-types.html#handling-of-redshift-unload-options","title":"Handling of Redshift Unload Options","text":"<p>The options for the Redshift UNLOAD command specified in the <code>args</code> key of the job parameters are pre-processed prior to use in the <code>UNLOAD</code> operation. Unless otherwise specified below, the option is passed through unmodified.</p>"},{"location":"06-jobs-job-types.html#partition-option","title":"Partition Option","text":"<p>The <code>PARTITION</code> option is supplied to the <code>UNLOAD</code> command in the following form:</p> <pre><code>PARTITION BY (col1, col2,...)\n</code></pre> <p>If the unload <code>args</code> contain the <code>PARTITION</code> clause in this format, it is used as is.</p> <p>The unload <code>args</code> can instead contain a <code>PARTITION</code> clause in this format:</p> <pre><code>PARTITION BY @&lt;SCHEMA_NAME&gt;.&lt;REL_NAME&gt;\n</code></pre> <p>In this case, the partition information is assumed to be contained in a partition table (or view) <code>&lt;SCHEMA_NAME&gt;.&lt;REL_NAME&gt;</code> that resides in the same Redshift cluster as the source relation. The partition table (or view) must include the following columns:</p> Name Type Description schema_name VARCHAR(127) Schema name in lower case. rel_name VARCHAR(127) Table or view name in lower case. partitions VARCHAR(n) A comma separated list of columns in the relation that will be used to populate the partition column list in the <code>UNLOAD</code> command. <p>The following SQL DDL would create a suitable partition table.</p> <pre><code>CREATE TABLE metadata.partitions\n(\n    schema_name VARCHAR(127)  NOT NULL,\n    rel_name    VARCHAR(127)  NOT NULL,\n    partitions  VARCHAR(2048) NOT NULL,\n    PRIMARY KEY (schema_name, rel_name)\n)\n</code></pre> <p>The <code>PARTITION</code> argument in the job parameters would then look like:</p> <pre><code>PARTITION BY @metadata.partitions\n</code></pre>"},{"location":"06-jobs-job-types.html#date-formatting","title":"Date Formatting","text":"<p>Redshift has some very nasty behaviour in its date handling. It will UNLOAD date fields with years less than 100 in <code>YY-MM-DD</code> format. Madness.</p> <p>Redshift COPY using <code>DATEFORMAT 'auto'</code> assumes dates in <code>YY-MM-DD</code> format must be Y2K adjusted (i.e. they are moved to post 2000). More madness.</p> <p>So, an UNLOAD followed by a COPY will mangle dates. While the COPY command could specify <code>DATEFORMAT 'YY-MM-DD'</code>, this will then fail for any dates with years greater than 99. So, if the column contains dates before and after the year 100, you're in trouble.</p> <p>In short, there is no reliable way to UNLOAD and then COPY a data set containing dates without manually formatting date fields.</p> <p>To get around this singular piece of genius, use the <code>dateformat</code> key in the job parameters.</p> <p>If the <code>dateformat</code> key is present, and the relation has one or more DATE fields, redshift_unload will construct a SELECT statement for the unload that includes all of the columns, applying the given date format to DATE columns. This is likely to impact UNLOAD performance, so it's best to avoid using it if you are certain all dates have years greater than 99. The safest format value is probably <code>YYYY-MM-DD</code>.</p> <p>If the <code>dateformat</code> key is not present, or the relation has no DATE columns, redshift_unload will simply use <code>SELECT *</code> for the target relation in the UNLOAD command.</p>"},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-s3-target-key","title":"Jinja Rendering of the S3 Target Key","text":"<p>The <code>prefix</code> parameter specifies the UNLOAD location in the target bucket. Its value is rendered using Jinja to allow injection of parameters relevant to the individual job specification and run.</p> <p>Info</p> <p>If a list of relations is to be unloaded, it is important to use this rendering facility to ensure that each relation is unloaded to its own area in S3.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, the S3 location of the unload (<code>prefix</code>) can be dynamically set to include components such as the schema, relation name, unload date etc.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. relation str The relation name. schema str The schema name. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#s3-bucket-security-checks","title":"S3 Bucket Security Checks","text":"<p>Redshift_unload performs some basic checks on the security of the target S3 bucket to reduce the risk of unloading data to somewhere unsafe. This is a convenience only and should not be relied upon for securing your data.</p> <p>By default, if any of the following are true, the bucket will not be used for unloaded data and the job run will fail:</p> <ul> <li> <p>The bucket has any form of public access</p> </li> <li> <p>The bucket does not have default encryption enabled</p> </li> <li> <p>Server logging is not enabled on the bucket</p> </li> <li> <p>The bucket is owned by an AWS account other than the one associated with the     profile being used by the lava worker.</p> </li> </ul> <p>These security checks can be disabled by setting the <code>insecure</code> parameter to <code>true</code>.</p> <p>If the security checks are enabled, the lava worker will require the following additional IAM permissions on the target bucket:</p> <ul> <li> <p>List all buckets: <code>s3:ListAllMyBuckets</code></p> </li> <li> <p>Get bucket logging configuration: <code>s3:GetBucketLogging</code></p> </li> <li> <p>Get bucket encryption configuration: <code>s3:GetEncryptionConfiguration</code></p> </li> <li> <p>Get bucket ACLs: <code>s3:GetBucketAcl</code></p> </li> </ul>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_11","title":"Dev Mode Behaviour","text":"<p>The redshift_unload job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_8","title":"Examples","text":"<p>The following example will unload the table <code>mytable</code></p> <pre><code>{\n  \"description\": \"Unload the table 'mytable'\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/unload-mytable\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"args\": [\n      \"parallel on\",\n      \"delimiter '|'\",\n      \"kms_key_id 'alias/lava-proto-user'\",\n      \"encrypted\",\n      \"header\",\n      \"allowoverwrite\"\n    ],\n    \"bucket\": \"my-bucket\",\n    \"conn_id\": \"redshift-conn-01\",\n    \"dateformat\": \"YYYY-MM-DD\",\n    \"prefix\": \"unload_demo/{{schema}}/{{relation}}/\",\n    \"relation\": \"mytable\",\n    \"s3_conn_id\": \"s3-conn-01\",\n    \"schema\": \"public\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"redshift_unload\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one unloads the same table but the destination in S3 includes a date component.</p> <pre><code>{\n  \"description\": \"Unload the table 'mytable'\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/unload-mytable\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"args\": [\n      \"parallel on\",\n      \"delimiter '|'\",\n      \"kms_key_id 'alias/lava-proto-user'\",\n      \"encrypted\",\n      \"header\",\n      \"allowoverwrite\"\n    ],\n    \"bucket\": \"my-bucket\",\n    \"conn_id\": \"redshift-conn-01\",\n    \"dateformat\": \"YYYY-MM-DD\",\n    \"prefix\": \"unload_demo/{{schema}}/{{relation}}/{{start.strftime('%Y/%m/%d')}}/\",\n    \"relation\": \"mytable\",\n    \"s3_conn_id\": \"s3-conn-01\",\n    \"schema\": \"public\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"redshift_unload\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one unloads a list of tables.</p> <pre><code>{\n  \"description\": \"Unload multiple tables\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/unload-multi\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"args\": [\n      \"parallel on\",\n      \"delimiter '|'\",\n      \"kms_key_id 'alias/lava-proto-user'\",\n      \"encrypted\",\n      \"header\",\n      \"allowoverwrite\"\n    ],\n    \"bucket\": \"my-bucket\",\n    \"conn_id\": \"redshift-conn-01\",\n    \"dateformat\": \"YYYY-MM-DD\",\n    \"prefix\": \"unload_demo/{{schema}}/{{relation}}/{{start.strftime('%Y/%m/%d')}}/\",\n    \"relation\": [\n      \"mytable\",\n      \"yourtable\"\n    ],\n    \"s3_conn_id\": \"s3-conn-01\",\n    \"schema\": \"public\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"redshift_unload\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sharepoint_get_doc","title":"Job type: sharepoint_get_doc","text":"<p>The sharepoint_get_doc job type downloads a SharePoint document to a specified destination file.</p> <p>Connection to the target SharePoint site is handled automatically by lava.</p>"},{"location":"06-jobs-job-types.html#payload_11","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_11","title":"Parameters","text":"Parameter Type Required Description basedir String No If the target file is specified as a relative filename, it will be treated as relative to the specified directory. Defaults to the lava temporary directory for the job. conn_id String Yes The connection ID for a SharePoint site. file String Yes The destination file name. If it starts with <code>s3://</code> it is assumed to be an object in S3, otherwise a local file. If a local file and not absolute, it will be relative to the <code>basedir</code> parameter. This value is Jinja rendered. jinja Boolean No If <code>false</code>, disable Jinja rendering. Default <code>true</code>. kms_key_id String No An AWS KMS encryption key to use when uploading to AWS S3. library String Yes Source SharePoint library name. This value is Jinja rendered. path String Yes The source document path in SharePoint. Use POSIX, not DOS, style path names (i.e. forward slash path separators). It must be an absolute path starting with <code>/</code>. This value is Jinja rendered. vars Map[String,*] No A map of variables injected when the parameters are Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-parameters","title":"Jinja Rendering of Parameters","text":"<p>Some of the parameters are  rendered using Jinja.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, the source path can be dynamically set to include components such as the date or part of the target file name.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_12","title":"Dev Mode Behaviour","text":"<p>The sharepoint_get_doc job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_9","title":"Examples","text":"<p>The following example downloads a document from SharePoint and places it in S3. The file will be KMS encrypted in S3 with the specified KMS key.</p> <pre><code>{\n  \"description\": \"Download a CSV file from SharePoint\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-doc\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"library\": \"My SharePoint Library\",\n    \"path\": \"/Interesting/Data/x.csv\",\n    \"file\": \"s3://my-bucket/x.csv\",\n    \"kms_key_id\": \"alias/data\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_get_doc\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one shows how Jinja can be used to include the current date in the source document name.</p> <pre><code>{\n  \"description\": \"Download a CSV file from SharePoint\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-doc\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"library\": \"My SharePoint Library\",\n    \"path\": \"/Interesting/Data/{{start.strftime('%Y-%m-%d')}}.csv\",\n    \"file\": \"s3://my-bucket/{{start.strftime('%Y-%m-%d')}}.csv\",\n    \"kms_key_id\": \"alias/data\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_get_doc\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sharepoint_get_list","title":"Job type: sharepoint_get_list","text":"<p>The sharepoint_get_list job type downloads a SharePoint list to a a specified destination file, one row per line.</p> <p>Connection to the target SharePoint site is handled automatically by lava.</p>"},{"location":"06-jobs-job-types.html#payload_12","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_12","title":"Parameters","text":"<p>The formatting related parameters are as defined for the Python CSV writer, although some of the defaults are different.  Defaults can be overridden at the realm level using configuration variables.</p> Parameter Type Required Description basedir String No If the target file is specified as a relative filename, it will be treated as relative to the specified directory. Defaults to the lava temporary directory for the job. conn_id String Yes The connection ID for a SharePoint site. data_columns String No A comma separated list of column names. If specified, then only columns listed are extracted (in addition to any specified <code>system_columns</code>). delimiter String No Single character field delimiter. Default <code>|</code>. doublequote Boolean No As for csv.writer. Default <code>false</code>. escapechar String No As for csv.writer. Default <code>null</code>. file String Yes The destination file name. If it starts with <code>s3://</code> it is assumed to be an object in S3, otherwise a local file. If a local file and not absolute, it will be relative to the <code>basedir</code> parameter. This value is Jinja rendered. header Boolean No If <code>true</code>, include a header line containing column names. Default <code>true</code>. jinja Boolean No If <code>false</code>, disable Jinja rendering. Default <code>true</code>. kms_key_id String No An AWS KMS encryption key to use when uploading to AWS S3. list String Yes Name of the list. It must already exist in SharePoint. This will be jinja rendered. quotechar String No As for csv.writer. Default <code>\"</code>. quoting String No As for csv.writer <code>QUOTE_*</code> parameters (without the QUOTE_ prefix). Default <code>minimal</code> (i.e. <code>QUOTE_MINIMAL</code>). system_columns String No A comma separated list of system columns to retrieve in addition to the data columns. Unless specified, only data columns are retrieved. vars Map[String,*] No A map of variables injected when the parameters are Jinja rendered. <p>Consult SharePoint documentation for available system columns. Currently known columns include:</p> <ul> <li><code>ComplianceAssetId</code></li> <li><code>AppAuthor</code></li> <li><code>AppEditor</code></li> <li><code>Attachments</code></li> <li><code>Author</code></li> <li><code>ContentType</code></li> <li><code>Created</code></li> <li><code>DocIcon</code></li> <li><code>Edit</code></li> <li><code>Editor</code></li> <li><code>FolderChildCount</code></li> <li><code>ID</code></li> <li><code>ItemChildCount</code></li> <li><code>LinkTitleNoMenu</code></li> <li><code>LinkTitle</code></li> <li><code>Modified</code></li> <li><code>_ComplianceFlags</code></li> <li><code>_ComplianceTagUserId</code></li> <li><code>_ComplianceTagWrittenTime</code></li> <li><code>_ComplianceTag</code></li> <li><code>_IsRecord</code></li> <li><code>_UIVersionString</code></li> </ul>"},{"location":"06-jobs-job-types.html#handling-of-personorgroup-columns","title":"Handling of personorgroup Columns","text":"<p>Columns of type <code>personorgroup</code> have an associated <code>LookupId</code> column. So for a column named <code>&lt;COLUMN&gt;</code>, the list also has a <code>&lt;COLUMN&gt;LookupId</code> column.</p> <p>If the <code>data_columns</code> parameter is not specified, meaning to retrieve all columns, the <code>&lt;COLUMN&gt;</code> and <code>&lt;COLUMN&gt;LookupId</code> will both be retrieved, in that order.</p> <p>If the <code>data_columns</code> parameter is specified, the <code>&lt;COLUMN&gt;LookupId</code> must be explicitly named in the list if required.</p> <p>System columns of type <code>personorgroup</code> also have an associated <code>LookupId</code> column that can be retrieved by naming it in the <code>system_columns</code> parameter.</p>"},{"location":"06-jobs-job-types.html#jinja-rendering-of-parameters_1","title":"Jinja Rendering of Parameters","text":"<p>Some of the parameters are  rendered using Jinja.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, target path can be dynamically set to include components such as the date or part of the source file name.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_13","title":"Dev Mode Behaviour","text":"<p>The sharepoint_get_list job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_10","title":"Examples","text":"<p>The following example replaces the contents of a list with data from S3.</p> <pre><code>{\n  \"description\": \"Download a SharePoint list to a CSV file\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-list\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"list\": \"My List\",\n    \"file\": \"s3://my-bucket/x.csv\",\n    \"kms_key_id\": \"alias/data\",\n    \"quote\": \"minimal\",\n    \"separator\": \",\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_get_list\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sharepoint_get_multi_doc","title":"Job type: sharepoint_get_multi_doc","text":"<p>The sharepoint_get_multi_doc job type downloads multiple SharePoint documents to a specified destination path.</p> <p>Connection to the target SharePoint site is handled automatically by lava.</p>"},{"location":"06-jobs-job-types.html#payload_13","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_13","title":"Parameters","text":"Parameter Type Required Description basedir String No If the target file is specified as a relative filename, it will be treated as relative to the specified directory. Defaults to the lava temporary directory for the job. conn_id String Yes The connection ID for a SharePoint site. glob String No The UNIX glob style pattern used to select files in the specified SharePoint folder path area. If not specified, all files are selected. Matching is case-insensitive. This value is Jinja rendered. jinja Boolean No If <code>false</code>, disable Jinja rendering. Default <code>true</code>. kms_key_id String No An AWS KMS encryption key to use when uploading to AWS S3. library String Yes Source SharePoint library name. This value is Jinja rendered. outpath String Yes The destination file location. If it starts with <code>s3://</code> it is assumed to be a prefix in S3 (add your own trailing <code>/</code> if needed), otherwise a local directory. If a local directory and not absolute, it will be relative to the <code>basedir</code> parameter. This value is Jinja rendered. path String Yes The source document path in SharePoint. Use POSIX, not DOS, style path names (i.e. forward slash path separators). It must be an absolute path starting with <code>/</code> and a SharePoint folder. This value is Jinja rendered. vars Map[String,*] No A map of variables injected when the parameters are Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-parameters_2","title":"Jinja Rendering of Parameters","text":"<p>Some of the parameters are  rendered using Jinja.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, the source path can be dynamically set to include components such as the date or part of the target file name.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_14","title":"Dev Mode Behaviour","text":"<p>The sharepoint_get_multi_doc job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_11","title":"Examples","text":"<p>The following example downloads all csv files from SharePoint path and places it in S3. The file will be KMS encrypted in S3 with the specified KMS key.</p> <pre><code>{\n  \"description\": \"Download a CSV file from SharePoint\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-multi-doc\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"library\": \"My SharePoint Library\",\n    \"path\": \"/Interesting/Data/\",\n    \"outpath\": \"s3://my-bucket/base-prefix/\",\n    \"glob\": \"*.csv\",\n    \"kms_key_id\": \"alias/data\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_get_multi_doc\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one shows how Jinja can be used to include the current date in the source document name.</p> <pre><code>{\n  \"description\": \"Download a CSV file from SharePoint\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-multi-doc\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"library\": \"My SharePoint Library\",\n    \"path\": \"/Interesting/Data/{{start.strftime('%Y-%m-%d')}}\",\n    \"outpath\": \"s3://my-bucket/{{start.strftime('%Y-%m-%d')}}/\",\n    \"kms_key_id\": \"alias/data\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_get_multi_doc\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sharepoint_put_doc","title":"Job type: sharepoint_put_doc","text":"<p>The sharepoint_put_doc job type creates or updates a SharePoint document from a specified source file.</p> <p>Connection to the target SharePoint site is handled automatically by lava.</p>"},{"location":"06-jobs-job-types.html#payload_14","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_14","title":"Parameters","text":"Parameter Type Required Description basedir String No If the source file is specified as a relative filename, it will be treated as relative to the specified directory. Defaults to the lava temporary directory for the job. conn_id String Yes The connection ID for a SharePoint site. file String Yes The source file name. If it starts with <code>s3://</code> it is assumed to be an object in S3, otherwise a local file. If a local file and not absolute, it will be relative to the <code>basedir</code> parameter. This value is Jinja rendered. jinja Boolean No If <code>false</code>, disable Jinja rendering. Default <code>true</code>. library String Yes Target SharePoint library name. This value is Jinja rendered. path String Yes The target document path in SharePoint. Use POSIX, not DOS, style path names (i.e. forward slash path separators). It must be an absolute path starting with <code>/</code>. This value is Jinja rendered. title String No A title for the document. This value is Jinja rendered. vars Map[String,*] No A map of variables injected when the parameters are Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-parameters_3","title":"Jinja Rendering of Parameters","text":"<p>Some of the parameters are  rendered using Jinja.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, target path can be dynamically set to include components such as the date or part of the source file name.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_15","title":"Dev Mode Behaviour","text":"<p>The sharepoint_put_doc job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_12","title":"Examples","text":"<p>The following example uploads a file from S3 to a SharePoint library.</p> <pre><code>{\n  \"description\": \"Upload a CSV file to SharePoint\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-doc\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"library\": \"My SharePoint Library\",\n    \"path\": \"/Interesting/Data/x.csv\",\n    \"file\": \"s3://my-bucket/x.csv\",\n    \"title\": \"This is an interesting data set\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_put_doc\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one shows how Jinja can be used to replicate the source file name into the target document name.</p> <pre><code>{\n  \"description\": \"Upload a CSV file to SharePoint\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-doc\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"library\": \"My SharePoint Library\",\n    \"path\": \"/Interesting/Data/{{job.parameters.file}}.split('/')[-1]}}\",\n    \"file\": \"s3://my-bucket/x.csv\",\n    \"title\": \"This is an interesting data set\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_put_doc\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one shows how Jinja can be used to include the current date in the target document name.</p> <pre><code>{\n  \"description\": \"Upload a CSV file to SharePoint\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-doc\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"library\": \"My SharePoint Library\",\n    \"path\": \"/Interesting/Data/{{start.strftime('%Y-%m-%d')}}.csv\",\n    \"file\": \"s3://my-bucket/x.csv\",\n    \"title\": \"This is an interesting data set\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_put_doc\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sharepoint_put_list","title":"Job type: sharepoint_put_list","text":"<p>The sharepoint_put_list job type updates a SharePoint list from a specified CSV source file.</p> <p>Connection to the target SharePoint site is handled automatically by lava.</p>"},{"location":"06-jobs-job-types.html#payload_15","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_15","title":"Parameters","text":"Parameter Type Required Description basedir String No If the source file is specified as a relative filename, it will be treated as relative to the specified directory. Defaults to the lava temporary directory for the job. conn_id String Yes The connection ID for a SharePoint site. data_columns String No A comma separated list of column names. If specified, then only columns listed are modified. Any columns specified in SharePoint as required must be included in the source data for modes <code>append</code> and <code>replace</code>. Required columns present in the source data will be included in the append/replace even if not explicitly included in the <code>data_columns</code> list. delimiter String No Single character field delimiter. Default <code>|</code>. doublequote Boolean No As for csv.reader. Default <code>false</code>. error_missing Boolean No If <code>true</code> and there are columns in the source file that are not in the SharePoint list, raise an error. If <code>false</code>, the extra columns are silently ignored. Default <code>false</code>. escapechar String No As for csv.reader. Default <code>null</code>. file String Yes The source file name. If it starts with <code>s3://</code> it is assumed to be an object in S3, otherwise a local file. If a local file and not absolute, it will be relative to the <code>basedir</code> parameter. This value is Jinja rendered. The contents must be a CSV data with a single header line. The columns must match the pre-existing list in SharePoint. jinja Boolean No If <code>false</code>, disable Jinja rendering. Default <code>true</code>. list String Yes Name of the list. It must already exist in SharePoint. This will be jinja rendered. mode String No See below. Default is <code>append</code>. quotechar String No As for csv.reader. Default <code>\"</code>. quoting String No As for csv.reader <code>QUOTE_*</code> parameters (without the QUOTE_ prefix). Default <code>minimal</code> (i.e. <code>QUOTE_MINIMAL</code>). vars Map[String,*] No A map of variables injected when the parameters are Jinja rendered. <p>The <code>mode</code> parameter can take the following values:</p> Mode Description append Rows are added to the existing list contents. delete Rows are deleted based on an <code>ID</code> column that must be present in the source data. No new data is added. replace Existing list contents are a deleted before adding new data. update Rows are updated based on an <code>ID</code> column that must be present in the source data. Fields in existing rows may be updated but no new rows will be added. <p>Note that any read-only columns in the SharePoint list are not updated. These are silently skipped in the update process.</p>"},{"location":"06-jobs-job-types.html#handling-of-personorgroup-columns_1","title":"Handling of personorgroup Columns","text":"<p>Columns of type <code>personorgroup</code> have an associated <code>LookupId</code> column. So for a column named <code>&lt;COLUMN&gt;</code>, the list also has a <code>&lt;COLUMN&gt;LookupId</code> column.</p> <p>A <code>personorgroup</code> type column can only be updated if it\u2019s provided using the name <code>&lt;COLUMN&gt;LookupId</code> and the actual <code>LookupId</code> value</p>"},{"location":"06-jobs-job-types.html#jinja-rendering-of-parameters_4","title":"Jinja Rendering of Parameters","text":"<p>Some of the parameters are  rendered using Jinja.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, target path can be dynamically set to include components such as the date or part of the source file name.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_16","title":"Dev Mode Behaviour","text":"<p>The sharepoint_put_list job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_13","title":"Examples","text":"<p>The following example replaces the contents of a list with data from S3.</p> <pre><code>{\n  \"description\": \"Upload a CSV file to a SharePoint list\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sharepoint-list\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"list\": \"My List\",\n    \"file\": \"s3://my-bucket/x.csv\",\n    \"mode\": \"replace\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"sharepoint_put_list\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-smb_get","title":"Job type: smb_get","text":"<p>The smb_get job type downloads a file from an SMB file server to the specified destination file.</p> <p>Connection to the target SMB server is handled automatically by lava.</p>"},{"location":"06-jobs-job-types.html#payload_16","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_16","title":"Parameters","text":"Parameter Type Required Description basedir String No If the target file is specified as a relative filename, it will be treated as relative to the specified directory. Defaults to the lava temporary directory for the job. conn_id String Yes The connection ID for an SMB file share. file String Yes The destination file name. If it starts with <code>s3://</code> it is assumed to be an object in S3, otherwise a local file. If a local file and not absolute, it will be relative to the <code>basedir</code> parameter. This value is Jinja rendered. jinja Boolean No If <code>false</code>, disable Jinja rendering. Default <code>true</code>. kms_key_id String No An AWS KMS encryption key to use when uploading to AWS S3. path String Yes The source file path on the SMB file share. Use POSIX, not DOS, style path names (i.e. forward slash path separators). This value is Jinja rendered. share_name String Yes The name of the file share. vars Map[String,*] No A map of variables injected when the parameters are Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-parameters_5","title":"Jinja Rendering of Parameters","text":"<p>Some of the parameters are  rendered using Jinja.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, the source path can be dynamically set to include components such as the date or part of the target file name.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_17","title":"Dev Mode Behaviour","text":"<p>Job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_14","title":"Examples","text":"<p>The following example downloads a file from an SMB file share and places it in S3.  The file will be KMS encrypted in S3 with the specified KMS key.</p> <pre><code>{\n  \"description\": \"Download a CSV file from a file share\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/smb-get\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"smb-conn-01\",\n    \"share_name\": \"Public\",\n    \"path\": \"/Interesting/Data/x.csv\",\n    \"file\": \"s3://my-bucket/x.csv\",\n    \"kms_key_id\": \"alias/data\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"smb_get\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one shows how Jinja can be used to include the current date in the source file name.</p> <pre><code>{\n  \"description\": \"Download a CSV file from a file share\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/smb-get\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"smb-conn-01\",\n    \"share_name\": \"Public\",\n    \"path\": \"/Interesting/Data/{{start.strftime('%Y-%m-%d')}}.csv\",\n    \"file\": \"s3://my-bucket/{{start.strftime('%Y-%m-%d')}}.csv\",\n    \"kms_key_id\": \"alias/data\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"smb_get\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-smb_put","title":"Job type: smb_put","text":"<p>The smb_put job type creates or updates a file on an SMB file share from a specified source file.</p> <p>Connection to the target SMB server is handled automatically by lava.</p>"},{"location":"06-jobs-job-types.html#payload_17","title":"Payload","text":"<p>The payload is ignored.</p>"},{"location":"06-jobs-job-types.html#parameters_17","title":"Parameters","text":"Parameter Type Required Description basedir String No If the source file is specified as a relative filename, it will be treated as relative to the specified directory. Defaults to the lava temporary directory for the job. conn_id String Yes The connection ID for an SMB server. create_dirs Boolean No If <code>true</code>, the target directory, including parent directories, will be created if it doesn't exist. Default is <code>false</code>. file String Yes The source file name. If it starts with <code>s3://</code> it is assumed to be an object in S3, otherwise a local file. If a local file and not absolute, it will be relative to the <code>basedir</code> parameter. This value is Jinja rendered. jinja Boolean No If <code>false</code>, disable Jinja rendering. Default <code>true</code>. path String Yes The target path on the SMB file share. Use POSIX, not DOS, style path names (i.e. forward slash path separators). This value is Jinja rendered. share_name String Yes The name of the file share. vars Map[String,*] No A map of variables injected when the parameters are Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-parameters_6","title":"Jinja Rendering of Parameters","text":"<p>Some of the parameters are rendered using Jinja.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, target path can be dynamically set to include components such as the date or part of the source file name.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_18","title":"Dev Mode Behaviour","text":"<p>Job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_15","title":"Examples","text":"<p>The following example uploads a file from S3 to an SMB file share.</p> <pre><code>{\n  \"description\": \"Upload a CSV file to an SMB file share\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/smb-put\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"smb-conn-01\",\n    \"share_name\": \"Public\",\n    \"path\": \"/Interesting/Data/x.csv\",\n    \"file\": \"s3://my-bucket/x.csv\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"smb_put\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one shows how Jinja can be used to replicate the source file name into the target file name.</p> <pre><code>{\n  \"description\": \"Upload a CSV file to an SMB file share\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/smb-put\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"smb-conn-01\",\n    \"share_name\": \"Public\",\n    \"path\": \"/Interesting/Data/{{job.parameters.file}}.split('/')[-1]}}\",\n    \"file\": \"s3://my-bucket/x.csv\"\n  },\n  \"payload\": \"--\",\n  \"type\": \"smb_put\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one shows how Jinja can be used to include the current date in the target path. Note that target directories are created on the fly.</p> <pre><code>{\n  \"description\": \"Upload a CSV file to an SMB file share\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/smb-put\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"sp-conn-01\",\n    \"share_name\": \"Public\",\n    \"path\": \"/Interesting/Data/{{start.strftime('%Y/%m/%d')}}/data.csv\",\n    \"file\": \"s3://my-bucket/x.csv\",\n    \"title\": \"This is an interesting data set\",\n    \"create_dirs\": true\n  },\n  \"payload\": \"--\",\n  \"type\": \"smb_put\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sql","title":"Job type: sql","text":"<p>The sql job type runs one or more SQL statements.</p> <p>The sql job type is functionally identically to the sqli job type with the exception that sql obtains the SQL statements from files in S3 whereas for sqli jobs the payload is inline in the job specification.</p> <p>Tip</p> <p>For help selecting the appropriate SQL job type, See Choosing an SQL Job Type.</p> <p>Connection to the target database is handled automatically by lava.</p> <p>The sql job type is intended for simple SQL sequences. For more complex cases, the sqlc or sqlv job types may be more appropriate.</p>"},{"location":"06-jobs-job-types.html#payload_18","title":"Payload","text":"<p>The payload is a location in S3 relative to the <code>s3_payloads</code> area specified in the realms table. It can be either an object key, in which case a single file is downloaded, or a prefix ending in /, in which case all files under that prefix will be downloaded and run in lexicographic order.</p> <p>See S3 Payloads for more information.</p> <p>Info</p> <p>The <code>sql</code> job type also allows the payload to be a list of S3 locations if the v2 Payload Downloader is enabled.</p> <p>Each payload file can contain one or more SQL statements that are compatible with the target database.</p> <p>Note that some database drivers are pickier than others about the presence or absence of a terminating semi-colon (e.g. Oracle) so these will be stripped from the end of each statement before execution.</p>"},{"location":"06-jobs-job-types.html#parameters_18","title":"Parameters","text":"<p>The formatting related parameters are as defined for the Python CSV writer, although some of the defaults are different. Defaults can be overridden at the realm level using configuration variables.</p> Parameter Type Required Description batch_size Integer No Fetch this many rows at a time. Default is 1000. conn_id String Yes The connection ID for an SQL database. delimiter String No Single character field delimiter. Default <code>|</code>. dialect String No As for csv.writer. Default <code>excel</code>. The <code>unix</code> option is useful when DOS style line endings must be avoided. doublequote Boolean No As for csv.writer. Default <code>false</code>. escapechar String No As for csv.writer. Default <code>null</code>. header Boolean No Add a header for data produced from SELECT queries. Default is <code>false</code>. jinja Boolean No If <code>false</code>, disable Jinja rendering of the payload. Default <code>true</code>. output String No If specified, the output from statements that produce result tuples will be placed in this subdirectory in both the job run temporary area in the local filesystem and in the <code>s3_temp</code> area. This option must be specified for dag jobs if the output is needed. See Output Data below. It must contain only alphanumeric characters and underscores. quotechar String No As for csv.writer. Default <code>\"</code>. quoting String No As for csv.writer <code>QUOTE_*</code> parameters (without the QUOTE_ prefix). Default <code>minimal</code> (i.e. <code>QUOTE_MINIMAL</code>). raw Boolean No By default, an attempt will be made to split each payload file into individual SQL statements. This should be safe in most cases. To suppress this behaviour and run the payload as-is, set raw to <code>true</code>. Default <code>false</code>. transaction Boolean No If <code>true</code>, auto-commit is disabled and the sequence of SQLs is run within a transaction. If <code>false</code>, auto-commit is enabled (if supported by the driver). Default <code>false</code>. vars Map[String,*] No A map of variables injected when the SQL is Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-payload_1","title":"Jinja Rendering of the Payload","text":"<p>Each SQL statement is rendered using Jinja prior to execution.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, the S3 location of the unload (<code>s3key</code>) can be dynamically set to include components such as the schema, table name, unload date etc.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#output-data","title":"Output Data","text":"<p>If any of the queries in the payload produce result tuples (e.g. <code>SELECT</code> statements), the output is placed in its own file in the local file system and in the <code>s3_temp</code> area.</p> <p>If the <code>output</code> parameter is not specified, the output files will be placed in the root of the temporary run directory on the local worker file system and at the root of the job run's <code>s3_temp</code> area.</p> <p>If the <code>output</code> parameter is specified, the output files will be placed in a subdirectory of that name under the root of the temporary run directory on the local worker file system and under a similarly named sub-prefix of the root of the job run's <code>s3_temp</code> area.</p> <p>Files are named <code>&lt;PAYLOAD&gt;.&lt;n&gt;.out</code>, where:</p> <ul> <li><code>&lt;PAYLOAD&gt;</code> is the name of the payload file containing the <code>SELECT</code> query;     and</li> <li><code>&lt;n&gt;</code> is the SQL statement sequence number within that file, starting from     zero.</li> </ul> <p>So with an <code>output</code> parameter of <code>whatever</code> and a payload file, <code>some-queries.sql</code>, containing only a <code>SELECT</code> statement, the output file would be named <code>whatever/some-queries.sql.0.out</code>.</p> <p>As child jobs in chain and dag jobs all share the same run directory, this provides a mechanism for one child job to leave data behind for a subsequent job.</p> <p>Info</p> <p>It is critical to use the <code>output</code> parameter in these circumstances.</p> <p>Unless modified by job parameters, the output of each <code>SELECT</code> statement is pipe separated values, one record per line.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_19","title":"Dev Mode Behaviour","text":"<p>The sql job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_16","title":"Examples","text":"<p>The following example runs the SQL statement in <code>demo/query-mytable.sql</code>. The output is written to <code>demo/query-mytable.sql.out</code> in the <code>s3_temp</code> area.</p> <pre><code>{\n  \"description\": \"Run a single SQL statement.\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sql-query-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"db-conn-01\",\n    \"delimiter\": \"|\",\n    \"quoting\": \"all\"\n  },\n  \"payload\": \"demo/query-mytable.sql\",\n  \"type\": \"sql\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sqlc","title":"Job type: sqlc","text":"<p>The sqlc job type runs one or more files containing native SQL client commands. It differs from the sql and sqli job types in that, instead of a Python DBAPI 2.0 driver, it uses a command line client specific to the target database type. This makes sqlc less portable than sql/sqli/sqlv as the former allows access to all the meta-commands available in the command line client.</p> <p>The sqlc job type is intended for more complex requirements where the specific capabilities of the native CLI are critical. In most cases, one of the sql, sqli or sqlv job types will be more appropriate.</p> <p>Tip</p> <p>For help selecting the appropriate SQL job type, See Choosing an SQL Job Type.</p> <p>The stdout and stderr of each SQL script, if any, is placed in its own file in the <code>s3_temp</code> area.</p> <p>For Postgres flavoured databases (including Redshift), the psql client is used. For MySQL databases, the mysql client is used. For Oracle databases, the SQL*Plus client is used. It's a horror. Sorry.</p>"},{"location":"06-jobs-job-types.html#payload_19","title":"Payload","text":"<p>The payload is a location in S3 relative to the <code>s3_payloads</code> area specified in the realms table. It can be either an object key, in which case a single file is downloaded, or a prefix ending in /, in which case all files under that prefix will be downloaded and run in lexicographic order.</p> <p>See S3 Payloads for more information.</p> <p>Note</p> <p>The <code>sqlc</code> job type also allows the payload to be a list of S3 locations if the v2 Payload Downloader is enabled.</p> <p>Each file may contain a mix of SQL statements and client meta-commands. The SQL must be compatible with the target database. SQL commands must always be properly terminated with semi-colons.</p>"},{"location":"06-jobs-job-types.html#parameters_19","title":"Parameters","text":"Parameter Type Required Description args List[String] No A list of zero or more additional arguments provided to the database client. These are necessarily specific to the database type and underlying database client. conn_id String Yes The connection ID for a database. jinja Boolean No If <code>false</code>, disable Jinja rendering of the payload. Default <code>true</code>. timeout String No By default, payload components run by sqlc jobs are killed after 10 minutes. This parameter can override that with values in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours). Note that the total timeout for the entire job (<code>timeout</code> multiplied by number of payload elements) must be less than the visibility timeout on the worker SQS queue. vars Map[String,*] No A map of variables injected when the SQL is Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-payload_2","title":"Jinja Rendering of the Payload","text":"<p>Each file in the payload is rendered using Jinja prior to execution.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, the S3 location of the unload (<code>s3key</code>) can be dynamically set to include components such as the schema, table name, unload date etc.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_20","title":"Dev Mode Behaviour","text":"<p>Normally, the sqlc job will copy stdout and stderr to S3 on the conclusion of the job. In dev mode, stdout and stderr are emitted locally during the job run instead of being copied to S3.</p>"},{"location":"06-jobs-job-types.html#examples_17","title":"Examples","text":"<p>The following example runs all of the commands in <code>demo/psql-commands.sql</code>. The <code>--no-align</code> argument is passed to the <code>psql</code> command line client. Stdout is written to <code>demo/sqlc-query-01.sql.stdout</code> and stderr is written to <code>demo/sqlc-query-01.sql.sterr</code> in the <code>s3_temp</code> area. The underlying database type is specified in the connection specification not the job specification but the payload must match the underlying database type.</p> <pre><code>{\n  \"description\": \"Run a psql file.\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sqlc-query-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"args\": [\n      \"--no-align\"\n    ],\n    \"conn_id\": \"pgdb-conn-01\"\n  },\n  \"payload\": \"demo/psql-commands.sql\",\n  \"type\": \"sql\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>This one runs all files found under the given payload prefix. Separate stdout and stderr files are created for each payload file. The timeout for each individual payload element is set to 30 minutes.</p> <pre><code>{\n  \"description\": \"Run a psql file.\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sqlc-query-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"args\": [\n      \"--no-align\"\n    ],\n    \"conn_id\": \"pgdb-conn-01\"\n  },\n  \"payload\": \"demo/psql-commands/\",\n  \"timeout\": \"30m\",\n  \"type\": \"sql\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sqli","title":"Job type: sqli","text":"<p>The sqli job type runs one or more SQL statements.</p> <p>The sqli job type is functionally identical to the sql job type with the exception that sqli obtains the SQL statements inline from the payload whereas for sql jobs the payload specifies S3 objects containing the SQL statements.</p> <p>Tip</p> <p>For help selecting the appropriate SQL job type, see Choosing an SQL Job Type.</p> <p>Connection to the target database is handled automatically by lava.</p> <p>The sqli job type is intended for simple, relatively small SQL sequences. For larger or more complex cases, the sql or sqlc job types may be more appropriate.</p>"},{"location":"06-jobs-job-types.html#payload_20","title":"Payload","text":"<p>The payload is a string, or list of strings, containing SQL statements.</p> <p>Each payload string can itself contain one or more SQL statements that are compatible with the target database.</p> <p>For statements that produce result tuples (e.g. <code>SELECT</code> statements), the output is placed in its own file in the <code>s3_temp</code> area. Unless modified by job parameters, the output of each statement is pipe separated values, one record per line.</p> <p>Note that some database drivers are pickier than others about the presence or absence of a terminating semi-colon (e.g. Oracle) so these will be stripped from the end of each statement before execution.</p>"},{"location":"06-jobs-job-types.html#parameters_20","title":"Parameters","text":"<p>Parameters are identical to those for the sql job type.</p>"},{"location":"06-jobs-job-types.html#output-data_1","title":"Output Data","text":"<p>If any of the queries in the payload produce result tuples (e.g. <code>SELECT</code> statements), the output is placed in its own file in the local file system and in the <code>s3_temp</code> area.</p> <p>If the <code>output</code> parameter is not specified, the output files will be placed in the root of the temporary run directory on the local worker file system and at the root of the job run's <code>s3_temp</code> area.</p> <p>If the <code>output</code> parameter is specified, the output files will be placed in a subdirectory of that name under the root of the temporary run directory on the local worker file system and under a similarly named sub-prefix of the root of the job run's <code>s3_temp</code> area.</p> <p>Files are named <code>&lt;m&gt;.&lt;n&gt;.out</code>, where:</p> <ul> <li><code>&lt;m&gt;</code> is the sequence number of the payload element, starting from zero.      Note that each payload element can contain multiple SQL statements.     Hence ...</li> <li><code>&lt;n&gt;</code> is the SQL statement sequence number within the payload element,     starting from zero.</li> </ul> <p>So, with an <code>output</code> parameter of <code>whatever</code> and a payload list containing a single element which is a <code>SELECT</code> statement, the output file would be named <code>whatever/0.0.out</code>.</p> <p>As child jobs in chain and dag jobs all share the same run directory, this provides a mechanism for one child job to leave data behind for a subsequent job.</p> <p>Info</p> <p>It is critical to use the <code>output</code> parameter in these circumstances.</p> <p>Unless modified by job parameters, the output of each <code>SELECT</code> statement is pipe separated values, one record per line.</p>"},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-payload_3","title":"Jinja Rendering of the Payload","text":"<p>Each SQL statement is rendered using Jinja prior to execution. The rendering process is identical to that for the sql job type.</p>"},{"location":"06-jobs-job-types.html#dev-mode-behaviour_21","title":"Dev Mode Behaviour","text":"<p>The sqli job behaviour is unchanged for dev mode.</p>"},{"location":"06-jobs-job-types.html#examples_18","title":"Examples","text":"<p>The following example runs a single SQL statement. The output is written to the <code>s3_temp</code> area.</p> <pre><code>{\n  \"description\": \"Run a single SQL statement.\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sql-query-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"db-conn-01\",\n    \"delimiter\": \"|\",\n    \"quoting\": \"all\"\n  },\n  \"payload\": \"SELECT numbers FROM lottery_results WHERE result_date &gt; NOW()\",\n  \"type\": \"sqli\",\n  \"worker\": \"default\"\n}\n</code></pre> <p>The following example runs multiple SQL statements in a transaction.</p> <pre><code>{\n  \"description\": \"Run multiple SQL statements.\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sql-query-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"db-conn-01\",\n    \"delimiter\": \"|\",\n    \"quoting\": \"all\",\n    \"transaction\": true\n  },\n  \"payload\": [\n    \"DELETE FROM main_table\",\n    \"INSERT INTO main_table (SELECT * FROM staging_table)\"\n  ],\n  \"type\": \"sqli\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"06-jobs-job-types.html#job-type-sqlv","title":"Job type: sqlv","text":"<p>The sqlv job type runs one or more files containing SQL statements. It is essentially a hybrid of the sql and sqlc job types. It has a consistent, lava controlled interface across all database types like the former, but uses an external CLI client like the latter.</p> <p>Tip</p> <p>For help selecting the appropriate SQL job type, see Choosing an SQL Job Type.</p> <p>The client, <code>lava-sql</code> is provided as part of the lava code base. It supports SQL statements only, not the meta-commands that are typical of the proprietary database clients used by sqlc. The lava-sql utility can also be used stand-alone or invoked by lava exe, pkg and docker jobs.</p> <p>Connection to the target database is handled automatically by lava.</p>"},{"location":"06-jobs-job-types.html#payload_21","title":"Payload","text":"<p>The payload is a location in S3 relative to the <code>s3_payloads</code> area specified in the realms table. It can be either an object key, in which case a single file is downloaded, or a prefix ending in /, in which case all files under that prefix will be downloaded and run in lexicographic order.</p> <p>See S3 Payloads for more information.</p> <p>Note</p> <p>The <code>sqlv</code> job type also allows the payload to be a list of S3 locations if the v2 Payload Downloader is enabled.</p> <p>Each payload file can contain one or more SQL statements that are compatible with the target database.</p> <p>For <code>SELECT</code> statements, the output is placed in its own file in the <code>s3_temp</code> area. Unless modified by job parameters, the output of each <code>SELECT</code> statement is pipe separated values, one record per line.</p> <p>Tip</p> <p>All output is placed in a single file. To avoid a mess, each job should either limit itself to a single <code>SELECT</code> statement that produces output or all <code>SELECT</code> statements should produce the same column structure.</p>"},{"location":"06-jobs-job-types.html#parameters_21","title":"Parameters","text":"<p>Some of the formatting related parameters are output format dependent. In most cases, defaults can be overridden at the realm level using configuration variables.</p> Parameter Type Required Description batch_size Integer No Fetch this many rows at a time. Default is 1000. conn_id String Yes The connection ID for an SQL database. delimiter String No Single character field delimiter. Default <code>|</code>. dialect String No As for csv.writer. Default <code>excel</code>. The <code>unix</code> option is useful when DOS style line endings must be avoided. doublequote Boolean No As for csv.writer. Default <code>false</code>. escapechar String No As for csv.writer. Default <code>null</code>. format String No Specify the output format for <code>SELECT</code> statements. Any of the formats supported by the lava-sql can be used. The default is <code>csv</code>. jinja Boolean No If <code>false</code>, disable Jinja rendering of the payload. Default <code>true</code>. header Boolean No Add a header for data produced from SELECT queries. Default is <code>false</code>. level String No Print log messages of a given severity level or above. The standard logging level names are available but <code>debug</code>, <code>info</code>, <code>warning</code> and <code>error</code> are most useful. The Default is <code>info</code>. quotechar String No As for csv.writer. Default <code>\"</code>. quoting String No As for csv.writer <code>QUOTE_*</code> parameters (without the QUOTE_ prefix). Default <code>minimal</code> (i.e. <code>QUOTE_MINIMAL</code>). raw Boolean No By default, an attempt will be made to split each payload file into individual SQL statements. This should be safe in most cases. To suppress this behaviour and run the payload as-is, set raw to <code>true</code>. Default <code>false</code>. timeout String No By default, jobs are killed after 10 minutes. This parameter can override that with values in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours). Note that the total timeout for the entire job (<code>timeout</code> multiplied by number of payload elements) must be less than the visibility timeout on the worker SQS queue. transaction Boolean No If <code>true</code>, auto-commit is disabled and the sequence of SQLs is run within a transaction. If <code>false</code>, auto-commit is enabled (if supported by the driver). Default <code>false</code>. vars Map[String,*] No A map of variables injected when the SQL is Jinja rendered."},{"location":"06-jobs-job-types.html#jinja-rendering-of-the-payload_4","title":"Jinja Rendering of the Payload","text":"<p>Each SQL statement is rendered using Jinja prior to execution.</p> <p>All of the injected parameters are effectively Python objects so the normal Jinja syntax and Python methods for those objects can be used in the Jinja templates. This is particularly useful for the datetime objects as <code>strftime()</code> becomes available. For example, the S3 location of the unload (<code>s3key</code>) can be dynamically set to include components such as the schema, table name, unload date etc.</p> <p>Refer to Jinja Rendering in Lava for more information.</p> <p>The following variables are made available to the renderer.</p> Name Type Description globals dict[str,*] The <code>globals</code> from the job specification updated with any globals received in the job dispatch. job dict[str,*] The augmented job specification. realm dict[str,*] The realm specification. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. vars dict[str,*] A dictionary of variables provided as the <code>vars</code> component of the job <code>parameters</code>."},{"location":"06-jobs-job-types.html#dev-mode-behaviour_22","title":"Dev Mode Behaviour","text":"<p>Normally, the sqlv job will copy stdout and stderr to S3 on the conclusion of the job. In dev mode, stderr is emitted locally during the job run instead of being copied to S3. Stdout will still be copied to S3 as it may contain binary information.</p>"},{"location":"06-jobs-job-types.html#examples_19","title":"Examples","text":"<p>The following example runs the SQL statements in <code>demo/query-mytable.sql</code>. The output is written to <code>demo/query-mytable.sql.out</code> in the <code>s3_temp</code> area.</p> <pre><code>{\n  \"description\": \"Run a file containing SQL statements.\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"demo/sql-query-01\",\n  \"owner\": \"demo@somewhere.com\",\n  \"parameters\": {\n    \"conn_id\": \"db-conn-01\",\n    \"delimiter\": \"|\",\n    \"quoting\": \"all\",\n    \"timeout\": \"20m\",\n    \"transaction\": true\n  },\n  \"payload\": \"demo/queries.sql\",\n  \"type\": \"sqlc\",\n  \"worker\": \"default\"\n}\n</code></pre>"},{"location":"07-connectors.html","title":"Connectors","text":"<p>Lava provides a mechanism to assist with connections to external resources to minimise the need for individual jobs to manage connectivity details and credentials. This also simplifies the process of migrating jobs from one environment or lava realm to another.</p> <p>Configuration information for connection handlers (aka connectors) is stored in the connections table. The required fields are dependent on the connector type.</p> <p>Note</p> <p>Loosely speaking, a connector is a handler for a specific type of resource and a connection is an instance of a connector for a specific instance of a resource.</p> <p>It probably doesn't matter that much and, historically, the user guide has played fast and loose with this distinction.</p> <p>The underlying implementation of connectors is specific to the type of target resource and the job type. Lava attempts to provide a connection handle to jobs in a form that is relatively native to the job type. For example, a database connection for an sql job is provided as a Python DBAPI 2.0 connection instance. For an exe or pkg job, it is provided as a command line wrapper that handles credential management and connectivity behind the scenes.</p> <p>Connector credentials are typically stored in the AWS SSM Parameter Store to provide a level of isolation and security. SSM parameters for a given realm should be stored with parameter names starting with <code>/lava/&lt;REALM&gt;/</code>.</p> <p>Connectors are implemented using a simple plugin architecture and new ones can be added relatively easily.</p>"},{"location":"07-connectors.html#database-connectors","title":"Database Connectors","text":"<p>Lava provides database connectors for a number of common database types, including MySQL, Postgres and Oracle.</p> <p>If used with sql, sqlc, sqli, sqlv, db_from_s3 and redshift_unload jobs, lava manages the connection process in the background.</p> <p>If used with exe, pkg and docker jobs, lava provides an environment variable pointing to a script that will connect to the database to run SQL. The executable in the job payload can run the script to access the database without worrying about managing database connectivity.</p> <p>Python programs in job payloads can access the lava connector subsystem directly to obtain either a DBAPI 2.0 connection object or an SQLAlchemy engine object. Refer to Developing Lava Jobs for more information.</p>"},{"location":"07-connectors.html#database-authentication-using-aws-ssm-parameter-store","title":"Database Authentication Using AWS SSM Parameter Store","text":"<p>The database connectors typically require a number of connection and authentication parameters to be specified, such as:</p> <ul> <li>host name</li> <li>port</li> <li>user name</li> <li>password.</li> </ul> <p>These can be defined explicitly in the connection specification, except for the password. By default, the value of this field is interpreted as the name of an encrypted SSM parameter that contains the actual password.</p> <p>The standard lava worker IAM policies will provide read access to SSM parameters with names of the form <code>/lava/&lt;REALM&gt;/*</code>. These must be encrypted with the realm KMS key <code>lava-&lt;REALM&gt;-sys</code>.</p>"},{"location":"07-connectors.html#database-authentication-using-aws-secrets-manager","title":"Database Authentication Using AWS Secrets Manager","text":"<p>The lava database connectors support the AWS Secrets Manager as an alternative source for some of the connection specification parameters where they are not provided directly in the specification.</p> <p>If the connection specification contains a <code>secret_id</code> field, a field in the named secret will be used to populate a missing component in the connector specification.</p> <p>Note that Secrets Manager and lava use slightly different naming conventions for fields. Lava will map Secrets Manager fields to lava fields automatically using the following translation:</p> Secrets Manager Field Lava Field dbClusterIdentifier description dbname database host host password password port port serviceName service_name sid sid username user <p>The standard lava worker IAM policies will provide read access to secrets with names of the form <code>/lava/&lt;REALM&gt;/*</code>. These must be encrypted with the realm KMS key <code>lava-&lt;REALM&gt;-sys</code>.</p>"},{"location":"07-connectors.html#database-authentication-using-iam-credential-generation","title":"Database Authentication Using IAM Credential Generation","text":"<p>Some AWS database types provide an IAM based mechanism for obtaining temporary database credentials. Lava supports this mechanism for some connectors. The mechanism will be used where the connection specification (after inclusion of any AWS Secrets Manager components) does not contain a password.</p> <p>Refer to individual connector details for more information.</p>"},{"location":"07-connectors.html#database-client-application-identification","title":"Database Client Application Identification","text":"<p>Some database types support a mechanism for the client to identify itself when connecting, in addition to the user authentication. This information may then be available in things such as connection logs, activity logs etc. The mechanism used is database dependent and not all databases provide a mechanism.</p> <p>Lava attempts to provide a uniform interface to the underlying database client identification mechanism where possible.</p> <p>For most of the built in database related job types, lava will automatically provide a client identifier when connecting. By default, this is in the form <code>lv-&lt;REALM&gt;-&lt;JOB-ID&gt;</code>. (See the <code>CONN_APP_NAME</code> worker configuration parameter.)</p> <p>Support in sqlc jobs is dependent on the capabilities of the database specific CLI tool used to support the connection. Likewise for executable job types (e.g. exe and pkg) using a CLI based connector. See also Connection Handling for Executable Jobs.</p> <p>When using the lava API <code>get_pysql_connection()</code>, a new, optional <code>application_name</code> parameter is available. If a value is not provided, a value in the form described above is used if the lava job ID can be determined from the presence of a <code>LAVA_JOB_ID</code> environment variable. This should work whenever the API is being used within a lava job. See also Connection Handling for Python Based Jobs.</p> <p>In short, in most normal usage patterns for databases for which lava supports client identification, it will, more or less, do the right thing without modifying jobs or additional configuration.</p> <p>Lava's support for a client identification mechanism is summarised in the following table:</p> Job Type MS SQL MySQL Oracle Postgres Redshift SQLite sql Yes Yes Yes Yes sqli Yes Yes Yes Yes sqlc Yes Yes sqlv Yes Yes Yes Yes db_from_s3 Yes Yes Yes Yes redshift_unload Yes lava-sql CLI (1) (1) (1) (1) Lava API (2) (2) (2) (2) <p>Notes:</p> <ol> <li> <p>The lava-sql utility will automatically populate a client connection     identifier when used as part of a lava job payload. In other usages, the     <code>-a</code> / <code>--app-name</code> argument will need to be specified.</p> </li> <li> <p>The <code>get_pysql_connection()</code> API will automatically populate a client     connection identifier when used as part of a lava job payload. In other     usages, the otherwise optional <code>application_name</code> parameter will need to be     specified.</p> </li> </ol> <p>Note</p> <p>This article by Andy Grunwald was very helpful when implementing database client identification in lava: your database connection deserves a name</p>"},{"location":"07-connectors.html#client-application-identification-for-postgres","title":"Client Application Identification for Postgres","text":"<p>Postgres flavoured databases use the <code>application_name</code> connection parameter to identify client connections. Postgres will truncate the supplied value to 63 characters.</p> <p>The following sample query will display connected application names.</p> <pre><code>SELECT usename, application_name, client_addr, backend_type\nFROM pg_stat_activity;\n</code></pre>"},{"location":"07-connectors.html#client-application-identification-for-redshift","title":"Client Application Identification for Redshift","text":"<p>Redshift, like Postgres, uses the <code>application_name</code> connection parameter to identify client connections. Redshift allows application names up to 250 characters.</p> <p>The following sample query can display application names:</p> <pre><code>SELECT RTRIM(username)         AS user,\n       sessionid,\n       SUBSTRING(event, 1, 20) AS event,\n       recordtime,\n       RTRIM(authmethod)       AS auth,\n       RTRIM(sslversion)       AS ssl,\n       RTRIM(application_name) AS app_name\nFROM stl_connection_log\nORDER BY recordtime DESC;\n</code></pre>"},{"location":"07-connectors.html#client-application-identification-for-mysql","title":"Client Application Identification for MySQL","text":"<p>MySQL use the <code>program_name</code> connection parameter to identify client connections.</p> <p>The performance schema must be enabled to run queries that access the <code>program_name</code> parameter. For AWS Aurora instances, see Turning on the Performance Schema for Performance Insights on Aurora MySQL for information on enabling the performance schema.</p> <p>The following sample query, when run as an admin user, shows currently active connections:</p> <pre><code>SELECT\n session_connect_attrs.ATTR_VALUE AS program_name,\n processlist.*\nFROM information_schema.processlist\nLEFT JOIN  performance_schema.session_connect_attrs ON (\n processlist.ID = session_connect_attrs.PROCESSLIST_ID\n AND session_connect_attrs.ATTR_NAME = \"program_name\"\n)\n</code></pre> <p>The following query shows active connections for the current user:</p> <pre><code>SELECT\n session_account_connect_attrs.ATTR_VALUE AS program_name,\n processlist.*\nFROM information_schema.processlist\nLEFT JOIN  performance_schema.session_account_connect_attrs ON (\n processlist.ID = session_account_connect_attrs.PROCESSLIST_ID\n AND session_account_connect_attrs.ATTR_NAME = \"program_name\";\n</code></pre>"},{"location":"07-connectors.html#client-application-identification-for-sql-server-ms-sql","title":"Client Application Identification for SQL Server (MS SQL)","text":"<p>SQL Server uses the <code>program_name</code> connection parameter to identify client connections.</p> <p>The following sample query, when run as an admin user, shows currently active connections:</p> <pre><code>SELECT hostname, program_name, loginame, cmd\nFROM sys.sysprocesses\nWHERE loginame != 'rdsa';\n</code></pre>"},{"location":"07-connectors.html#other-connectors","title":"Other Connectors","text":"<p>Lava also provides connectors for various other types of resource, including sFTP servers, SharePoint sites, SMB fileshares and the AWS CLI. These are typically used either by a job type that is specific to the target resource or in exe, pkg and docker jobs.</p>"},{"location":"07-connectors.html#connector-type-aws","title":"Connector type: aws","text":"<p>The aws connector manages access to AWS access keys. It supports static access keys as well as session credentials obtained by assuming an IAM role in either the current AWS account or another account.</p> <p>Note</p> <p>IAM assumed role session credentials are new in version 8.1 (K\u012blauea).</p> <p>When used with redshift_unload jobs, this connector provides the access keys that are used in the S3 <code>AUTHORIZATION</code> parameters in the <code>UNLOAD</code> command.</p> <p>When used with db_from_s3 jobs, this connector provides the access keys that are used to provide the database the required access to S3 to load the data.</p> <p>When used with exe and pkg jobs, it provides an environment variable pointing to a script that will run the AWS CLI with an appropriate AWS authentication profile.</p> Field Type Required Description access_keys String Note 1 The name of an encrypted SSM parameter containing the access keys. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code>. The value must be in the format <code>access_key_id,access_secret_key</code> and must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. conn_id String Yes Connection identifier. description String No Description. duration Duration No The duration of a session created when an IAM role is assumed. Defaults to the value of the AWS_CONN_DURATION configuration parameter. As AWS credentials are cached, it is critical that this is significantly longer than the cache duration as specified by the AWS_ACCESS_KEY_CACHE_TTL parameter. enabled Boolean Yes Whether or not the connection is enabled. external_id String No Name of an SSM parameter containing an external ID to use when assuming a role to obtain session credentials. While AWS does not consider this to be a sensitive security parameter, it is stored in the SSM parameter store for ease of management. It is still recommended to use a secure parameter. Can't hurt. policy_arns String | List[String] No The ARNs of IAM managed policies to use as managed session policies. The policies must exist in the same account as the role. The session permissions are the intersection of these policies and the policies of the role being assumed. It is not possible to expand the underlying role permissions. policy Map[String,*] No An IAM policy to use as an inline session policy. The value must be a fully-formed AWS IAM policy. The session permissions are the intersection of the specified policy and the policies of the role being assumed. It is not possible to expand the underlying role permissions. region String No The AWS region name. If not specified, the current region is assumed. role_arn String Note 1 The ARN of an IAM role to assume to obtain session credentials. tags Map[String,String] No A map of session tags to pass. See Tagging Amazon Web Services STS Sessions. type String Yes <code>aws</code>. <p>Notes:</p> <ol> <li> <p>One of <code>access_keys</code> or <code>role_arn</code> must be specified.</p> </li> <li> <p>If a <code>role_arn</code> is specified, The trust policy on the role must allow it to     be assumed by the lava worker. If session tags are specified using the <code>tags</code>     field, the trust policy must also permit this.</p> </li> <li> <p>When assuming a role, the lava worker will set the role session name. By     default, this is in the form <code>lv-&lt;REALM&gt;-&lt;JOB-ID&gt;</code>, cleansed as necessary to     satisfy the requirements for session names. (See the <code>CONN_APP_NAME</code> worker configuration parameter.)</p> </li> </ol>"},{"location":"07-connectors.html#using-the-aws-connector-in-shell-scripts","title":"Using the AWS Connector in Shell Scripts","text":"<p>The aws connector creates a small shell script that is a wrapper around the AWS CLI that handles the access keys. The shell script is a drop in replacement for the AWS CLI when used in lava jobs.</p> <p>Consider the following exe job:</p> <pre><code>{\n    \"description\": \"Show usage of aws CLI connector in a shell script\",\n    \"enabled\": true,\n    \"job_id\": \"aws-cli-example\",\n    \"parameters\": {\n        \"connections\": {\n            \"aws1\": \"aws-conn-id-1\",\n            \"aws2\": \"aws-conn-id-2\"\n        }\n    },\n    \"payload\": \"example/aws-cli-conn.sh\",\n    \"type\": \"exe\",\n    \"worker\": \"core\"\n}\n</code></pre> <p>The <code>connections</code> element in the <code>parameters</code> will result in lava preparing connector shell scripts whose names are placed in the environment variables <code>LAVA_CONN_AWS1</code> and <code>LAVA_CONN_AWS2</code> respectively.</p> <p>The payload (<code>example/aws-cli-conn.sh</code> in this example) can then use these scripts just like the AWS CLI. For example:</p> <pre><code>#!/bin/bash\n\n$LAVA_CONN_AWS1 sts get-caller-identity\n\n$LAVA_CONN_AWS2 s3 ls\n</code></pre>"},{"location":"07-connectors.html#using-the-aws-connector-in-python","title":"Using the AWS Connector in Python","text":"<p>Note</p> <p>New in version 8.1 (K\u012blauea).</p> <p>Python jobs can call the lava connector subsystem directly via the lava API.</p> <p>Consider the following exe job:</p> <pre><code>{\n    \"description\": \"Show usage of aws CLI connector in a Python program\",\n    \"enabled\": true,\n    \"job_id\": \"aws-python-example\",\n    \"parameters\": {\n        \"connections\": {\n            \"aws3\": \"aws-conn-id-3\"\n        }\n    },\n    \"payload\": \"example/aws-cli-conn.py\",\n    \"type\": \"exe\",\n    \"worker\": \"core\"\n}\n</code></pre> <p>Once again, lava will create a shell script accessed via the <code>LAVA_CONN_AWS3</code> environment variable. It will also populate the <code>LAVA_CONNID_AWS3</code> environment variable with the connection ID. This can be used with the lava connector API to obtain a boto3 Session object, thus:</p> <pre><code>import os\nfrom lava.connection import get_aws_session\n\nrealm = os.environ['LAVA_REALM']\n\n# Note we want the connection ID, not the CLI script here.\nconn_id = os.environ['LAVA_CONNID_AWS3']\n\n# Use the lava API to the connection subsystem to obtain a boto3 Session.\naws_session = get_aws_session(conn_id, realm)\n\nsts = aws_session.client('sts')\nprint(sts.get_caller_identity())\n</code></pre> <p>Note</p> <p>A Python script can use the CLI script as well (e.g. via the subprocess module) but why would you want to?</p>"},{"location":"07-connectors.html#connector-type-docker","title":"Connector type: docker","text":"<p>The docker connector manages access to a docker daemon and docker registry for use with docker jobs.</p> <p>Lava supports the following registry options:</p> <ul> <li>AWS ECR</li> <li>Private docker registries</li> <li>The standard docker public registry.</li> </ul> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. email String No Email address for registry login. enabled Boolean Yes Whether or not the connection is enabled. password String No Name of the SSM parameter containing the password for authenticating to the registry. Required for private docker repositories. Ignored for ECR registries. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. registry String No Either the URL for a standard registry or <code>ecr[:account-id]</code>. In the latter case, lava will connect to the AWS ECR registry in the specified AWS account or the current account if no <code>account-id</code> is specified. If no registry is specified, the default public docker registry is used. server String No URL for the docker server. If not specified, then the normal docker environment variables are used. Generally, this means using the local docker daemon accessed via the UNIX socket. timeout Number No Timeout on docker API calls in seconds. tls Boolean No Use TLS when connecting to the docker server. Default True. type String Yes <code>docker</code>. user String No User name for authenticating to the registry. Required for private docker repositories. Ignored for ECR registries."},{"location":"07-connectors.html#accessing-external-registries","title":"Accessing External Registries","text":"<p>Lava prefers to obtain its docker images from the local AWS ECR. It's safer, simpler and more robust than relying on external registries to provide safe, secure code at run-time, particularly for a production environment.</p> <p>Tip</p> <p>If you need to use an external image, copy it to the local AWS ECR and use it from there. The lava job framework will place the built payloads for docker jobs in ECR. A trivial Dockerfile can copy an external image as part of the build process.</p> <p>If you must do this damn fool thing, lava permits it. There are some considerations:</p> <ol> <li> <p>Private registries (i.e. requiring authentication to access) will     require a connection specification as described above, including the      registry identifier and credentials. The registry will also be part of the     image name as usual.</p> </li> <li> <p>Public registries, such as Docker Hub and public repositories on GitHub     Container Registry (GHCR), can be addressed by a common connection     specification containing neither registry, nor credentials. The registry     will be part of the image name as usual (except for Docker Hub which is the     default registry).</p> </li> <li> <p>Proxies can be a problem. Lava will not help you here. The docker daemon     proxy configuration will need to be handled at the platform level, however     that is done.</p> </li> </ol>"},{"location":"07-connectors.html#examples","title":"Examples","text":"ECR ConnectorPublic RegistriesPrivate Registries <p>This is the standard connection specification for the local AWS ECR.</p> <pre><code>{\n    \"type\": \"docker\",\n    \"conn_id\": \"docker/ecr\",\n    \"description\": \"Docker ECR connection\",\n    \"enabled\": true,\n    \"registry\": \"ecr\",\n}\n</code></pre> <p>This connection specification should handle most public registries.</p> <pre><code>{\n    \"type\": \"docker\",\n    \"conn_id\": \"docker/public\",\n    \"description\": \"Docker basic connection (covers public repos)\",\n    \"enabled\": true\n}\n</code></pre> <p>This connection specification is for a private registry on the Github Container Registry:</p> <pre><code>{\n    \"type\": \"docker\",\n    \"conn_id\": \"docker/ghcr/xyzzy\",\n    \"description\": \"Github Container Registry for user xyzzy\",\n    \"enabled\": true,\n    \"registry\": \"ghcr.io\"\n    \"user\": \"not-used-by-ghcr\",\n    \"password\": \"/lava/my-realm/ghcr/xyzzy/access-token\"\n}\n</code></pre>"},{"location":"07-connectors.html#connector-type-email","title":"Connector type: email","text":"<p>The email connector provides a generic interface for an email sending subsystem. It is implemented by one or more actual email handlers. The email subsystem type is selected by the <code>subtype</code> field in the connection specification. Each subtype may have extra field requirements of its own.</p> <p>Currently supported email handler subtypes are:</p> <ul> <li> <p><code>ses</code>: AWS Simple Email Service (SES)</p> </li> <li> <p><code>smtp</code>: SMTP, including optional TLS support.</p> </li> </ul> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. from String No The email address that is sending the email. While this is not a mandatory field in the connector, there must be a value available at the time an email is sent, either from the job itself, the connection specification or an email handler specific mechanism. It is strongly recommended to include a default value in the connection specification. reply_to String or List[String] No The default reply-to email address(es) for messages. subtype String No Specifies the underlying email handler. If not specified, <code>ses</code> is assumed, in which case the field requirements for this subtype must be met. type String Yes <code>email</code>."},{"location":"07-connectors.html#subtype-ses","title":"Subtype: ses","text":"<p>The <code>ses</code> subtype uses AWS Simple Email Service to send email.</p> <p>The following fields are specific to the <code>ses</code> subtype.</p> Field Type Required Description configuration_set String No Use the specified SES Configuration Set when sending an email. If not specified, the value specified by the SES_CONFIGURATION_SET realm configuration parameter is used. from String No The email address that is sending the email. This email address must be either individually verified with Amazon SES, or from a domain that has been verified with Amazon SES. If not specified, the value specified by the SES_FROM realm configuration parameter is used. A value must be specified by one of these mechanisms. region String No The AWS region name for the SES service. If not specified, the value specified by the SES_REGION realm configuration parameter is used, which itself defaults to <code>us-east-1</code>. subtype String No Either <code>ses</code> or missing."},{"location":"07-connectors.html#subtype-smtp","title":"Subtype: smtp","text":"<p>The <code>smtp</code> subtype uses standard SMTP to send email. SMTP over TLS is also supported.</p> <p>The following fields are specific to the <code>smtp</code> subtype.</p> Field Type Required Description host String Yes The SMTP server host DNS name or IP address. password String Sometimes The name of an encrypted SSM parameter containing the SMTP server password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. This field is required if the <code>host</code> field is specified. port Number No The SMTP port number. If not specified, the default is 25 without TLS and 465 with TLS. Note that Gmail requires TLS on port 587. subtype String Yes <code>smtp</code> tls Boolean No If <code>true</code>, use SMTP over TLS. Default is <code>false</code>. user String No SMTP server user name. If specified, the <code>password</code> field must also be specified. If not specified, the connection will be unauthenticated."},{"location":"07-connectors.html#using-the-email-connector","title":"Using the Email Connector","text":"<p>The <code>email</code> connector provides two distinct interfaces:</p> <ol> <li> <p>A native Python interface</p> </li> <li> <p>A command line interface.</p> </li> </ol>"},{"location":"07-connectors.html#python-interface-for-email-connectors","title":"Python Interface for Email Connectors","text":"<p>Python scripts can directly access the underlying Python interface of an email connector. In this case, the connector returns a <code>lava.lib.email.Emailer</code> object as described in the lava API documentation.</p> <p>As an example, consider an exe job specification that looks something like this:</p> <pre><code>{\n    \"job_id\": \"...\",\n    \"parameters\": {\n        \"connections\": {\n            \"email\": \"email-connection-id\"\n        }\n    },\n    \"payload\": \"my-payload.py ...\"\n}\n</code></pre> <p>A Python program can use the email connector like this:</p> <pre><code>import os\nfrom lava.connection import get_email_connection\n\n# If running as a lava exe/pkg/docker, get some info provided by lava in the\n# environment. Assume our connector is labeled `email` in the job spec.\nrealm = os.environ['LAVA_REALM']\nconn_id = os.environ['LAVA_CONNID_EMAIL']\n\n# We can use the email connection as a context manager\nwith get_email_connection(conn_id, realm) as emailer:\n    emailer.send(\n        subject='Oh no',\n        message='Your oscillation overthruster has malfunctioned',\n        to='Buckaroo.Banzai@dimension8.com',\n        cc=[\n            'Professor.Hikita@dimension8.com',\n            'Sidney Zweibel@dimension8.com'\n        ]\n    )\n</code></pre>"},{"location":"07-connectors.html#executable-interface-for-email-connectors","title":"Executable Interface for Email Connectors","text":"<p>When used with exe, pkg and docker job types (e.g. shell scripts), the connection is implemented by the <code>lava-email</code> command.</p> <p>When used as a connection script within a lava job, the <code>-r REALM</code> and <code>-c CONN_ID</code> arguments don't need to be provided by the job as these are provided by lava in the connection script.</p> <p>Also, values for the <code>--from</code> and <code>--reply-to</code> options will be provided by lava if it has values available from the connection specification or other configuration data. These values can be overridden by providing the appropriate options to then connection script.</p> lava-email Usage <pre><code>usage: lava-email [-h] [--profile PROFILE] [-v] -c CONN_ID [-r REALM]\n                  [--bcc EMAIL] [--cc EMAIL] [--from EMAIL] [--reply-to EMAIL]\n                  [--to EMAIL] -s SUBJECT [--html FILENAME] [--text FILENAME]\n                  [--no-colour] [-l LEVEL] [--log LOG] [--tag TAG]\n                  [FILENAME]\n\nSend email using lava email connections.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --profile PROFILE     As for AWS CLI.\n  -v, --version         show program's version number and exit\n\nlava arguments:\n  -c CONN_ID, --conn-id CONN_ID\n                        Lava connection ID. Required.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the environment\n                        variable LAVA_REALM must be set.\n\nemail arguments:\n  --bcc EMAIL           Recipients to place on the Bcc: line of the message.\n                        Can be used multiple times.\n  --cc EMAIL            Recipients to place on the Cc: line of the message.\n                        Can be used multiple times.\n  --from EMAIL          Message sender. If not specified, a value must be\n                        available in either the connection specification or\n                        the realm specification.\n  --reply-to EMAIL      Reply-to address of the message.Can be used multiple\n                        times.\n  --to EMAIL            Recipients to place on the To: line of the message.Can\n                        be used multiple times.\n  -s SUBJECT, --subject SUBJECT\n                        Message subject. Required.\n\nmessage source arguments:\n  At most one of the following arguments is permitted.\n\n  --html FILENAME       This is a legacy argument for backward compatibility.\n  --text FILENAME       This is a legacy argument for backward compatibility.\n  FILENAME              Name of file containing the message body. If not\n                        specified or \"-\", the body will be read from stdin. An\n                        attempt is made to determine if the message is HTML and\n                        send it accordingly. Only the first 2MB is read.\n\nlogging arguments:\n  --no-colour, --no-color\n                        Don't use colour in information messages.\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The default\n                        is info.\n  --log LOG             Log to the specified target. This can be either a file\n                        name or a syslog facility with an @ prefix (e.g.\n                        @local0).\n  --tag TAG             Tag log entries with the specified value. The default\n                        is lava-email.\n</code></pre> <p>As an example, consider an exe job specification that looks something like this:</p> <pre><code>{\n    \"job_id\": \"...\",\n    \"parameters\": {\n        \"connections\": {\n            \"email\": \"email-connection-id\"\n        }\n    },\n    \"payload\": \"my-payload.sh ...\"\n}\n</code></pre> <p>Note the <code>email</code> connection. This will provide the job with an environment variable <code>LAVA_CONN_EMAIL</code> which points to the executable handling the connection.</p> <p>If the job payload is a shell script, the connector would be invoked thus:</p> <pre><code># Send an email with a text message body.\n$LAVA_CONN_EMAIL --to Buckaroo.Banzai@dimension8.com --subject \"Oh no\" &lt;&lt;!\n    Dear Buckaroo,\n\n    Your oscillation overthruster has malfunctioned.\n\n    -- John Bigboot\u00e9\n!\n\n# But wait -- we can do HTML as well\n$LAVA_CONN_EMAIL --to Buckaroo.Banzai@dimension8.com --subject \"Oh no\" &lt;&lt;!\n&lt;HTML&gt;\n    &lt;BODY&gt;\n        &lt;P&gt;Dear Buckaroo,&lt;/P&gt;\n        &lt;P&gt;Your oscillation overthruster has malfunctioned&lt;/P&gt;\n        &lt;P&gt;-- John Bigboot\u00e9&lt;/P&gt;\n    &lt;/BODY&gt;\n&lt;/HTML&gt;\n!\n</code></pre>"},{"location":"07-connectors.html#connector-type-generic","title":"Connector type: generic","text":"<p>The generic connector provides a general purpose mechanism to group a set of associated attributes together and have them made available to lava jobs at run-time. Lava doesn't actually connect to any external resources other than to obtain attribute values.</p> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. attributes Map[String,*] Yes A map comprising the attributes for the connector. The keys are the attribute names and the values are either simple scalars or another map specifying how to obtain the value. See below for more information. type String Yes <code>generic</code>."},{"location":"07-connectors.html#specifying-generic-connector-attribute-values","title":"Specifying Generic Connector Attribute Values","text":"<p>The <code>attributes</code> field of the generic connector specifies the names of the connector attributes and how the attribute values are obtained. The following variants are supported.</p>"},{"location":"07-connectors.html#simple-scalar-attributes","title":"Simple Scalar Attributes","text":"<p>Simple scalar attributes are specified thus:</p> <pre><code>{\n  \"attributes\": {\n    \"name\": \"value\"\n  }\n}\n</code></pre> <p>In addition to string values, integer and float values are also supported.</p>"},{"location":"07-connectors.html#local-parameters","title":"Local Parameters","text":"<p>This is an alternative syntax to the simple scalar attribute syntax described above.</p> <pre><code>{\n  \"attributes\": {\n    \"name\": {\n      \"type\": \"local\",\n      \"value\": \"value\"\n    }\n  }\n}\n</code></pre>"},{"location":"07-connectors.html#ssm-parameters","title":"SSM Parameters","text":"<p>Values from SSM parameters are specified thus:</p> <pre><code>{\n  \"attributes\": {\n    \"name\": {\n      \"type\": \"ssm\",\n      \"parameter\": \"SSM parameter name\"\n    }\n  }\n}\n</code></pre> <p>Lava will obtain the value from the SSM parameter store, decrypting as required.</p>"},{"location":"07-connectors.html#example-generic-connector-specification","title":"Example Generic Connector Specification","text":"<pre><code>{\n  \"conn_id\": \"widget-conn-id\",\n  \"description\": \"Sample generic connector\",\n  \"enabled\": true,\n  \"type\": \"generic\",\n  \"attributes\": {\n    \"a\": \"a string\",\n    \"b\": {\n      \"type\": \"local\",\n      \"value\": 30\n    },\n    \"c\": {\n      \"type\": \"ssm\",\n      \"parameter\": \"/lava/&lt;REALM&gt;/my_var\"\n    }\n  }\n}\n</code></pre>"},{"location":"07-connectors.html#using-the-generic-connector","title":"Using the Generic Connector","text":"<p>The generic connector provides two distinct interfaces:</p> <ol> <li> <p>A native Python interface</p> </li> <li> <p>A command line interface.</p> </li> </ol>"},{"location":"07-connectors.html#python-interface-for-generic-connectors","title":"Python Interface for Generic Connectors","text":"<p>Python scripts can directly access the underlying Python interface of a generic connector. In this case, the connector returns a dictionary of resolved attribute values.</p> <p>As an example, consider an exe job specification that looks something like this:</p> <pre><code>{\n    \"job_id\": \"...\",\n    \"parameters\": {\n        \"connections\": {\n            \"widget\": \"widget-connection-id\"\n        }\n    },\n    \"payload\": \"my-payload.py ...\"\n}\n</code></pre> <p>A Python program can use the generic connector like this:</p> <pre><code>import os\nfrom lava.connection import get_generic_connection\n\n# If running as a lava exe/pkg/docker, get some info provided by lava in the\n# environment. Assume our connector is labeled `widget` in the job spec.\nrealm = os.environ['LAVA_REALM']\nconn_id = os.environ['LAVA_CONNID_WIDGET']\n\nattributes = get_generic_connection(conn_id, realm)\n</code></pre> <p>The <code>attributes</code> dictionary would then look like:</p> <pre><code>{\n    'a': 'a string',\n    'b': 30,\n    'c': 'Value of SSM parameter /lava/&lt;REALM&gt;/my_var'\n}\n</code></pre>"},{"location":"07-connectors.html#executable-interface-for-generic-connectors","title":"Executable Interface for Generic Connectors","text":"<p>When used with exe, pkg and docker job types (e.g. shell scripts), the connection is implemented by a simple script that can be used to obtain the value of individual attributes.</p> <p>As an example, consider an exe job specification that looks something like this:</p> <pre><code>{\n  \"job_id\": \"...\",\n  \"parameters\": {\n    \"connections\": {\n      \"widget\": \"widget-connection-id\"\n    }\n  },\n  \"payload\": \"my-payload.sh ...\"\n}\n</code></pre> <p>Note the <code>widget</code> connection. This will provide the job with an environment variable <code>LAVA_CONN_WIDGET</code> which points to the executable handling the connection.</p> <p>If the job payload is a shell script, the connector would be invoked thus:</p> <pre><code># Get the values of the attributes\nATTR_A=$($LAVA_CONN_WIDGET a)\nATTR_B=$($LAVA_CONN_WIDGET b)\nATTR_C=$($LAVA_CONN_WIDGET c)\n</code></pre>"},{"location":"07-connectors.html#connector-type-git","title":"Connector type: git","text":"<p>The git connector manages access to Git repositories by providing support for managing SSH private keys.</p> <p>When used with exe and pkg jobs, it provides an environment variable pointing to a script that will run the Git CLI with SSH keys managed in the background.</p> <p>Note that only SSH access to repositories is supported. HTTPS is not supported.</p> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. ssh_key String Yes The name of an encrypted SSM parameter containing the SSH private key. There must not be any passphrase on the key. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. Refer to the ssh connector for more information on how to prepare and store the key. ssh_options List[String] No A list of SSH options as per ssh_config(5). e.g. <code>StrictHostKeyChecking=no</code> type String Yes <code>git</code>."},{"location":"07-connectors.html#connector-type-mariadb-rds","title":"Connector type: mariadb-rds","text":"<p>This is currently a synonym for mysql.</p> <p>It has been defined in the event of future feature differences between conventional MySQL and AWS RDS MariaDB.</p>"},{"location":"07-connectors.html#connector-type-mariadb","title":"Connector type: mariadb","text":"<p>This is a synonym for mysql.</p> <pre><code>## Connector type: mssql\n</code></pre> <p>The mssql connector handles connections to Microsoft SQL Server databases.</p> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. database String Yes* The name of the database within the database server. driver String No The ODBC driver specification. This must correspond to the name of a section in <code>/etc/odbcinst.ini</code>. The default is <code>FreeTDS</code>. enabled Boolean Yes Whether or not the connection is enabled. host String Yes* The database host DNS name or IP address. password String Yes* The name of an encrypted SSM parameter containing the password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. port Number Yes* The database port number. preserve_case Boolean No If <code>true</code>, don't fold database object names to lower case when quoting them for use in db_from_s3 jobs. The default is <code>false</code> (i.e. case folding is enabled). secret_id String No Obtain missing fields from AWS Secrets Manager. More information. subtype String No Specifies the underlying DBAPI 2.0 driver. The default and only allowed value is <code>pyodbc</code>. timeout Integer No Connection timeout in seconds. If not specified, no timeout is applied. type String Yes <code>mssql</code>. user String Yes* Database user name. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> can have a value provided directly in the connection specification or indirectly via AWS Secrets Manager using the <code>secret_id</code> field.  See Database Authentication Using AWS Secrets Manager for more information.</p> <p>SSL connections are not currently supported.</p> <p>When used with exe and pkg job types, the connection is implemented by the lava-sql CLI.</p> <p>Note</p> <p>There are some MSSQL CLI tools that come with the TDS or unixODBC packages. None of them are wonderful so for now <code>lava-sql</code> will have to do. Also not wonderful but what do you expect for free?</p>"},{"location":"07-connectors.html#implementation-notes","title":"Implementation Notes","text":"<p>The current implementation requires the following components be installed and configured on the lava worker:</p> <ul> <li> <p>Free TDS</p> </li> <li> <p>unixODBC</p> </li> <li> <p>pyodbc</p> </li> </ul> <p>Configuring unixODBC with Free TDS</p>"},{"location":"07-connectors.html#connector-type-mysql-aurora","title":"Connector type: mysql-aurora","text":"<p>The mysql-aurora connector handles connections to AWS RDS Aurora MySQL database clusters. This is almost a synonym for mysql. Key differences are:</p> <ul> <li> <p>The db_from_s3 job can take     advantage of an AWS facility to load data directly from S3.</p> </li> <li> <p>Database authentication using IAM credential generation is supported.</p> </li> </ul> Field Type Required Description ca_cert String No The name of a file containing the CA certificate for the database server. Ignored unless <code>ssl</code> is <code>true</code>. conn_id String Yes Connection identifier. description String No Description. database String Yes* The name of the database (schema) within the database server. enabled Boolean Yes Whether or not the connection is enabled. host String Yes* The database host DNS name or IP address. password String No* The name of an encrypted SSM parameter containing the password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. If not specified, the worker will attempt to generate temporary IAM user credentials. port Number Yes* The database port number. preserve_case Boolean No If <code>true</code>, don't fold database object names to lower case when quoting them for use in db_from_s3 jobs. The default is <code>false</code> (i.e. case folding is enabled). secret_id String No Obtain missing fields from AWS Secrets Manager. More information. ssl Boolean No Set to <code>true</code> to enable SSL. Default is <code>false</code>. type String Yes <code>mysql-aurora</code> user String Yes* Database user name. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> can have a value provided directly in the connection specification or indirectly via AWS Secrets Manager using the <code>secret_id</code> field.  See Database Authentication Using AWS Secrets Manager for more information.</p> <p>When used with exe and pkg job types, the connection is implemented by the <code>mysql</code> CLI.  Apart from the connection parameters, it is invoked with the following options:</p> <pre><code>mysql --batch --connect-timeout=10\n</code></pre>"},{"location":"07-connectors.html#creating-temporary-iam-user-credentials-for-aws-rds-aurora-mysql","title":"Creating Temporary IAM User Credentials for AWS RDS Aurora MySQL","text":"<p>If the <code>password</code> field is not present in the connection specification, lava will attempt to generate temporary IAM credentials using the <code>generate-db-auth-token</code> mechanism.</p> <p>The specified user must already exist in the database. Enable IAM authentication for a user thus:</p> <pre><code>CREATE USER a_user IDENTIFIED WITH AWSAuthenticationPlugin AS 'RDS';\n</code></pre> <p>The IAM policy attached to the worker will need to contain an element something like this:</p> <pre><code>\"Statement\": [\n    {\n        \"Sid\": \"GetRdsCreds\",\n        \"Effect\": \"Allow\",\n        \"Action\": \"rds-db:connect\",\n        \"Resource\": [\n            \"arn:aws:rds-db:ap-southeast-2:123456789012:dbuser:db-JMH2...6KW6Q/a_user\"\n        ]\n    }\n]\n</code></pre> <p>The DB instance ID for use in the IAM policy can be obtained thus:</p> <pre><code>aws rds describe-db-instances --db-instance-identifier 'DB_ID' \\\n     --query 'DBInstances[0].DbiResourceId' --output text\n</code></pre> <p>Info</p> <p>SSL is mandatory when using temporary IAM user credentials.</p>"},{"location":"07-connectors.html#connector-type-mysql-rds","title":"Connector type: mysql-rds","text":"<p>This is currently a synonym for mysql-aurora.</p> <p>It has been defined in the event of future feature differences between conventional AWS RDS Aurora MySQL and AWS RDS MySQL.</p>"},{"location":"07-connectors.html#connector-type-mysql","title":"Connector type: mysql","text":"<p>The mysql connector handles connections to MySQL compatible databases.</p> Field Type Required Description ca_cert String No The name of a file containing the CA certificate for the database server. Ignored unless <code>ssl</code> is <code>true</code>. conn_id String Yes Connection identifier. description String No Description. database String Yes* The name of the database (schema) within the database server. enabled Boolean Yes Whether or not the connection is enabled. host String Yes* The database host DNS name or IP address. password String Yes* The name of an encrypted SSM parameter containing the password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. port Number Yes* The database port number. preserve_case Boolean No If <code>true</code>, don't fold database object names to lower case when quoting them for use in db_from_s3 jobs. The default is <code>false</code> (i.e. case folding is enabled). secret_id String No Obtain missing fields from AWS Secrets Manager. More information. ssl Boolean No Set to <code>true</code> to enable SSL. Default is <code>false</code>. type String Yes <code>mysql</code>. user String Yes* Database user name. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> can have a value provided directly in the connection specification or indirectly via AWS Secrets Manager using the <code>secret_id</code> field.  See Database Authentication Using AWS Secrets Manager for more information.</p> <p>When used with exe and pkg job types, the connection is implemented by the <code>mysql</code> CLI, either the MySQL Community version, or the MariaDB version, depending on the variant installed on the worker. These have some minor CLI parameter differences which lava manages for the connection parameters. Apart from the connection parameters, it is invoked with the following options:</p> <pre><code>mysql --batch --connect-timeout=10\n</code></pre>"},{"location":"07-connectors.html#connector-type-oracle-rds","title":"Connector type: oracle-rds","text":"<p>This is currently a synonym for oracle.</p> <p>It has been defined in the event of future feature differences between conventional Oracle and AWS RDS Oracle.</p>"},{"location":"07-connectors.html#connector-type-oracle","title":"Connector type: oracle","text":"<p>The oracle connector handles connections to Oracle databases.</p> Field Type Required Description conn_id String Yes Connection identifier. database String No* A deprecated synonym for <code>sid</code>. description String No Description. edition String No Oracle version for compatibility in the form <code>x.y[.z]</code>. enabled Boolean Yes Whether or not the connection is enabled. host String Yes* The database host DNS name or IP address. password String Yes* The name of an encrypted SSM parameter containing the password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. port Number Yes* The database port number. secret_id String No Obtain missing fields from AWS Secrets Manager. More information. service_name String No* The Oracle data base service name. Generally exactly one of <code>service_name</code> or <code>sid</code> must be specified. sid String No* The Oracle System Identifier of the database. Generally exactly one of <code>service_name</code> or <code>sid</code> must be specified. type String Yes <code>oracle</code>. user String Yes* Database user name. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> can have a value provided directly in the connection specification or indirectly via AWS Secrets Manager using the <code>secret_id</code> field.  See Database Authentication Using AWS Secrets Manager for more information.</p> <p>When used with exe and pkg job types, the connection is implemented by the SQL*Plus CLI, <code>sqlplus</code>.  Apart from the connection parameters, it is invoked with the following options:</p> <pre><code>sqlplus -NOLOGINTIME -L -S -C &lt;version&gt;\n</code></pre> <p>The SQL*Plus CLI is a particularly contrary beast. It is important to explicitly exit the CLI using an <code>EXIT</code> command at the end of any session or else it will drop into interactive mode and sit there waiting for further commands until the job reaches its timeout and is killed by lava. A safer approach is to send commands to the connector via stdin, thus:</p> <pre><code># Assume our conn_id is ora\n\n$LAVA_CONN_ORA &lt;&lt;!\nSELECT whatever FROM whichever;\n!\n</code></pre> <p>When used with sql jobs, do not terminate the SQL with a semi-colon or a syntax error results.</p> <p>When used with sqlc jobs, SQL commands must be terminated with a semi-colon or either a syntax error or no output will result.</p>"},{"location":"07-connectors.html#security-warnings","title":"Security Warnings","text":"<p>Oracle CLI clients, including <code>sqlplus</code>, do not provide any means to automate login to the database without specifying the password on the command line. This means the password is exposed in a process listing. Do not use the oracle command line connector on any worker that has multi-user access.</p> <p>The oracle connector does not currently support SSL/TLS.</p>"},{"location":"07-connectors.html#connector-type-postgres-aurora","title":"Connector type: postgres-aurora","text":"<p>This connector support AWS RDS Aurora PostgreSQL clusters. This is almost a synonym for postgres. Key differences are:</p> <ul> <li> <p>The db_from_s3 job can take     advantage of an AWS facility to load data directly from S3.</p> </li> <li> <p>Database authentication using IAM credential generation     is supported.</p> </li> </ul> Field Type Required Description conn_id String Yes Connection identifier. database String Yes* The name of the database within the database server. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. host String Yes* The database host DNS name or IP address. password String No* The name of an encrypted SSM parameter containing the password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. If not specified, the worker will attempt to generate temporary IAM user credentials. port Number Yes* The database port number. preserve_case Boolean No If <code>true</code>, don't fold database object names to lower case when quoting them for use in db_from_s3 jobs. The default is <code>false</code> (i.e. case folding is enabled). secret_id String No Obtain missing fields from AWS Secrets Manager. More information. ssl Boolean No Set to <code>true</code> to enable SSL. Default is <code>false</code> subtype String No Specifies the underlying DBAPI 2.0 driver. The default is <code>pg8000</code> which should be used wherever possible. The <code>pygresql</code> driver is also available. type String Yes <code>psql</code> or <code>postgres</code>. user String Yes* Database user name. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> can have a value provided directly in the connection specification or indirectly via AWS Secrets Manager using the <code>secret_id</code> field.  See Database Authentication Using AWS Secrets Manager for more information.</p> <p>When used with exe and pkg job types, the connection is implemented by the <code>psql</code> CLI.  Apart from the connection parameters, it is invoked with the following options:</p> <pre><code>psql --no-psqlrc --quiet --set ON_ERROR_STOP=on --pset footer=off\n</code></pre>"},{"location":"07-connectors.html#creating-temporary-iam-user-credentials-for-aws-rds-aurora-postgresql","title":"Creating Temporary IAM User Credentials for AWS RDS Aurora PostgreSQL","text":"<p>If the <code>password</code> field is not present in the connection specification, lava will attempt to generate temporary IAM credentials using the <code>generate-db-auth-token</code> mechanism.</p> <p>The specified user must already exist in the database. Enable IAM authentication for a user thus:</p> <pre><code>CREATE USER a_user; \nGRANT rds_iam TO a_user;\n</code></pre> <p>Info</p> <p>SSL is mandatory when using temporary IAM user credentials.</p>"},{"location":"07-connectors.html#psql-cli-password-limitations","title":"Psql CLI Password Limitations","text":"<p>The psql CLI will not accept passwords in a PGPASS file (or entered interactively) that are longer than a certain (undocumented) length. IAM based authentication for RDS involves temporary passwords that are much longer than this limit.</p> <p>To workaround this limitation, lava has to put long passwords into an environment variable. While this is not ideal from a security perspective, at least the passwords are short lived.</p>"},{"location":"07-connectors.html#connector-type-postgres-rds","title":"Connector type: postgres-rds","text":"<p>This is currently a synonym for postgres-aurora.</p> <p>It has been defined in the event of future feature differences between conventional AWS RDS Aurora PostgreSQL and AWS RDS PostgreSQL.</p>"},{"location":"07-connectors.html#connector-type-postgres","title":"Connector type: postgres","text":"<p>The postgres connector handles connections to Postgres compatible databases.</p> Field Type Required Description conn_id String Yes Connection identifier. database String Yes* The name of the database within the database server. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. host String Yes* The database host DNS name or IP address. password String Yes* The name of an encrypted SSM parameter containing the password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. port Number Yes* The database port number. preserve_case Boolean No If <code>true</code>, don't fold database object names to lower case when quoting them for use in db_from_s3 jobs. The default is <code>false</code> (i.e. case folding is enabled). secret_id String No Obtain missing fields from AWS Secrets Manager. More information. ssl Boolean No Set to <code>true</code> to enable SSL. Default is <code>false</code> subtype String No Specifies the underlying DBAPI 2.0 driver. The default is <code>pg8000</code> which should be used wherever possible. The <code>pygresql</code> driver is also available. type String Yes <code>psql</code> or <code>postgres</code>. user String Yes* Database user name. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> can have a value provided directly in the connection specification or indirectly via AWS Secrets Manager using the <code>secret_id</code> field.  See Database Authentication Using AWS Secrets Manager for more information.</p> <p>When used with exe and pkg job types, the connection is implemented by the <code>psql</code> CLI.  Apart from the connection parameters, it is invoked with the following options:</p> <pre><code>psql --no-psqlrc --quiet --set ON_ERROR_STOP=on --pset footer=off\n</code></pre>"},{"location":"07-connectors.html#connector-type-psql","title":"Connector type: psql","text":"<p>This is a synonym for postgres.</p>"},{"location":"07-connectors.html#connector-type-redshift-serverless","title":"Connector type: redshift-serverless","text":"<p>This is the connector for Redshift Serverless clusters.</p> Field Type Required Description conn_id String Yes Connection identifier. database String Yes* The name of the database within the Redshift Serverless namespace. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. external_id String No Name of an SSM parameter containing an external ID to use when assuming the IAM role specified by <code>role_arn</code> when generating temporary IAM user credentials. While AWS does not consider this to be a sensitive security parameter, it is stored in the SSM parameter store for ease of management. It is still recommended to use a secure parameter. Can't hurt. host String Yes* The Redshift serverless workgroup endpoint address. password_duration String No The password duration when generating temporary IAM user credentials in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours). If not specified, the default worker configuration is used. Limits imposed by the Redshift Serverless GetCredentials API apply. password String No* The name of an encrypted SSM parameter containing the password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. If not specified, the worker will attempt to generate temporary IAM user credentials. port Number Yes* The Redshift serverless workgroup port number. preserve_case Boolean No If <code>true</code>, don't fold database object names to lower case when quoting them for use in db_from_s3 jobs. The default is <code>false</code> (i.e. case folding is enabled). role_arn String No The ARN of an IAM role that will be assumed when generating temporary IAM user credentials. secret_id String No Obtain missing fields from AWS Secrets Manager. More information. ssl Boolean No Set to <code>true</code> to enable SSL. Default is <code>false</code>. subtype String No Specifies the underlying DBAPI 2.0 driver. See Redshift Connector Subtypes below. type String Yes <code>redshift-serverless</code>. user String Yes* Database user name. workgroup String No The name of the workgroup associated with the database. This is used when generating temporary IAM user credentials. If required and not specified, the first component of the <code>host</code> field is used. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> can have a value provided directly in the connection specification or indirectly via AWS Secrets Manager using the <code>secret_id</code> field.  See Database Authentication Using AWS Secrets Manager for more information.</p>"},{"location":"07-connectors.html#redshift-serverless-connector-subtypes","title":"Redshift Serverless Connector Subtypes","text":"<p>The <code>subtype</code> field of the connection specification allows selection of different database drivers.</p> Subtype Description pg8000 Pg8000 is the default if no subtype is specified. redshift This is the AWS Redshift connector."},{"location":"07-connectors.html#creating-temporary-iam-user-credentials-for-redshift-serverless","title":"Creating Temporary IAM User Credentials for Redshift Serverless","text":"<p>Note</p> <p>The AWS documentation on this leaves a lot to be desired.</p> <p>If a password is not obtained from the <code>password</code> field or secrets manager, lava will attempt to use the Redshift Serverless GetCredentials API to generate temporary IAM-based database user credentials.</p> <p>Unlike the Redshift provisioned GetClusterCredentials API, the Redshift Serverless GetCredentials API does not allow the target database user name to be specified. The username is derived automatically from the IAM principal as follows:</p> <ul> <li> <p>For IAM users, the database username is <code>IAM:&lt;IAM-USER-NAME&gt;</code>.</p> </li> <li> <p>For IAM roles, the database username is <code>IAMR:&lt;IAM-ROLE-NAME&gt;</code>.</p> </li> </ul> <p>If the user does not already exist in the database, it will be automatically created and given access to the public schema. This is daft but that's how it is. The user can be created manually or given additional database permissions via the normal GRANT mechanism, as required.</p> <p>This can be very limiting in terms of fine grained access control from lava to Redshift. To provide some flexibility, the Redshift Serverless connector can assume a different IAM role prior to generating database access credentials by specifying the <code>role_arn</code> (and optional <code>external_id</code>) elements in the connection specification. The assumed role is then the one that will determine the database user name.</p> <p>For example, assume the lava worker normally operates under the IAM role <code>lava-prod-worker-core</code>. If no <code>role_arn</code> is specified, the database user will be <code>IAMR:lava-dev-worker-core</code>.</p> <p>If <code>role_arn</code> is <code>arn:aws:iam::123456789123:role/rs01</code>, the database user will be <code>IAMR:rs01</code>.</p> <p>The IAM policy attached to the <code>lava-dev-worker-core</code> role will need to contain something like this:</p> <pre><code>\"Statement\": [\n    {\n        \"Sid\": \"AssumeRoleForRedshiftServerlessAccess\"\n        \"Effect\": \"Allow\",\n        \"Action\": \"sts:AssumeRole\",\n        \"Resource\": [\n            \"arn:aws:iam::123456789123:role/rs01\"\n        ]\n    }\n]\n</code></pre> <p>The IAM policy attached to the <code>rs01</code> role will need to contain something like this:</p> <pre><code>\"Statement\": [\n    {\n        \"Sid\": \"GetRedshiftServerlessCreds\",\n        \"Effect\": \"Allow\",\n        \"Action\": \"redshift-serverless:GetCredentials\",\n        \"Resource\": [\n            \"arn:aws:redshift-serverless:ap-southeast-2:123456789123:workgroup/3741886a-223d-446f-a77c-a5d0e7b5ad32\"\n        ]\n    }\n]\n</code></pre> <p>The trust policy for the <code>rs01</code> role will need to contain the elements necessary to allow it to be assumed by <code>lava-dev-worker-core</code>.</p> <p>Note</p> <p>Lava currently does not cache temporary credentials. Watch out for throttling on the <code>GetCredentials</code> API.</p>"},{"location":"07-connectors.html#connector-type-redshift","title":"Connector type: redshift","text":"<p>This is the connector for Redshift provisioned clusters. It can also be used for Redshift Serverless clusters except when IAM generated user credentials are used. In that case, the redshift-serverless connector must be used.</p> <p>This connector is similar to postgres. Note that some operations are specific to Redshift and are not supported on conventional Postgres databases (e.g. the <code>COPY</code> and <code>UNLOAD</code> commands).</p> Field Type Required Description cluster_id String No The Redshift cluster identifier. If required and not specified, the first component of the <code>host</code> name is used. conn_id String Yes Connection identifier. database String Yes* The name of the database within the database server. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. host String Yes* The database host DNS name or IP address. password_duration String No The password duration when generating temporary IAM user credentials in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours). If not specified, the default worker configuration is used. Limits imposed by the GetClusterCredentials API apply. password String No* The name of an encrypted SSM parameter containing the password. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. If not specified, the worker will attempt to generate temporary IAM user credentials. port Number Yes* The database port number. preserve_case Boolean No If <code>true</code>, don't fold database object names to lower case when quoting them for use in db_from_s3 jobs. The default is <code>false</code> (i.e. case folding is enabled). secret_id String No Obtain missing fields from AWS Secrets Manager. More information. ssl Boolean No Set to <code>true</code> to enable SSL. Default is <code>false</code>. subtype String No Specifies the underlying DBAPI 2.0 driver. See Redshift Connector Subtypes below. type String Yes <code>redshift</code>. user String Yes* Database user name. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> can have a value provided directly in the connection specification or indirectly via AWS Secrets Manager using the <code>secret_id</code> field.  See Database Authentication Using AWS Secrets Manager for more information.</p>"},{"location":"07-connectors.html#redshift-connector-subtypes","title":"Redshift Connector Subtypes","text":"<p>The <code>subtype</code> field of the connection specification allows selection of different database drivers.</p> Subtype Description pg8000 Pg8000 is the default if no subtype is specified. redshift This is the AWS Redshift connector. <p>Info</p> <p>As of version 8.1 (K\u012blauea), the Redshift connector no longer supports PyGreSQL. This is not a lava change. PyGreSQL just doesn't work with Redshift any more.</p>"},{"location":"07-connectors.html#creating-temporary-iam-user-credentials-for-redshift","title":"Creating Temporary IAM User Credentials for Redshift","text":"<p>If the <code>password</code> field is not present in the connection specification, lava will attempt to use the Redshift GetClusterCredentials API to generate temporary IAM-based database user credentials.</p> <p>The specified user must already exist in the database as lava (deliberately) does not support <code>AutoCreate</code> of users.</p> <p>Lava will specify the target cluster ID, database and target user in the credentials request. This means that the IAM policy attached to the worker will need to contain an element something like this:</p> <pre><code>\"Statement\": [\n    {\n        \"Sid\": \"GetRedshiftCreds\",\n        \"Effect\": \"Allow\",\n        \"Action\": \"redshift:GetClusterCredentials\",\n        \"Resource\": [\n            \"arn:aws:redshift:ap-southeast-2:123456789012:dbuser:cluster_id/target_user\",\n            \"arn:aws:redshift:ap-southeast-2:123456789012:dbname:cluster_id/mydb\"\n        ]\n    }\n]\n</code></pre> <p>Info</p> <p>Lava currently does not cache temporary credentials. Watch out for throttling on the <code>GetClusterCredentials</code> API.</p>"},{"location":"07-connectors.html#connector-type-ses","title":"Connector type: ses","text":"<p>Warning</p> <p>This is a legacy implementation. It is now deprecated and will be removed in a future release. Use the email connector instead.</p> <p>The ses connector provides access to the AWS Simple Email Service (SES).</p> <p>If can be used only with exe and pkg jobs. It provides an environment variable pointing to a script that will run the AWS CLI with appropriate parameters to access the SES service.</p> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. from String No The email address that is sending the email. This email address must be either individually verified with Amazon SES, or  from  a  domain that  has been verified with Amazon SES. If not specified, the value specified by the SES_FROM realm configuration parameter is used. A value must be specified by one of these mechanisms. region String No The AWS region name for the SES service. If not specified, the value specified by the SES_REGION realm configuration parameter is used, which itself defaults to <code>us-east-1</code>. reply_to String or List[String] No The reply-to email address(es) for messages. return_path String No The email address that bounces and complaints will be  forwarded  to when feedback forwarding is enabled. <p>In an exe or package pkg job, the job specification will look something like this:</p> <p><pre><code>{\n    \"job_id\": \"...\",\n    \"parameters\": {\n        \"connections\": {\n            \"email\": \"email-connection-id\"\n        }\n    },\n    \"payload\": \"my-payload.sh ...\"\n}\n</code></pre> Note the <code>email</code> connection. This will provide the job with an environment variable <code>LAVA_CONN_EMAIL</code> which points to the executable handling the connection.</p> <p>If the job payload is a shell script, the connector would be invoked thus:</p> <pre><code># Send an email with a text message body.\n$LAVA_CONN_EMAIL --to fred@somewhere.com --subject \"Hello Fred\" --text msg.txt\n\n# But wait -- we can do HTML as well\n$LAVA_CONN_EMAIL --to fred@somewhere.com --subject \"Hello Fred\" --html msg.html\n\n# Or read from stdin. The connector will look for &lt;HTML&gt; at start of message\n# to determine if message is text or HTML.\n$LAVA_CONN_EMAIL --to fred@somewhere.com --subject \"Hello Fred\" &lt; msg.xxx\n</code></pre> <p>The connector script accepts the following arguments:</p> <ul> <li> <p>--to email ...   --cc email ...   --bcc email ...    </p> <p>One or more recipient email addresses.</p> </li> <li> <p>--subject text</p> <p>Message subject.</p> </li> <li> <p>--text filename</p> <p>File containing the text body of the message. Optional.</p> </li> <li> <p>--html filename</p> <p>File containing the HTML body of the message. Optional.</p> </li> </ul> <p>If neither --text nor --html options are specified, the message body is read from stdin. If the content begins with <code>&lt;HTML&gt;</code> (case insensitive), the connector will send it as HTML otherwise as text.</p>"},{"location":"07-connectors.html#connector-type-sharepoint","title":"Connector type: sharepoint","text":"<p>The sharepoint connector manages connections to SharePoint sites.</p> <p>It is possible for Microsoft to have made this process more complex and unwieldy, but it is not obvious how.</p> Field Type Required Description client_id String Yes The Application ID that the SharePoint registration portal assigned your app. This resembles a UUID. client_secret String Yes Name of the SSE parameter containing the client secret. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. conn_id String Yes Connection identifier. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. https_proxy String No HTTPS proxy to use for accessing the SharePoint API endpoints. If not specified, the <code>HTTPS_PROXY</code> environment variable is used, if set. org_base_url String Yes The hostname component of the organisation's SharePoint base URL. e.g. <code>acme.sharepoint.com</code>. password String Yes Name of the SSM parameter containing the password for authenticating to SharePoint. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. site_name String Yes The SharePoint site name. tenant String Yes The Azure AD registered domain ID. This resembles a UUID. type String Yes <code>sharepoint</code>. user String No User name for authenticating to SharePoint. <p>The connector supports the sharepoint_get_doc, sharepoint_get_list, sharepoint_put_doc, sharepoint_put_list and sharepoint_get_multi_doc and job types.</p>"},{"location":"07-connectors.html#using-sharepoint-connectors","title":"Using SharePoint Connectors","text":"<p>The <code>sharepoint</code> connector provides two distinct interfaces:</p> <ol> <li> <p>A native Python interface</p> </li> <li> <p>A command line interface.</p> </li> </ol>"},{"location":"07-connectors.html#python-interface-for-sharepoint-connectors","title":"Python Interface for SharePoint Connectors","text":"<p>The sharepoint connector can be used with Python based exe and pkg jobs that invoke the lava connection manager directly. In this case, the connector returns a <code>lava.lib.sharepoint.Sharepoint</code> object as described in the lava API documentation. In summary, this class has the following methods:</p> <p><pre><code>delete_all_list_items(list_id, list_name)\n\nget_doc(lib_name, path, out_file)\n\nget_list(list_name, out_file, system_columns=None, data_columns=None,\n    header=True, **csv_writer_args)\n\nput_doc(lib_name, path, src_file, title=None)\n\nput_list(list_name, src_file, mode='append', error_missing=False,\n    data_columns=None, **csv_reader_args)\n\nget_multi_doc(lib_name, path, out_path, glob=None)\n\nclose()\n</code></pre> Note that this is the low level connector. It does not handle moving files in or out of S3 or Jinja rendering of parameters. It is up to the caller to do that as required.</p> <p>If the SharePoint connector key in the job's <code>connectors</code> map is <code>spoint</code>, typical usage would be something like:</p> <pre><code>import os\nfrom lava.connection import get_sharepoint_connection\n\n# Get a lava.lib.sharepoint.Sharepoint instance\nsp_conn = get_sharepoint_connection(\n    conn_id=os.environ['LAVA_CONNID_SPOINT'],\n    realm=os.environ['LAVA_REALM']\n)\n\n# Get a list from SharePoint and store it locally.\nrow_count = sp_conn.get_list('postcodes', 'postcodes.csv', delimiter=',')\n\n# Close the connection\nsp_conn.close()\n</code></pre>"},{"location":"07-connectors.html#executable-interface-for-sharepoint-connectors","title":"Executable Interface for SharePoint Connectors","text":"<p>When used with exe, pkg and docker job types (e.g. shell scripts), the connection is implemented by the <code>lava-sharepoint</code> command.</p> <p>This is a somewhat higher level interface to the connector in that it can also handle moving data in and out of S3. Jinja rendering is handled as per the sharepoint_get_list, sharepoint_put_list, sharepoint_get_doc, sharepoint_put_doc and sharepoint_get_multi_doc job types.</p> <p>If the SharePoint connector key in the job's <code>connectors</code> map is <code>spoint</code>, usage is:</p> <pre><code>usage: $LAVA_CONN_SPOINT [-J] [-l LEVEL] {put-doc,put-list,get-doc,get-list,get-multi-doc} ...\n\nsub-commands:\n  {put-doc,put-list,get-doc,get-list,get-multi-doc}\n    put-doc             Copy a file into a SharePoint document library.\n    put-list            Copy a file into a SharePoint list.\n    get-doc             Copy a file from a SharePoint document library.\n    get-list            Copy a SharePoint list to a file\n    get-multi-doc       Copy multiple files from a SharePoint document library path.\n\noptional arguments:\n  -J, --no-jinja        Disable Jinja rendering of the transfer parameters.\n\nlogging arguments:\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The Default\n                        is info.\n</code></pre> <p>Usage for the <code>get-doc</code> sub-command:</p> <pre><code>usage: $LAVA_CONN_SPOINT get-doc [options] SharePoint-path file\n\npositional arguments:\n  SharePoint-path       Source location. Must be in the form library:path.\n                        This will be jinja rendered.\n  file                  Target file. Values starting with s3:// will be copied\n                        to S3. This will be jinja rendered.\n\noptional arguments:\n  -k KMS_KEY_ID, --kms-key-id KMS_KEY_ID\n                        AWS KMS key to use for uploading data to S3.\n</code></pre> <p>Usage for the <code>get-list</code> sub-command:</p> <pre><code>usage: $LAVA_CONN_SPOINT get-list [options] SharePoint-list file\n\npositional arguments:\n  SharePoint-list       Source SharePoint list name. This will be jinja\n                        rendered.\n  file                  Target file. Values starting with s3:// will be copied\n                        to S3. This will be jinja rendered.\n\noptional arguments:\n  -k KMS_KEY_ID, --kms-key-id KMS_KEY_ID\n                        AWS KMS key to use for uploading data to S3.\n  -H, --no-header       Don't include a header row. A header is included by\n                        default.\n  --delimiter DELIMITER\n                        Output field delimiter.\n  --double-quote        As for csv.writer.\n  --escape-char ESCAPECHAR\n                        As for csv.writer.\n  --quote-char QUOTECHAR\n                        As for csv.writer.\n  --quoting QUOTING     As for csv.writer QUOTE_ parameters (without the\n                        QUOTE_ prefix).\n</code></pre> <p>Usage for the <code>get-doc</code> sub-command:</p> <pre><code>usage: $LAVA_CONN_SPOINT get-doc [options] SharePoint-path file\n\npositional arguments:\n  SharePoint-path       Source location. Must be in the form library:path.\n                        This will be jinja rendered.\n  file                  Target file. Values starting with s3:// will be copied\n                        to S3. This will be jinja rendered.\n\noptional arguments:\n  -k KMS_KEY_ID, --kms-key-id KMS_KEY_ID\n                        AWS KMS key to use for uploading data to S3.\n</code></pre> <p>Usage for the <code>put-doc</code> sub-command:</p> <pre><code>usage: $LAVA_CONN_SPOINT put-doc [options] file SharePoint-path\n\npositional arguments:\n  file                  Source file. Values starting with s3:// will be copied\n                        from S3. This will be jinja rendered.\n  SharePoint-path       Target location. Must be in the form library:path.\n                        This will be jinja rendered.\n\noptional arguments:\n  -t TITLE, --title TITLE\n                        Document title. This will be jinja rendered.\n</code></pre> <p>Usage for the <code>get-multi-doc</code> sub-command:</p> <pre><code>usage: $LAVA_CONN_SPOINT get-multi-doc [options] SharePoint-path outpath [glob]\n\npositional arguments:\n  SharePoint-path       Source location. Must be in the form library:path.\n                        This will be jinja rendered.\n  outpath               Target path. Values starting with s3:// will be copied\n                        to S3 using given bucket and key as key prefix. This\n                        will be jinja rendered.\n  glob                  Filter files in sharepoint path on this given glob.\n                        This will be jinja rendered.\n\noptional arguments:\n  -k KMS_KEY_ID, --kms-key-id KMS_KEY_ID\n                        AWS KMS key to use for uploading data to S3.\n</code></pre> <p>The following examples show how to use the connector in an exe job using bash:</p> <pre><code>#!/bin/bash\n\n# Copy a list from S3 to SharePoint, replacing existing contents.\n$LAVA_CONN_SPOINT put-list --replace s3://my-bucket/data.csv My-List\n\n# Get list back from SharePoint and place in S3. Include a header\n$LAVA_CONN_SPOINT get-list -k alias/data --delimiter \",\" \\\n    My-List s3://my-bucket/data.csv\n\n# Copy a document from S3 to SharePoint.\n$LAVA_CONN_SPOINT put-doc s3://my-bucket/lava.docx \"Lava Docs:/Lava/User Guide.docx\"\n\n# Get a document from SharePoint and place in S3.\n$LAVA_CONN_SPOINT get-doc \"Lava Docs:/Lava/User Guide.docx\" s3://my-bucket/lava.docx \n\n# Get all docx files from SharePoint path and place in S3 base-prefix.\n$LAVA_CONN_SPOINT get-multi-doc \"Lava Docs:/Lava/\" s3://my-bucket/base-prefix *.docx\n\n# Get all files from SharePoint path and place in S3 base-prefix.\n$LAVA_CONN_SPOINT get-multi-doc \"Lava Docs:/Lava/\" s3://my-bucket/base-prefix\n</code></pre>"},{"location":"07-connectors.html#connector-type-slack","title":"Connector type: slack","text":"<p>The slack connector uses Slack webhooks to send messages to Slack channels. The target Slack workspace and channel are specified in Slack itself when the webhook is created.</p> Field Type Required Description colour Style No Default colour for the sidebar for Slack messages sent using <code>attachment</code> style. This can be any hex colour code or one of the Slack special values <code>good</code>, <code>warning</code> or <code>danger</code>. If not specified a default value is used. conn_id String Yes Connection identifier. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. from String No An arbitrary source identifier for display in Slack messages. If not specified, a default value is constructed when required. preamble String No Default preamble at the start of Slack messages. Useful values include things such as <code>&lt;!here&gt;</code> and <code>&lt;!channel&gt;</code> which will cause Slack to insert <code>@here</code> and <code>@channel</code> alert tags respectively. If not specified, no preamble is used. style String No Display style for Slack messages. Options are <code>block</code> (default), <code>attachment</code> and <code>plain</code>. The first two use the corresponding block or attachment message construction mechanism provided by Slack to make messages more presentable. type String Yes <code>slack</code>. webhook_url String Yes The webhook URL provided by Slack for sending messages."},{"location":"07-connectors.html#using-the-slack-connector","title":"Using the Slack Connector","text":"<p>The slack connector provides two distinct interfaces:</p> <ol> <li> <p>A native Python interface</p> </li> <li> <p>A command line interface.</p> </li> </ol>"},{"location":"07-connectors.html#python-interface-for-slack-connectors","title":"Python Interface for Slack Connectors","text":"<p>Python scripts can directly access the underlying Python interface of a slack connector. In this case, the connector returns a <code>lava.lib.slack.Slack</code> object as described in the lava API documentation.</p> <p>As an example, consider an exe job specification that looks something like this:</p> <pre><code>{\n    \"job_id\": \"...\",\n    \"parameters\": {\n        \"connections\": {\n            \"slack\": \"slack-connection-id\"\n        }\n    },\n    \"payload\": \"my-payload.py ...\"\n}\n</code></pre> <p>A Python program can use the slack connector like this:</p> <pre><code>import os\nfrom lava.connection import get_slack_connection\n\n# If running as a lava exe/pkg/docker, get some info provided by lava in the\n# environment. Assume our connector is labeled `slack` in the job spec.\nrealm = os.environ['LAVA_REALM']\nconn_id = os.environ['LAVA_CONNID_SLACK']\n\n# Get a slack connection \nslacker = get_slack_connection(conn_id, realm)\n\n# Send a formatted message\nslacker.send(\n    subject='Oh no',\n    message='Your oscillation overthruster has malfunctioned',\n    style='attachment',  # Overrides value in connection spec.\n    colour='#ff0000'  # Nice bright red. Overrides value in connection spec.\n)\n</code></pre>"},{"location":"07-connectors.html#executable-interface-for-slack-connectors","title":"Executable Interface for Slack Connectors","text":"<p>When used with exe, pkg and docker job types (e.g. shell scripts), the connection is implemented by the <code>lava-slack</code> command.</p> <p>When used as a connection script within a lava job, the <code>-r REALM</code> and <code>-c CONN_ID</code> arguments don't need to be provided by the job as these are provided by lava in the connection script.</p> <p>Also, values for the <code>---bar-colour</code>, <code>--from</code>, <code>--preamble</code> and <code>--style</code> options will be supplied from the connection specification where possible. These values can be overridden by providing the appropriate options to then connection script.</p> <pre><code>usage: lava-slack [-h] [--profile PROFILE] [-v] -c CONN_ID [-r REALM]\n                  [--bar-colour COLOUR] [--from NAME] [--preamble PREAMBLE]\n                  [-s SUBJECT] [--style {block,plain,attachment}]\n                  [--no-colour] [-l LEVEL] [--log LOG] [--tag TAG]\n                  [FILENAME]\n\nSend Slack messages using lava slack connections.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --profile PROFILE     As for AWS CLI.\n  -v, --version         show program's version number and exit\n\nlava arguments:\n  -c CONN_ID, --conn-id CONN_ID\n                        Lava connection ID. Required.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the environment\n                        variable LAVA_REALM must be set.\n\nslack arguments:\n  --bar-colour COLOUR   Colour for the sidebar for messages sent using\n                        attachment style. This can be any hex colour code or\n                        one of the Slack special values good, warning or\n                        danger.\n  --from NAME           Message sender. If not specified, the value specified\n                        in the connection specification, if any, will be used.\n  --preamble PREAMBLE   An optional preamble at the start of the message.\n                        Useful values include things such as &lt;!here&gt; and\n                        &lt;!channel&gt; which will cause Slack to insert @here and\n                        @channel alert tags respectively.\n  -s SUBJECT, --subject SUBJECT\n                        Message subject.\n  --style {block,plain,attachment}\n                        Slack message style. Must be one of attachment, block,\n                        plain. If not specified, any value specified in the\n                        connection specification will be used or block as a\n                        last resort.\n\nmessage source arguments:\n  FILENAME              Name of file containing the message body. If not\n                        specified or \"-\", the body will be read from stdin.\n                        Only the first 3000 bytes are read.\n\nlogging arguments:\n  --no-colour, --no-color\n                        Don't use colour in information messages.\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The default\n                        is info.\n  --log LOG             Log to the specified target. This can be either a file\n                        name or a syslog facility with an @ prefix (e.g.\n                        @local0).\n  --tag TAG             Tag log entries with the specified value. The default\n                        is lava-slack.\n</code></pre> <p>As an example, consider an exe job specification that looks something like this:</p> <pre><code>{\n    \"job_id\": \"...\",\n    \"parameters\": {\n        \"connections\": {\n            \"slack\": \"slack-connection-id\"\n        }\n    },\n    \"payload\": \"my-payload.sh ...\"\n}\n</code></pre> <p>Note the <code>slack</code> connection. This will provide the job with an environment variable <code>LAVA_CONN_SLACK</code> which points to the executable handling the connection.</p> <p>If the job payload is a shell script, the connector would be invoked thus:</p> <pre><code># Send a Slack message\n$LAVA_CONN_SLACK --subject \"Oh no\" &lt;&lt;!\n    Dear Buckaroo,\n\n    Your oscillation overthruster has malfunctioned.\n\n    -- John Bigboot\u00e9\n!\n</code></pre>"},{"location":"07-connectors.html#connector-type-smb","title":"Connector type: smb","text":"<p>The smb connector manages connections to SMB file shares.</p> <p>Info</p> <p>The smb connector has undergone a significant upgrade in v8.0 (Incahuasi) to support the smbprotocol SMB implementation as well as the existing pysmb. The former has a number of advantages (e.g. DFS support). An effort has been made to retain backward compatibility for lava jobs, notwithstanding the two implementations have significant interface differences. Be warned, though, that some more esoteric usage patterns could experience a backward compatibility issue.</p> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. domain String No The network domain. Defaults to an empty string. enabled Boolean Yes Whether or not the connection is enabled. encrypt Boolean No Whether to encrypt the connection between Lava and the SMB server. Only available with the <code>smbprotocol</code> connection subtype. Default <code>false</code>. host String Yes DNS name or IP address of the SMB host. is_direct_tcp Boolean No If <code>false</code>, use NetBIOS over TCP/IP. If <code>true</code> use SMB over TCP/IP. Default <code>false</code>. my_name String No Local NetBIOS machine name that will identify the origin of connections. If not specified, defaults to the first 15 characters of <code>lava-&lt;REALM&gt;</code> password String Yes Name of the SSM parameter containing the password for authenticating to the SMB server. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. port Integer No Connection port number. If not specified, 139 is used if <code>is_direct_tcp</code> is <code>false</code> and 445 otherwise. remote_name String Yes NetBIOS machine name of the remote server. subtype String No Which connection type to use, <code>smbprotocol</code> or the default <code>pysmb</code>. To use encryption or DFS for the connection use the <code>smbprotocol</code> subtype. type String Yes <code>smb</code>. use_ntlm_v2 Boolean No Indicates whether pysmb should be NTLMv1 or NTLMv2 authentication algorithm for authentication. Default is <code>true</code>. user String Yes User name for authenticating to the SMB server. <p>The connector supports the smb_get and smb_put job types.</p>"},{"location":"07-connectors.html#use-with-python-based-executable-jobs","title":"Use with Python-based Executable Jobs","text":"<p>The connector can also be used with Python based exe and pkg jobs that invoke the lava connection manager directly. In this case, the connector returns a <code>lava.lib.smb.LavaSMBConnection</code> which provides a basic, common interface to the different subtypes.</p> <p>The <code>lava.lib.smb.LavaSMBConnection</code> interface class provides enough functionality for most common use-cases (list path, put file, get file etc.).</p> <p>The concrete implementation is handled by of one of two subclasses (depending on the <code>subtype</code> given in the connection spec):</p> <ul> <li> <p>a <code>lava.lib.smb.PySMBConnection</code> which implements <code>LavaSMBConnection</code> using     the Python package     pysmb.     This is the default if no connection <code>subtype</code> is given.</p> </li> <li> <p>a <code>lava.lib.smb.SMBProtocolConnection</code> which implements <code>LavaSMBConnection</code>     using the Python package     smbprotocol.</p> </li> </ul> <p>Note that this is the low level connector. It does not handle moving files in or out of S3 or Jinja rendering of parameters. It is up to the caller to do that as required.</p> <p>If the SMB connector key in the job's <code>connectors</code> map is <code>fserver</code>, typical usage would be something like:</p> <pre><code>import os\nfrom lava.connection import get_smb_connection\n\n# Get an smb.SMBConnection.SMBConnection instance\nsmb_conn = get_smb_connection(\n    conn_id=os.environ['LAVA_CONNID_FSERVER'],\n    realm=os.environ['LAVA_REALM']\n)\n\n# Get a file from share 'Public' and store locally\nwith open('local.txt', 'wb') as fp:\n    attributes, size = smb_conn.retrieve_file('Public', 'some_file.txt', fp)\n\nsmb_conn.close()\n</code></pre>"},{"location":"07-connectors.html#use-with-other-executable-jobs","title":"Use with Other Executable Jobs","text":"<p>When used with other exe and pkg job types (e.g. shell scripts), the connection is implemented by the <code>lava-smb</code> command.</p> <p>This is a somewhat higher level interface to the connector in that it can also handle moving data in and out of S3. Jinja rendering is handled as per the smb_get and smb_put job types.</p> <p>If the SMB connector key in the job's <code>connectors</code> map is <code>fserver</code>, usage is:</p> <pre><code>usage: $LAVA_CONN_FSERVER [-J] [-l LEVEL] {put,get} ...\n\nsub-commands:\n  {put,get}\n    put                 Copy a file to an SMB file share.\n    get                 Copy a file from an SMB file share.\n\noptional arguments:\n  -J, --no-jinja        Disable Jinja rendering of the transfer parameters.\n\nlogging arguments:\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The Default\n                        is info.\n</code></pre> <p>Usage for the <code>get</code> sub-command:</p> <pre><code>usage: $LAVA_CONN_FSERVER get [options] SMB-path file\n\npositional arguments:\n  SMB-path              Source location. Must be in the form share-name:path.\n                        This will be jinja rendered.\n  file                  Target file. Values starting with s3:// will be copied\n                        to S3. This will be jinja rendered.\n\noptional arguments:\n  -k KMS_KEY_ID, --kms-key-id KMS_KEY_ID\n                        AWS KMS key to use for uploading data to S3.\n</code></pre> <p>Usage for the <code>put</code> sub-command:</p> <pre><code>usage: $LAVA_CONN_FSERVER put [options] file SMB-path\n\npositional arguments:\n  file         Source file. Values starting with s3:// will be copied from S3.\n               This will be jinja rendered.\n  SMB-path     Target location. Must be in the form share-name:path. This will\n               be jinja rendered.\n\noptional arguments:\n  -m, --mkdir  Create the target directory if it doesn't exist\n</code></pre> <p>For example, the following code in an exe job would transfer files between S3 and the <code>Public</code> share on an SMB server:</p> <pre><code>#!/bin/bash\n\n# Copy file from S3 to SMB\n$LAVA_CONN_FSERVER put --mkdir \\\n    s3://my-bucket/data.csv Public:/a/path/data.csv\n\n# Copy file from SMB to S3\n$LAVA_CONN_FSERVER get --kms-key-id alias/data \\\n    Public:/a/path/data.csv s3://my-bucket/data.csv\n</code></pre>"},{"location":"07-connectors.html#connector-type-sqlite3","title":"Connector type: sqlite3","text":"<p>The sqlite3 connector handles connections to SQLite3 file based databases.</p> <p>Its use in general lava jobs is pretty marginal at best. It is mostly present to facilitate testing of lava itself.</p> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. host String Yes The name of the file containing the SQLite3 database. If it starts with <code>s3://</code>, the file will be copied from S3 when the connection is created and returned to S3 when the connection is closed if it has been modified. port Number Yes* A value is required but is ignored. preserve_case Boolean No If <code>true</code>, don't fold database object names to lower case when quoting them for use in db_from_s3 jobs. The default is <code>false</code> (i.e. case folding is enabled). type String Yes <code>sqlite3</code>. user String Yes* A value is required but is ignored. <p>Info</p> <p>Fields with a Required column marked with <code>*</code> must be present but the value is ignored. This is an unfortunate interface idiosyncrasy resulting from the need to maintain some internal compatibility with the other database connectors.</p> <p>When used with exe and pkg job types, the connection is implemented by the <code>sqlite3</code> CLI.  It is invoked with the following options:</p> <pre><code>sqlite3 -bail -batch DATABASE-FILE\n</code></pre>"},{"location":"07-connectors.html#connector-type-ssh-scp-sftp","title":"Connector type: ssh, scp, sftp","text":"<p>This group of connectors provides support for the SSH family of clients.</p> <p>When used with exe and pkg jobs, each connector provides an environment variable pointing to a script that will run the corresponding CLI with SSH keys managed in the background.</p> Field Type Required Description conn_id String Yes Connection identifier. description String No Description. enabled Boolean Yes Whether or not the connection is enabled. ssh_key String Yes The name of an encrypted SSM parameter containing the SSH private key. There must not be any passphrase on the key. For a given <code>&lt;REALM&gt;</code>, the SSM parameter name must be of the form <code>/lava/&lt;REALM&gt;/...</code> and the value must be a secure string encrypted using the <code>lava-&lt;REALM&gt;-sys</code> KMS key. ssh_options List[String] No A list of SSH options as per ssh_config(5). e.g. <code>StrictHostKeyChecking=no</code> type String Yes <code>ssh</code>, <code>sftp</code> or <code>scp</code>. <p>The process for saving an SSH private key in the SSM parameter store using the  AWS CLI looks like this:</p> <pre><code># Create a new SSH key\nssh-keygen -f mykey\n\n# Upload the private key to the SSM parameter store. Here realm name is \"dev\"\naws ssm put-parameter --name \"/lava/dev/ssh01/ssh-key\"  \\\n    --description \"SSH key for ssh01\" \\\n    --type SecureString \\\n    --value \"$(cat mykey)\" \\\n    --key-id alias/lava-dev-sys\n</code></pre>"},{"location":"08-job-actions.html","title":"Job Actions","text":"<p>Lava can perform specified actions on the completion of a job depending on whether the job:</p> <ul> <li>completed successfully (<code>on_success</code>); or</li> <li>failed and will be retried (<code>on_retry</code>); or</li> <li>failed and won't be retried (<code>on_fail</code>).</li> </ul> <p>Actions are not executed if the job does not start (e.g. if the job specification is malformed).</p> <p>These actions are defined as part of the job specification or can be inherited from the realm specification. An action specification at the job level will override one at the realm level.</p> <p>A typical action specification within a job or realm specification looks like:</p> <pre><code>{\n  \"on_fail\": [\n    {\n      \"action\": \"sns\",\n      \"message\": \"{{job.job_id}}@{{realm.realm}} ({{job.run_id}}) failed: {{result.error}}\",\n      \"topic\": \"arn:aws:sns:ap-southeast-2:990565496141:lava-proto-notices\"\n    }\n  ],\n  \"on_retry\": [\n    {\n      \"action\": \"log\",\n      \"message\": \"Failed on attempt {{ globals.lava.iteration }} but will try again!\"\n    }\n  ],\n  \"on_success\": [\n    {\n      \"action\": \"log\",\n      \"message\": \"It worked!\"\n    },\n    {\n      \"action\": \"dispatch\",\n      \"job_id\": \"downstream_job\"\n    }\n  ]\n}\n</code></pre> <p>Actions in an action list are executed in the order specified.</p> <p>Apart from the mandatory <code>action</code> field, action specification parameters are dependent on the action type.</p> <p>Some actions support Jinja rendering of some of the action parameters.</p> <p>Actions are implemented using a simple plugin mechanism. Adding new actions is relatively straightforward.</p>"},{"location":"08-job-actions.html#action-type-dispatch","title":"Action type: dispatch","text":"<p>The dispatch action type dispatches another lava job.</p> <p>Any globals available in the job that triggers the action will also be passed in the dispatch request to the job being dispatched.</p> Field Type Required Description action String Yes The action type: <code>dispatch</code>. delay String No Dispatch message sending delay in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds) or <code>m</code> (minutes). The maximum allowed value is 15 minutes. job_id String Yes The job to be dispatched. This is Jinja rendered prior to use. parameters Map[String,*] No A map of parameters that will be sent with the dispatch. These are Jinja rendered prior to use. The dispatched job will receive these in the <code>parameters</code> element of its job specification. <p>The Jinja rendered action parameters have the following variables injected.</p> Name Type Description globals dict[str,*] The globals map for the job that triggered the action. job dict[str,*] The augmented job specification for the job that triggered the action. realm dict[str,*] The realm specification. result dict[str,*] The result object from the job that triggered the action. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>Refer to Jinja Rendering in Lava for more information.</p>"},{"location":"08-job-actions.html#action-type-email","title":"Action type: email","text":"<p>The email action uses either the email or a bare-metal AWS Simple Email Service (SES) default to send an email message.</p> Field Type Required Description action String Yes The action type: <code>email</code>. attachments List[string] No A list of file names to add as attachments. If specified, <code>email_conn</code> must also be specified, as the default bare-metal AWS SES sender does not support attachments. Files can be local or in S3 (<code>s3://...</code>). Each filename is individually Jinja rendered. Be careful with this to avoid using untrusted globals in filenames. email_conn String No The name of an email connector to use for sending email. If not specified, then bare-metal AWS SES is used. from String No The source email address. If not specified, lava will defer to the <code>email_conn</code> connector if specified, or else look for a value in the realm specification (see below) or construct a default by scanning the SES configuration for validated domains. message String Yes The message body. If the message text begins with <code>&lt;HTML&gt;</code> (case-insensitive), it is sent as a HTML message body, otherwise as text. This is Jinja rendered prior to use. region String No The AWS region name for the SES service. If not specified, lava will look for a value in the realm specification (see below) or use the default of <code>us-east-1</code>. Ignored if <code>email_conn</code> is specified. subject String Yes The message subject. This is Jinja rendered prior to use. to String or List[String] Yes The destination email address(es). <p>The Jinja rendered action parameters have the following variables injected.</p> Name Type Description globals dict[str,*] The globals map for the job that triggered the action. job dict[str,*] The augmented job specification for the job that triggered the action. realm dict[str,*] The realm specification. result dict[str,*] The result object from the job that triggered the action. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>Refer to Jinja Rendering in Lava for more information.</p> <p>Default values for some of the fields required for the email action to use AWS SES can be set in the realms table by setting entries in the <code>config</code> realm entry, thus:</p> <pre><code>{\n  \"...\": \"... other realm related elements ...\",\n  \"config\": {\n    \"ses_region\": \"... SES region name ...\",\n    \"ses_from\": \"... Default source email address\"\n  }\n}\n</code></pre>"},{"location":"08-job-actions.html#action-type-event","title":"Action type: event","text":"<p>The event action type sends an event to AWS EventBridge.</p> Field Type Required Description action String Yes The action type: <code>event</code>. detail_type String No The event detail type. This is Jinja rendered prior to use. Defaults to <code>Lava Job Action</code>. detail String or Map[String,*] No Event detail. The string, or the map values, are Jinja rendered prior to use. Defaults to a map containing realm, worker, job_id, run_id and exit_status. event_bus String No The event bus name. This is Jinja rendered prior to use. Defaults to the default event bus. resources List[String] No A list of resources. This is Jinja rendered prior to use. Defaults to an empty list. source String No The event source. This is Jinja rendered prior to use. Defaults to <code>lava.&lt;REALM&gt;</code>. <p>Info</p> <p>The realm CloudFormation template only provides workers with permission to put events on the default event bus.</p> <p>The Jinja rendered action parameters have the following variables injected.</p> Name Type Description globals dict[str,*] The globals map for the job that triggered the action. job dict[str,*] The augmented job specification for the job that triggered the action. realm dict[str,*] The realm specification. result dict[str,*] The result object from the job that triggered the action. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>Refer to Jinja Rendering in Lava for more information.</p> <p>This is the minimal action specification:</p> <pre><code>{\n    \"on_success\": [\n        {\n            \"action\": \"event\"\n        }\n    ]\n}\n</code></pre> <p>It will produce an event in the default event bus like so:</p> <pre><code>{\n  \"version\": \"0\",\n  \"id\": \"1bdf1ead-95ef-79fd-1342-8f8ff1e57765\",\n  \"detail-type\": \"Lava Job Action\",\n  \"source\": \"lava.dev\",\n  \"account\": \"123456789012\",\n  \"time\": \"2021-07-25T05:12:23Z\",\n  \"region\": \"ap-southeast-2\",\n  \"resources\": [],\n  \"detail\": {\n    \"realm\": \"dev\",\n    \"worker\": \"core\",\n    \"job_id\": \"action/event/default\",\n    \"run_id\": \"05175b7c-8b0d-40ed-af31-0bc846bbe400\",\n    \"exit_status\": 0\n  }\n}\n</code></pre> <p>This is a sample, custom event:</p> <pre><code>{\n    \"on_success\": [\n        {\n            \"action\": \"event\",\n            \"detail\": {\n                \"job_id\": \"{{ job.job_id }}\",\n                \"realm\": \"{{ realm.realm }}\",\n                \"result\": \"{{result | tojson}}\",\n                \"run_id\": \"{{job.run_id }}\",\n                \"status\": \"success\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"08-job-actions.html#action-type-log","title":"Action type: log","text":"<p>The log action type writes a message to the logger local to the worker with log level <code>info</code>. The worker log configuration is specified as command line arguments so it depends on these where the message ends up.</p> <p>For more information on worker logging arguments run:</p> <pre><code>lava-worker --help\n</code></pre> Field Type Required Description action String Yes The action type: <code>log</code>. message String Yes The message to log. This is Jinja rendered. <p>The Jinja rendered action parameters have the following variables injected.</p> Name Type Description globals dict[str,*] The globals map for the job that triggered the action. job dict[str,*] The augmented job specification for the job that triggered the action. realm dict[str,*] The realm specification. result dict[str,*] The result object from the job that triggered the action. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>Refer to Jinja Rendering in Lava for more information.</p>"},{"location":"08-job-actions.html#action-type-slack","title":"Action type: slack","text":"<p>The slack action sends a message to a Slack channel. It relies on the slack connector which uses Slack webhooks.</p> Field Type Required Description action String Yes The action type: <code>slack</code>. colour String No As per the <code>colour</code>  field of the slack connector. If not specified, the value specified in the connector is used. from String No An arbitrary source identifier for display in the Slack message. If not specified, any default value specified in the connector will be used. message String Yes The message body. This is Jinja rendered prior to use. Only the first 3,000 characters of the rendered value are used. preamble String No As per the <code>preamble</code>  field of the slack connector. If not specified, the value specified in the connector is used. slack_conn String Yes The name of a slack connector to use. style String No As per the <code>style</code>  field of the slack connector. If not specified, the value specified in the connector is used. subject String No The message subject. This is Jinja rendered prior to use. Only the first 250 characters of the rendered value are used. <p>The Jinja rendered action parameters have the following variables injected.</p> Name Type Description globals dict[str,*] The globals map for the job that triggered the action. job dict[str,*] The augmented job specification for the job that triggered the action. realm dict[str,*] The realm specification. result dict[str,*] The result object from the job that triggered the action. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>Refer to Jinja Rendering in Lava for more information.</p>"},{"location":"08-job-actions.html#action-type-sns","title":"Action type: sns","text":"<p>The sns action type sends a message to an SNS topic.</p> Field Type Required Description action String Yes The action type: <code>sns</code>. message String or Map[String,*] Yes The message body. This can be either a string or an object. If it is an object it will be JSON encoded before sending. This is Jinja rendered. subject String No The message subject. This is Jinja rendered. The rendered value will be truncated to 100 characters. topic String Yes The topic ARN. <p>The Jinja rendered action parameters have the following variables injected.</p> Name Type Description globals dict[str,*] The globals map for the job that triggered the action. job dict[str,*] The augmented job specification for the job that triggered the action. realm dict[str,*] The realm specification. result dict[str,*] The result object from the job that triggered the action. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>Refer to Jinja Rendering in Lava for more information.</p> <p>This is an example of how a JSON formatted object can be sent:</p> <pre><code>{\n    \"on_success\" : [\n        {\n            \"action\": \"sns\",\n            \"message\": {\n                \"key1\": \"Hello world\",\n                \"key2\": {\n                    \"key2a\": \"{{ job.globals.a_global }}\",\n                    \"key2b\": 42\n                }\n            },\n            \"topic\": \"arn:aws:sns:ap-southeast-2:123456789012:mytopic\"\n        }\n    ]\n}\n</code></pre>"},{"location":"08-job-actions.html#action-type-sqs","title":"Action type: sqs","text":"<p>The sqs action type sends a message to an SQS queue.</p> Field Type Required Description action String Yes The action type: <code>sqs</code>. dedup_id String No The token used for deduplication of sent messages. This parameter applies only to FIFO queues. This is Jinja rendered. delay String No Message sending delay in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds) or <code>m</code> (minutes). The maximum allowed value is 15 minutes. group_id String No The tag that specifies that a message belongs to a specific message group. This parameter applies only to FIFO queues. This is Jinja rendered. message String or  Map[String,*] Yes The message body. This can be either a string or an object. If it is an object it will be JSON encoded before sending. This is Jinja rendered. queue String Yes The name or URL of the queue (not the ARN!). <p>The Jinja rendered action parameters have the following variables injected.</p> Name Type Description globals dict[str,*] The globals map for the job that triggered the action. job dict[str,*] The augmented job specification for the job that triggered the action. realm dict[str,*] The realm specification. result dict[str,*] The result object from the job that triggered the action. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>Refer to Jinja Rendering in Lava for more information.</p> <p>This is an example of how a JSON formatted object can be sent:</p> <pre><code>{\n    \"on_success\" : [\n        {\n            \"action\": \"sqs\",\n            \"message\": {\n                \"key1\": \"Hello world\",\n                \"key2\": {\n                    \"key2a\": \"{{ job.globals.a_global }}\",\n                    \"key2b\": 42\n                }\n            },\n            \"topic\": \"myqueue\"\n        }\n    ]\n}\n</code></pre>"},{"location":"08-job-actions.html#action-type-state","title":"Action type: state","text":"<p>The state action type allows jobs to post state items that can be subsequently retrieved by other jobs or authorised external actors.</p> Field Type Required Description action String Yes The action type: <code>state</code>. kms_key String No The secure state item type supports KMS encryption of the value. This field specifies the KMS key to use, either as a KMS key ARN or a key alias in the form <code>alias/key-id</code>. Defaults to the <code>sys</code> key for the lava realm. Ignored for other state item types. publisher String No An arbitrary identifier for the entity posting the event item. Not used by lava itself. The default is the job ID. This is Jinja rendered prior to use. state_id String Yes The state entry ID. This is Jinja rendered prior to use. ttl String No Time to live for the state item specified as a duration in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes), <code>h</code> (hours) or <code>d</code> (days). If greater than the maximum specified for the realm, it will be silently reduced to that value. A default value is provided by lava which can be overridden at the realm level. type String No The state item type. If not specified, the <code>json</code> type is used. value * Yes A JSON encodable object. This is Jinja rendered prior to use. <p>Info</p> <p>Do not create state items with a <code>state_id</code> starting with <code>lava</code>. This prefix is reserved.</p> <p>The Jinja rendered action parameters have the following variables injected.</p> Name Type Description globals dict[str,*] The globals map for the job that triggered the action. job dict[str,*] The augmented job specification for the job that triggered the action. realm dict[str,*] The realm specification. result dict[str,*] The result object from the job that triggered the action. start datetime The local time when the job run started. state dict[str,*] A dictionary of the state items imported into the job, keyed on state_id. The default values are updated at run-time with any current values obtainable from the state table. ustart datetime The UTC time when the job run started. utils dict[str,runnable] A dictionary of utility functions that can be used in the Jinja markup. <p>Refer to Jinja Rendering in Lava for more information.</p>"},{"location":"09-lava-state-manager.html","title":"The Lava State Manager","text":"<p>Lava jobs generally act in isolation from each other or within the scope of a controlling master job that can pass global variables to the child jobs. In some situations, it is useful for one job to be able to pass state information directly to other jobs in a peer to peer fashion.</p> <p>The lava state manager provides the ability for a job to publish state information (state items) for subsequent use by another job or by an authorised external entity. State items are persistent, structured data elements with a defined expiry time. State items are stored in the state DynamoDB table.</p> <p>Note</p> <p>The state manager is intended for use with small amounts of data (e.g. control parameters, job status information etc.). Where large amounts of data need to be exchanged between jobs, S3 is the preferred solution.</p> <p>The state manager supports the following capabilities:</p> <ul> <li> <p>Ability to post state items from lava jobs and     external actors with appropriate authorisation.</p> </li> <li> <p>Ability to read state items by both lava     jobs and external actors with appropriate authorisation.</p> </li> <li> <p>Support for Jinja rendering of state item values into job specifications at     run-time.</p> </li> </ul> <p>The state manager handles any encoding / decoding required during the posting / retrieval process.</p>"},{"location":"09-lava-state-manager.html#posting-state-items","title":"Posting State Items","text":"<p>State items can be posted via the following mechanisms:</p> <ul> <li> <p>The state action type.</p> </li> <li> <p>The lava state API.</p> </li> <li> <p>The lava state utility.</p> </li> </ul> <p>Posting an item will create a new item or completely replace an existing item, as appropriate.</p> <p>Info</p> <p>Do not create state items with a <code>state_id</code> starting with <code>lava</code>. This prefix is reserved.</p> <p>The lava state manager does not support incremental update of an existing state item.</p>"},{"location":"09-lava-state-manager.html#retrieving-state-items","title":"Retrieving State Items","text":"<p>State items can be retrieved via the following mechanisms:</p> <ul> <li> <p>Real-time retrieval during job initialisation.</p> </li> <li> <p>The lava state API.</p> </li> <li> <p>The lava state utility.</p> </li> </ul> <p>The retrieval process uses DynamoDB strongly consistent reads.</p> <p>Note</p> <p>While this gives more reliability in state item availability, it does require higher throughput on the state table.</p>"},{"location":"09-lava-state-manager.html#state-item-retrieval-during-job-initialisation","title":"State Item Retrieval During Job Initialisation","text":"<p>When a job is run, an initialisation procedure populates elements of the job specification to produce the augmented job specification. As part of this process, lava expands the <code>state</code> element of the job specification, if present. This element is a map of state item IDs (state_id) and default values. If a state_id is present in the state table, the value will be extracted and used to replace the default value in the job specification. The resulting map is then made available to the Jinja rendering process used to prepare the job.</p> <p>This means, for example, that one job can post a state item (e.g. using an on_success job action) that lava will then automatically make available to another job via the Jinja renderer.</p> <p>Neat eh?</p> <p>The state map can be referenced in a Jinja expression as <code>job.state</code> or via the shorthand <code>state</code>. A state item with a <code>state_id</code> of <code>my_state_id</code> can then be referenced in a Jinja expression using any of the following equivalents:</p> <pre><code>{{ job.state.my_state_id }}\n{{ job.state['my_state_id'] }}\n{{ state.my_state_id }}\n{{ state['my_state_id'] }}\n</code></pre> <p>The square brackets variant is required when <code>my_state_id</code> is not a valid Python identifier.</p>"},{"location":"09-lava-state-manager.html#example-usage-of-the-state-manager","title":"Example Usage of the State Manager","text":"<p>Consider the following simple job that gets triggered by an object creation in S3 (via s3trigger). The job simply counts the number of lines in the file and stores the result as a state item with ID <code>sid</code>.</p> <pre><code>{\n  \"description\": \"Count lines in file from S3\",\n  \"enabled\": true,\n  \"globals\": {\n    \"bucket\": \"-- provided by s3trigger --\",\n    \"key\": \"-- provided by s3trigger --\"\n  },\n  \"job_id\": \"demo/line-counter\",\n  \"parameters\": {\n    \"args\": [\n      \"lava-state put sid -p lines=$(aws s3 cp 's3://{{globals.bucket}}/{{globals.key}}' - | wc -l)\"\n    ]\n  },\n  \"payload\": \"/bin/sh -c\",\n  \"type\": \"cmd\",\n  \"worker\": \"core\"\n}\n</code></pre> <p>The resulting state item will look like this:</p> <pre><code>{\n  \"publisher\": \"demo/line-counter\",\n  \"state_id\": \"sid\",\n  \"timestamp\": \"2022-03-26T12:18:04+11:00\",\n  \"ttl\": 1648343884,\n  \"type\": \"json\",\n  \"value\": \"{\\\"lines\\\": \\\"45\\\"}\"\n}\n</code></pre> <p>Another job can access this state item in the job spec rendering process or in the job logic itself. Here is an example of the former.</p> <pre><code>{\n  \"description\": \"Print out how many lines there were\",\n  \"dispatcher\": \"...\",\n  \"enabled\": true,\n  \"job_id\": \"demo/line-reporter\",\n  \"payload\": \"echo The file contained {{state.sid}} lines\",\n  \"state\": {\n    \"sid\": \"-- default value to be replaced at run-time --\"\n  },\n  \"schedule\": \"...\",\n  \"type\": \"cmd\",\n  \"worker\": \"core\"\n}\n</code></pre> <p>Note that the state item that is required at run-time has to be declared (with a default value) so that it can be obtained from the state table and made available to the Jinja renderer.</p>"},{"location":"09-lava-state-manager.html#the-lava-state-api","title":"The Lava State API","text":"<p>The lava state API provides for the posting and retrieval of state items.</p> <p>The code to post a state item would look like this:</p> <pre><code>import os\nfrom lava.lib.state import LavaStateItem\n\nrealm = os.environ['LAVA_REALM']\n\nvalue1 = \"Hello world\"\n# .. or maybe ...\nvalue2 = {\n    \"volcano_name\": \"Tronador\",\n    \"elevation\": 3491,\n    \"active\": False\n}\n\n# Create our item. This is a local operation so far.\nmy_state_item = LavaStateItem.new('json', 'my_state_id', realm, value1, ttl='2d')\n\n# Changed my mind\nmy_state_item.value = value2\n\n# Post to DynamoDB\nmy_state_item.put()\n</code></pre> <p>Secure state items are KMS encrypted for storage and automatically decrypted on loading. An additional parameter, <code>kms_key</code>, specifies the KMS key to use, either as a KMS key ARN or a key alias in the form <code>alias/key-id</code>. This defaults to the <code>sys</code> key for the lava realm.</p> <p>Info</p> <p>Note that the use of KMS encryption imposes a maximum size limit of 4096 bytes on the JSON encoded state item value.</p> <pre><code>import os\nfrom lava.lib.state import LavaStateItem\n\nrealm = os.environ['LAVA_REALM']\nvalue = 'Big Secret'\n\n# Create a secure item with the default key.\nmy_state_item = LavaStateItem.new('secure', 'my_state_id', realm, value)\n\n# or\nmy_state_item = LavaStateItem.new(\n    'secure', 'my_state_id', realm, value, kms_key='alias/my-key'\n)\n</code></pre> <p>To retrieve a state item:</p> <pre><code>import os\nfrom lava.lib.state import LavaStateItem\n\nrealm = os.environ['LAVA_REALM']\n\nvalue = LavaStateItem.get('my_state_id', realm).value\n</code></pre> <p>The lava state manager handles all the encoding / decoding processes and the interaction with DynamoDB.</p>"},{"location":"10-installation-operation.html","title":"Lava Installation and Operation","text":"<p>Info</p> <p>Lava requires Python 3.9+. The minimum recommended version is 3.11.</p> <p>Warning</p> <p>Support for Python 3.9 ends with lava v8.2 (K\u012blauea).</p> <p>There are two distinct paths for deploying lava, depending on whether the goal is to:</p> <ol> <li> <p>Run a standalone, locally hosted worker for development,     experimentation or debugging; or</p> </li> <li> <p>Run a fully AWS hosted worker.</p> </li> </ol> <p>Both options require the core realm / worker AWS components, such as the, DynamoDB tables and worker SQS queues. The first option assumes the lava worker will run on the local desktop and hence does not require any AWS EC2 compute resources. The second option assumes the lava worker will be running on one or more EC2 instances.</p> <p>A lava realm can simultaneously contain workers of both types.</p> <p>Let's compare the two options in more detail.</p> Local Install AWS Hosted Lava repo Not required. Use pre-built components. Required. Realm AWS components Required. Required. Worker AWS components Minimal (worker SQS queue, KMS keys, S3 etc). No EC2. Required, including IAM, EC2 etc. Worker compute Local PC native (macOS, Linux) or one of the pre-built lava docker images. See Running Lava in Docker. AWS EC2 instances based on the lava AMI. Worker code deploy Basically <code>pip install</code> or <code>docker run</code>. See Desktop Lava Workers and Docker Based Lava Workers. A fully self-contained lava code bundle is placed in S3 for workers to find and install on boot. No <code>pip install</code>. Can't have production hanging on PyPI. Worker startup Manual. Fully automated. Worker security Permissions determined by the desktop user's AWS IAM profile. Permissions determined by dedicated IAM components created for the realm / worker. Suitable for production No. Yes. Suitable for multi-user No. Yes. AWS Costs Negligible for light usage. Primarily EC2 costs. <p>A quick navigator for both install options is provided below. Subsequent sections provide a lot more detail.</p> Local InstallAWS Hosted Install <p>Installation of a locally hosted lava worker follows these steps:</p> <ol> <li>Create a lava realm.</li> <li>Create a lava worker. Follow the instructions     for either a desktop worker or docker based worker, as appropriate.</li> </ol> <p>This path does not require the lava repo to be cloned. Pre-built versions of the required components are provided, either from PyPI, or as part of a release on GitHub.</p> <p>Installation of lava involves the following steps:</p> <ol> <li>Clone the lava repo and     initialise the build area.</li> <li>Download the Oracle client binaries.</li> <li>Build the code bundles.</li> <li>Deploy the code bundles.</li> <li>Build the lava EC2 AMI.</li> <li>Create a lava realm.</li> <li>Create a lava worker. Follow the instructions     for creating an EC2 based worker.</li> </ol> <p>This path does require the lava repo to be cloned. The repo provides  automation for elements of the deployment process, including creating the required artefact layout in S3.</p>"},{"location":"10-installation-operation.html#the-lava-repo","title":"The Lava Repo","text":""},{"location":"10-installation-operation.html#getting-started-with-the-repo","title":"Getting Started with the Repo","text":"<p>The lava repo is available on GitHub.</p> <p>After cloning the repo:</p> <pre><code>cd lava\n\n# Create the virtualenv and install required Python packages.\n# This can be rerun as needed, even with an existing virtualenv.\nmake init\n\n# Activate the virtualenv\nsource venv/bin/activate\n</code></pre> <p>Apart from a standard UNIX-like Python development environment, the following additional tools are required:</p> Tool Why it's needed aspell Spell checking the user guide. Install it on macOS with Homebrew. AWS CLI v2 Used for deployment of lava components and various other things. docker Cross platform builds, testing and building the lava docker images. Docker Desktop is just fine. packer Building the lava AMI. shellcheck Used as part of pre-commit checks. Install it on macOS with Homebrew. tokei Required to count lines of code (optional). Install it on macOS with Homebrew. <p>The Oracle basic client and SQL*Plus are also required. See Oracle Client Binaries.</p>"},{"location":"10-installation-operation.html#a-quick-tour-of-the-repo","title":"A Quick Tour of the Repo","text":"<pre><code>.\n\u251c\u2500\u2500 ami                         | Code and resources to build the lava AMI\n\u2502   \u251c\u2500\u2500 conf.d                  | Reusable build script components\n\u2502   \u251c\u2500\u2500 deploy -&gt; ../deploy     |\n\u2502   \u251c\u2500\u2500 os                      | Build scripts for various target O/S\n\u2502   \u251c\u2500\u2500 resources               | Local resources used in the build\n\u2502   \u2514\u2500\u2500 resources.s3            | Larger resources used in the build synced to S3\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 bin                         | Source for lava CLI utilities\n\u251c\u2500\u2500 cfn                         | CloudFormation templates (raw form only - see dist/cfn)\n\u251c\u2500\u2500 deploy                      | YAML config files for deploying into AWS accounts\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 dev-tools                   |\n\u2502   \u2514\u2500\u2500 templates               | lava job templates (auto built into the user guide)\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 dist                        | Ephemeral directory created by build process (ex-repo)\n\u2502   \u251c\u2500\u2500 ami                     | Manifests produced by AMI builds\n\u2502   \u251c\u2500\u2500 cfn                     | CloudFormation templates ready to deploy\n\u2502   \u251c\u2500\u2500 dev-tools               | Lava job framework builds\n\u2502   \u251c\u2500\u2500 doc                     | Lava user guide builds\n\u2502   \u251c\u2500\u2500 lambda                  | Lava Lambda function builds (zip files)\n\u2502   \u251c\u2500\u2500 jinlava                 | Builds of the jinlava Python package\n\u2502   \u251c\u2500\u2500 pkg                     | Lava install package builds by O/S\n\u2502   \u2514\u2500\u2500 test                    | Unit test coverage cache and reports\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 doc                         |\n\u2502   \u251c\u2500\u2500 img                     | Images referenced in the user guide\n\u2502   \u251c\u2500\u2500 img.src                 |\n\u2502   \u251c\u2500\u2500 mkdocs                  |\n\u2502   \u2514\u2500\u2500 user-guide              | Lava user guide source\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 docker                      | Source for building lava docker images\n\u2502   \u251c\u2500\u2500 common                  | Common (O/S independent) build components\n\u2502   \u251c\u2500\u2500 os                      | O/S dependent build components\n\u2502   \u2514\u2500\u2500 pkg                     | External packages required (e.g. Oracle)\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 etc                         | Tools and utilities needed to build / maintain lava\n\u2502   \u251c\u2500\u2500 boot                    | Boot scripts for EC2 instances running a lava worker\n\u2502   \u251c\u2500\u2500 builders                | Dockerfiles for building lava for foreign platforms\n\u2502   \u2514\u2500\u2500 git-hooks               |\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 external-packages           | External non-Python packages that lava requires\n\u2502   \u2514\u2500\u2500 oracle                  | \n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 lambda                      | Source for the lava Lambda functions\n\u2502   \u251c\u2500\u2500 dispatch                |\n\u2502   \u251c\u2500\u2500 metrics                 |\n\u2502   \u251c\u2500\u2500 s3trigger               |\n\u2502   \u2514\u2500\u2500 stop                    |\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 lava                        | Source for lava\n\u2502   \u251c\u2500\u2500 connection              | Lava connector handlers\n\u2502   \u251c\u2500\u2500 handlers                | Lava job type handlers\n\u2502   \u251c\u2500\u2500 lib                     | Miscellaneous library utilities\n\u2502   \u2514\u2500\u2500 resources               | Lava job framework source\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u251c\u2500\u2500 misc                        | Miscallaneous components, boot scripts etc\n\u2502   \u251c\u2500\u2500 boot                    | Boot scriptlets called from root.boot.sh\n\u2502   \u251c\u2500\u2500 boot.optional           | Optional boot scriptlets\n\u2502   \u2514\u2500\u2500 ssm                     | Legacy CFN for SSM command documents\n\u2502   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\u2514\u2500\u2500 test                        | Lava units tests and test jobs (gnarly)\n    \u251c\u2500\u2500 data                    | Test data (e.g. used to populate test DBs)\n    \u251c\u2500\u2500 jobs                    | Test jobs, connectors etc (lava job framework fmt)\n    \u251c\u2500\u2500 services                | Data for Docker compose services used for testing\n    \u2514\u2500\u2500 unit                    | Unit test source\n</code></pre> <p>Note</p> <p>On macOS, you may want to add the <code>dist</code> and <code>external-packages</code> directories to the exclusion list for Time Machine backups. They can get pretty big and it's all ephemeral.</p>"},{"location":"10-installation-operation.html#docker-in-the-build-process","title":"Docker in the Build Process","text":"<p>Docker is used as part of the build process for a number of lava components, either because of a requirement to match the characteristics of the target deployment environment, need for specialised tools, or because the build product itself is a docker artefact.</p> <p>Components that use docker as part of the build process include:</p> <ul> <li>the lava worker installation bundle</li> <li>the lava docker images</li> <li>the lava Lambda functions</li> <li>lava tests for docker based resources (databases etc).</li> </ul> <p>This has all been developed using Docker Desktop on macOS. Your mileage may vary on other platforms. These are the suggested settings in Docker Desktop on macOS.</p> <p></p>"},{"location":"10-installation-operation.html#the-local-docker-registry","title":"The Local Docker Registry","text":"<p>The build process sometimes requires multi-platform docker images to provide both x86 and ARM support. This in turn requires a docker registry to which these multi-platform images can be pushed as part of the build process. Unlike single platform images that can be built and then pushed to a registry in separate steps, multi-platform images must be built and pushed in a single step.</p> <p>The lava build process will start a local docker registry in a container as, and when, required as <code>localhost:5001</code>. This is managed using the jindr tool which is installed as part of the lava repo setup.</p> <pre><code>jindr --help\n# Start the local registry manually, if required\njindr start\n# The registry will continue to run until manually stopped\njindr stop\n# List the images in the registry\njindr images\n# Copy an image to ECR (latest + 8.1.0 tags)\njindr copy2ecr localhost:5001/build/lava/amzn2023-py3.11:latest 8.1.0\n</code></pre>"},{"location":"10-installation-operation.html#oracle-client-binaries","title":"Oracle Client Binaries","text":"<p>Lava's oracle connector requires the Oracle Instant Client. The following packages are required:</p> <ul> <li>Basic Light or Basic</li> <li>SQL*Plus</li> </ul> <p>The ZIP format for these must be downloaded from Oracle and placed in <code>external-packages/oracle</code> before any lava components are built. There is a particular layout required.  To download or update the required bundles run the following from the top of the repository.</p> <pre><code>make oracle\n</code></pre>"},{"location":"10-installation-operation.html#building-lava-components","title":"Building Lava Components","text":"<p>The build process is controlled by GNU make. To get help on the build process, run:</p> <pre><code>make help\n# ... or ...\nmake\n</code></pre> <p>These are the primary build targets controlled from the top level Makefile:</p> Target Description <code>cfn</code> The lava CloudFormation templates. <code>preview</code> The lava documentation, including the lava user guide and the API documentation. This will build the user guide and open a browser window to preview it locally. <code>lambda</code> The lava Lambda function code bundles. <code>jinlava</code> The jinlava Python package. <code>pkg</code> The lava worker code bundles. Additional parameters on the make command control whether the bundle is intended for the build host or a foreign host. <code>tools</code> The lava job framework. <p>These components are built by running make in one of the subdirectories:</p> <ul> <li>the lava AMI (<code>ami/</code>)</li> <li>lava docker images (<code>docker/</code>).</li> </ul>"},{"location":"10-installation-operation.html#a-thunder-run-for-the-reckless","title":"A Thunder Run for the Reckless","text":"<p>This is a quick summary of a typical build sequence. Please read the full section before launching into this. You, and possibly everyone who tries to use the mess you create, will be sorry if you don't.</p> <p>This process will leave lots of artefacts in the <code>dist</code> directory.</p> <pre><code># If you are using a private PyPI server, set that here.\n# Do not rely on pip.conf.\nexport PIP_INDEX_URL=...\n\n# Get ready to build.\nmake init\nsource venv/bin/activate\n\n# Download the Oracle binaries\nmake oracle\n\n# Build and preview the user guide because we all read the doco first, right?\nmake preview\n\n# Make the bits that don't need a builder docker image\nmake cfn jinlava tools\n\n# Make a couple of builder docker images. This will take quite a while.\n# The process does a full Python install from source for ARM and x86.\nmake builder runtime=amzn2023-py3.11\nmake builder runtime=amzn2023-py3.13\n\n# Make the lava worker install bundle for multiple runtimes/platforms\n# First ... for our build machine.\nmake pkg\n# Now use our builders to build for foreign hosts.\nmake pkg runtime=amzn2023-py3.11 platform=linux/amd64\nmake pkg runtime=amzn2023-py3.11 platform=linux/arm64\nmake pkg runtime=amzn2023-py3.13 platform=linux/amd64\nmake pkg runtime=amzn2023-py3.13 platform=linux/arm64\n\n# Make the lambda code bundles. These need the amzn2023-py3.13 builder image.\nmake lambda\n\n# Make the lava docker images. You might want to start this and head off to a\n# lunch.\ncd docker\nmake build-all\nmake check-all\n# Make sure the images have been pushed to our local private docker registry.\n# This registry is on localhost:5001 and runs in a container.\njindr images\n</code></pre>"},{"location":"10-installation-operation.html#building-the-cloudformation-templates","title":"Building the CloudFormation Templates","text":"<p>Note</p> <p>Pre-built versions of the CloudFormation templates are provided as part of a release on GitHub.</p> <p>Lava comes with several CloudFormation templates to build the core components for realms and workers. The sources for these are in the <code>cfn</code> directory. These are not deployable as-is. The CloudFormation templates must be built by doing:</p> <pre><code>make cfn\n</code></pre> <p>This will place ready-to-deploy versions in <code>dist/cfn/*.cfn.json</code>. Automatically generated documentation for these is placed in the same directory as <code>dist/cfn/*.cfn.md</code> and <code>dist/cfn/*.cfn.html</code>.</p> <p>Warning</p> <p>Once more ... Do not deploy CloudFormation templates from the <code>cfn/</code> directory. It will not work. Deploy only from <code>dist/cfn</code>.</p> Name Description lava-common.cfn.json Contains the common components that are not realm dependent. This stack needs to be deleted and recreated for each update. This is safe to do. lava-realm.cfn.json Contains the worker independent parts for creating a realm. See Creating a Lava Realm. lava-worker.cfn.json Contains the worker dependent parts for creating a worker. The realm must already exist. See Creating a Lava Worker. <p>Note</p> <p>Generally, it's not critical for the CloudFormation template minor version to match the deployed lava code minor version. Major versions should match.</p>"},{"location":"10-installation-operation.html#building-the-lava-worker-bundle-for-the-build-host","title":"Building the Lava Worker Bundle for the Build Host","text":"<p>The lava worker bundle is the artefact required for a deployed lava worker. It is a fully self-contained compressed tar file containing the lava code and includes all of the required Python packages. As the lava worker uses some C based Python modules, the build for the lava worker bundle must be done on the same O/S type, Python version and platform architecture as the target deployment environment.</p> <p>To create the lava worker code bundle to match the build host:</p> <pre><code>make pkg\n</code></pre> <p>If an alternative PyPI server is being used (e.g. a Sonatype Nexus instance), set the environment variable <code>PIP_INDEX_URL</code> appropriately:</p> <pre><code>PIP_INDEX_URL=... make pkg\n\n# or ...\nexport PIP_INDEX_URL=...\nmake pkg\n</code></pre> <p>The build artefacts are placed in the <code>dist</code> directory as:</p> <pre><code>dist/pkg/&lt;OS&gt;/lava-&lt;LAVA-VERSION&gt;-&lt;OS&gt;-&lt;PYTHON-VERSION&gt;-&lt;ARCHITECTURE&gt;.tar.bz2\n</code></pre> <p>e.g.</p> <pre><code>dist/pkg/amzn2023/lava-8.1.0-amzn2023-py3.11-aarch64.tar.bz2\n                             ^^^^^^^^^^^^^^^ ^^^^^^^\n                               \"runtime\"    \"platform\" (as the host defines itself)\n</code></pre>"},{"location":"10-installation-operation.html#building-the-lava-worker-bundle-for-a-foreign-host","title":"Building the Lava Worker Bundle for a Foreign Host","text":"<p>Note</p> <p>This process has changed in version 8.1 (K\u012blauea). It is now possible to build lava worker bundles for different runtimes and architectures using Docker Desktop on macOS. The process has not been tested on other environments although it uses only docker standard multi-platform build capabilities.</p> <p>First, terminology:</p> <ul> <li>Runtime: The operating system type + Python version (e.g. <code>amzn2023-py3.11</code>)</li> <li>Platform: hardware architecture. </li> <li>Foreign host: A host that differs in runtime or platform from the build host</li> </ul> <p>Platform terminology is quite varied, depending on context:</p> Platform O/S terminology Docker terminology Oracle S/W x86 <code>x86_64</code> <code>linux/amd64</code> <code>x86</code> ARM <code>arm64</code> (macOS) or <code>aarch64</code> (Linux) <code>linux/arm64</code> <code>arm64</code> <p>To run a lava worker bundle on a host, the runtime and platform must match. The build process described above assumes the bundle will be deployed on a machine with the same runtime and platform as the build machine.</p> <p>A foreign host build uses a docker container based on a builder image. The builder images support both x86 and ARM architectures using docker multi-platform builds.</p> <p>The process involves two steps:</p> <ol> <li>Build the builder image.</li> <li>Use the builder image to build the lava worker    bundle.</li> </ol>"},{"location":"10-installation-operation.html#building-the-builder-image","title":"Building the builder image","text":"<p>The docker builder image defines the runtime and platform for the lava worker bundle it produces.</p> <p>The process for \"building the builder\" is:</p> <pre><code>make builder runtime=&lt;RUNTIME&gt;\n# e.g.\nmake builder runtime=amzn2023-py3.11\n</code></pre> <p>This will build a multi-platform docker image supporting both <code>linux/amd64</code> and <code>linux/arm64</code> and push it to the local docker registry. The image name is:</p> <pre><code>localhost:5001/build/lava/&lt;RUNTIME&gt;:latest\n</code></pre> <p>e.g.</p> <pre><code>localhost:5001/build/lava/amzn2023-py3.11:latest\n</code></pre> <p>Note</p> <p>The builder images incorporate a Python installation built from source (for two different platforms) to ensure consistency. This can take a reasonable amount of time. Be patient.</p> <p>To see which builders are available in the local docker registry:</p> <pre><code>jindr images\n</code></pre> <p>Check the <code>etc/builders</code> directory for a list of supported foreign runtime types. The following are currently available:</p> Runtime Operating System Python Version Deprecated <code>amzn2023-py3.9</code> Amazon Linux 2023 3.9 Yes <code>amzn2023-py3.11</code> Amazon Linux 2023 3.11 <code>amzn2023-py3.12</code> Amazon Linux 2023 3.12 <code>amzn2023-py3.13</code> Amazon Linux 2023 3.13 <code>rocky9-py3.9</code> Rocky Linux 9 3.9 Yes"},{"location":"10-installation-operation.html#using-the-builder-image","title":"Using the builder image","text":"<p>To build the bundles for a foreign host, first build the appropriate builder image, then:</p> <pre><code>make pkg runtime=&lt;RUNTIME&gt; platform=&lt;PLATFORM&gt;\n# e.g.\nmake pkg runtime=amzn2023-py3.11 platform=linux/amd64\n</code></pre> <p>Once again, set <code>PIP_INDEX_URL</code> if an alternative PyPI server is being used:</p> <pre><code>PIP_INDEX_URL=... make pkg runtime=&lt;RUNTIME&gt; platform=&lt;PLATFORM&gt;\n\n# or ...\nexport PIP_INDEX_URL=...\nmake pkg runtime=&lt;RUNTIME&gt; platform=&lt;PLATFORM&gt;\n</code></pre> <p>The built worker bundles are placed in the <code>dist</code> directory as:</p> <pre><code>dist/pkg/&lt;OS&gt;/lava-&lt;LAVA-VERSION&gt;-&lt;OS&gt;-&lt;PYTHON-VERSION&gt;-&lt;ARCHITECTURE&gt;.tar.bz2\n</code></pre>"},{"location":"10-installation-operation.html#building-the-lava-lambda-function-code-bundles","title":"Building the Lava Lambda Function Code Bundles","text":"<p>The lava lambda functions are pure Python and so are not particularly sensitive to the build platform or runtime. Nevertheless, they are built inside a docker container derived from the <code>amzn2023-py3.13</code> builder image using the <code>linux/arm64</code> platform. This is close enough to the deployed environment.</p> <p>The build process is:</p> <pre><code>make lambda\n</code></pre> <p>The bundles will be placed in the directory <code>dist/lambda</code>.</p>"},{"location":"10-installation-operation.html#building-the-lava-docker-images","title":"Building the Lava Docker Images","text":"<p>Lava comes with a suite of docker images that are suitable for use as base images for docker jobs. The images can also be used to create containers that run the lava worker itself.</p> <p>Note</p> <p>As of version 8.1 (K\u012blauea), multi-platform (ARM and x86) images are built by default. Docker pull operations will automatically select the platform matching the client host unless overridden.</p> <p>To build the multi-platform images do the following. The images will be built and pushed to the local docker registry at <code>localhost:5001</code>.</p> <p><pre><code>cd docker\n\n# Get help -- lots of build options. Available O/S types will be listed.\nmake\n\n# Build all of the images. This will push the images to our local\n# docker registry at localhost:5001.\nmake build-all\n\n# Do a fast (and flimsy) health check\nmake check-all\n\n# ... or ... make a specific O/S (see docker/os directory)\nmake build os=&lt;OS&gt;\nmake check os=&lt;OS&gt;\n\n# Check the local registry to see the images are present\njindr images\n</code></pre> The images can be run locally in the normal way:</p> <pre><code># Run the image that matches the local machine architecture\ndocker run -it --rm localhost:5001/lava/amzn2023/base\n\n# Specify which platform we want. M-series Macs can run both arm64\n# and amd64 (under emulation)\ndocker run -it --rm --platorm linux/amd64 localhost:5001/lava/amzn2023/base\n</code></pre> <p>Prior to v8.2 (K\u012blauea), the standard lava build process only supported use of AWS ECR for deploying lava docker images. The images can now be deployed to ECR or a different registry.</p> <p>See Docker Images for Lava for publicly available docker images.</p> Deploying to ECRDeploying to another registry <p>Deploying the images to AWS ECR involves creating the target repositories and copying  the images from the local docker registry to ECR.</p> <p>First, login to AWS for command line access.</p> <p>Create the ECR target repositories, if not already present:</p> <pre><code>make ecr-repo os=...\n# ... or do all at once if none of them exist yet.\nmake ecr-repo-all\n</code></pre> <p>This will also apply a lifecycle policy to the repositories to only retain 4 images, and to remove untagged images after 1 day. If this doesn't suit, modify the policy in ECR after the repository has been created.</p> <p>Push images to ECR:</p> <pre><code>make ecr-push os=&lt;OS&gt;\n# ... or push all at once\nmake ecr-push-all\n</code></pre> <p>The repositories will be created with names of the form <code>dist/lava/&lt;OS&gt;/&lt;TYPE&gt;</code> (e.g. <code>dist/lava/amzn2023/base</code>). These can be changed if it's critical, but it's best not to. The IAM policies created by the supplied CloudFormation Templates assume this structure.</p> <p>If it's all too much to bear, the <code>dist</code> prefix can be overridden, thus:</p> <pre><code>make ecr-repo os=... prefix=...\nmake ecr-push os=... prefix=...\n</code></pre> <p>Additional, custom IAM policies will need to be attached to the lava IAM groups to provide users with access.</p> <p>Deploying the images to a docker registry involves creating the target  repositories (if required by the registry), and copying the images from the local docker registry to the target registry.</p> <p>Note</p> <p>The examples in this section assume Github Container Registry (ghcr.io) for an account named <code>my-github</code>. Update the examples as appropriate.</p> <p>First, login to the registry. The makefile will not do that for you.</p> <pre><code># Yes, I know, GHCR ignores the username but you get the idea.\ndocker login ghcr.io -u my-github\n</code></pre> <p>If the target registry requires creation of repositories prior to pushing images, that must be done manually. GHCR doesn't require this. Don't forget to create a repository for both <code>base</code> and <code>full</code> images.</p> <p>Repositories should be named like so:</p> <pre><code>&lt;PREFIX&gt;/lava/&lt;OS&gt;/base\n&lt;PREFIX&gt;/lava/&lt;OS&gt;/full\n</code></pre> <p>e.g. The public images are named like so:</p> <pre><code>ghcr.io/jin-gizmo/amzn2023/base\n---+--- ----+---- ----+--- -+--\n   |        |         |     |\nregistry    |         OS    |\n         prefix           type\n</code></pre> <p>Push the images:</p> <pre><code>make push registry=ghcr.io prefix=my-github\n</code></pre>"},{"location":"10-installation-operation.html#building-the-jinlava-python-package","title":"Building the jinlava Python Package","text":"<p>The jinlava package is a standard Python package that can be installed using pip. This is handy when developing lava jobs using an IDE such as PyCharm. Full API documentation is also available.</p> <p>The package includes the lava APIs and all of the CLI components and utilities.</p> <p>To build a source distribution of the jinlava package:</p> <pre><code>make jinlava\n</code></pre> <p>In Python programs, this is imported as <code>lava</code>:</p> <pre><code>import lava\n\nprint(lava.__version__)\n</code></pre>"},{"location":"10-installation-operation.html#building-the-lava-job-framework","title":"Building the Lava Job Framework","text":"<p>Note</p> <p>Pre-built versions of the lava job framework are (for now) provided as part of a release on GitHub. However, the preferred mechanism is to use the lava-new instead.</p> <p>The lava job framework can be built thus:</p> <pre><code># From the root of the lava code repo ...\nmake tools\n</code></pre> <p>The framework cookiecutter zipped bundle is placed in <code>dist/dev-tools</code>.</p>"},{"location":"10-installation-operation.html#building-the-lava-documentation","title":"Building the Lava Documentation","text":"<p>The lava user guide and API documentation can be built thus:</p> <pre><code># From the root of the lava code repo ...\nmake preview\n</code></pre> <p>This will create the user guide and open a browser window to preview it. Partial build artefacts are placed in <code>dist/doc</code>.</p> <p>When published, the user guide is hosted on GitHub Pages. The publishing process is simply:</p> <pre><code>make publish\n</code></pre> <p>If the user guide is modified, it must be spell checked:</p> <pre><code>make spell\n</code></pre> <p>Warning</p> <p>Failure to check spelling, or, worse, recklessly adding your orthographic ignorance to the custom dictionary will result in much wailing and gnashing of teeth.</p>"},{"location":"10-installation-operation.html#deploying-lava-components","title":"Deploying Lava Components","text":"<p>The lava code bundles can be deployed in any way that suits the target environment. However ...</p> <p>The mechanism that is supported by the lava distribution is to place them into AWS S3 where they can be found, either by the supplied realm CloudFormation templates in the case of the lambda functions, or the included deployment script and EC2 instance boot scripts in the case of the worker / dispatcher bundle.</p> <p>Note</p> <p>Note that the supplied CloudFormation template for a lava worker is dependent on the lava EC2 AMI because of the unusual way it uses instance user data to run boot scripts to deploy the worker. Sorry about that. It's a legacy thing but it works.</p>"},{"location":"10-installation-operation.html#lava-component-layout-in-s3","title":"Lava Component Layout in S3","text":"<p>See also: Deploying the Lava Components to S3.</p> <p>The components must be installed in AWS S3 in a layout like this:</p> <pre><code>s3://&lt;s3CodeBucket&gt;/&lt;s3CodePrefix&gt;/\n\u251c\u2500\u2500 _ami_\n\u2502   \u2514\u2500\u2500 oracle\n\u2502       \u251c\u2500\u2500 instantclient-basiclite-linux-arm64.zip\n\u2502       \u251c\u2500\u2500 instantclient-basiclite-linux-x64.zip\n\u2502       \u251c\u2500\u2500 instantclient-sqlplus-linux-arm64.zip\n\u2502       \u2514\u2500\u2500 instantclient-sqlplus-linux-x64.zip\n|\n\u251c\u2500\u2500 _boot_\n\u2502   \u251c\u2500\u2500 10-secupdate.sh\n\u2502   \u251c\u2500\u2500 50-lava.sh\n\u2502   \u2514\u2500\u2500 90-motd.sh\n|\n\u251c\u2500\u2500 _dist_\n\u2502   \u251c\u2500\u2500 cfn\n\u2502   \u2502   \u2514\u2500\u2500 v8.1.0\n\u2502   \u2502       \u251c\u2500\u2500 lava-common.cfn.json\n\u2502   \u2502       \u251c\u2500\u2500 lava-realm.cfn.json\n\u2502   \u2502       \u2514\u2500\u2500 lava-worker.cfn.json\n\u2502   \u251c\u2500\u2500 lambda\n\u2502   \u2502   \u251c\u2500\u2500 dispatch-8.1.0.zip\n\u2502   \u2502   \u251c\u2500\u2500 metrics-8.1.0.zip\n\u2502   \u2502   \u251c\u2500\u2500 s3trigger-8.1.0.zip\n\u2502   \u2502   \u2514\u2500\u2500 stop-8.1.0.zip\n\u2502   \u2514\u2500\u2500 pkg\n\u2502       \u2514\u2500\u2500 amzn2023\n\u2502           \u251c\u2500\u2500 lava-8.0.0-amzn2023-py3.11-x86_64.tar.bz2\n\u2502           \u251c\u2500\u2500 lava-8.1.0-amzn2023-py3.11-aarch64.tar.bz2\n\u2502           \u2514\u2500\u2500 lava-8.1.0-amzn2023-py3.11-x86_64.tar.bz2\n|\n\u251c\u2500\u2500 &lt;REALM-1&gt;\n\u2502   \u251c\u2500\u2500 &lt;WORKER-1A&gt;\n\u2502   \u2502   \u251c\u2500\u2500 root.boot.sh\n\u2502   \u2502   \u2514\u2500\u2500 root.boot0.sh\n\u2502   \u2514\u2500\u2500 &lt;WORKER-1B&gt;\n\u2502       \u251c\u2500\u2500 root.boot.sh\n\u2502       \u2514\u2500\u2500 root.boot0.sh\n|\n\u2514\u2500\u2500 &lt;REALM-2&gt;\n    \u2514\u2500\u2500 &lt;WORKER-2A&gt;\n        \u251c\u2500\u2500 root.boot.sh\n        \u2514\u2500\u2500 root.boot0.sh\n</code></pre> <p>Info</p> <p><code>s3CodeBucket</code> and <code>s3CodePrefix</code> are parameters in the realm CloudFormation template.</p> Prefix Description <code>_ami_</code> Used by the build process for the lava AMI. It contains large packages that are baked into the lava AMI as it's quicker to get them from S3 rather than upload them to the build instance each time. It is not used in the worker boot process. <code>_boot_</code> These scripts are downloaded and run during the worker EC2 boot process. The critical one for the lava worker is <code>50-lava.sh</code> which downloads the worker code bundle from S3 and installs it. The source for these scripts is <code>misc/boot/</code> in the lava repo. <code>_dist_</code> Contains the lava distribution. Multiple lava versions can safely sit side by side in here. <code>_dist_/cfn</code> Contains the CloudFormation templates and associated auto-generated documentation. <code>_dist_/lambda</code> Contains the code bundles for the Lambda functions. Version selection is controlled by a parameter in the realm CloudFormation template. <code>_dist_/pkg</code> Contains the main lava worker code bundle organised by target O/S, architecture and Python version. Multiple versions are allowed. See also Lava Version Selection. <code>&lt;REALM&gt;/&lt;WORKER&gt;</code> Contains the worker EC2 instance primary boot scripts. While these scripts are installed on a per worker basis they should generally be identical across all accounts, realms and workers. It is possible to have a worker specific setup if required. Versions of these are provided in the <code>misc</code> directory of the lava repo. The supplied versions basically just download and run the scripts in the <code>_boot_</code> area."},{"location":"10-installation-operation.html#deploying-the-lava-components-to-s3","title":"Deploying the Lava Components to S3","text":"<p>While not mandatory, lava assumes that most environments will need to deploy all of the built assets to AWS S3 where they can be accessed as required. This includes the worker and dispatcher code and lambda functions. This can be tedious to do manually so a basic deployment script is provided in <code>etc/deploy.sh</code>.</p> <p>It will read a YAML configuration file that tells it what to deploy and where to deploy it. It supports deployment of the following lava components (depending on the contents of the configuration file):</p> <ul> <li> <p>S3 artefacts in the structure illustrated above.</p> <ul> <li>lava code bundles (<code>_dist_/pkg/</code>)</li> <li>boot scriptlets (<code>_boot_/</code>)</li> <li>the lava Lambda functions (<code>_dist_/lambda/</code>)</li> </ul> </li> <li> <p>The lava job framework bundle to S3</p> </li> </ul> <p>Usage is:</p> <pre><code>Usage: deploy.sh -e environment [-f deploy.yaml ] [-d] [-h] [-n] [component ...]\n\nArgs:\n    -e environment      Target environment. The deploy.yaml file must contain a\n                        key for this.\n\n    -f deploy.yaml      Specify a YAML file containing S3 target locations. If\n                        not specified, the default is deploy.yaml in the current\n                        directory.\n\n    -d                  Dry-run.\n\n    -h                  Print help and exit.\n\n    -l                  List deployable components and exit.\n\n    -n                  Same as -d like make(1).\n\n    -v                  Deploy the specified lava version. If not specified,\n                        the latest version is deployed (currently 8.1.0).\n\n\n    component           Only deploy the named components. If not specified, all\n                        available components for which there is a a deployment\n                        on on the specification file will be deployed.\n</code></pre> <p>It should be run from the base directory of the repo. A sample deployment configuration file is provided as <code>deploy/sample.yaml</code>. Format information is contained in that file. The script is rather basic but will attempt to avoid copying files that aren't required in a given environment or that don't need updating.</p> <p>A configuration file may contain specifications for multiple environments. These are selected by the mandatory <code>-e environment</code> of <code>etc/deploy.sh</code>. To avoid accidents, the configuration for an environment must specify the target AWS account ID.</p> <p>Info</p> <p>The configuration file also contains parameters used to build the lava AMI. These are ignored by <code>etc/config.sh</code>.</p> <p>To list the possible deployable components run <code>etc/deploy.sh -l</code>. This will produce something like this:</p> <pre><code>\u276f etc/deploy.sh -l\nami        Components required to be in S3 to build the AMI (not the AMI itself).\nboot       Lava worker boot scriptlets (not root.boot*).\nframework  Lava job framework.\nlambda     Lambda function code bundles.\npkg        Main lava code package.\n</code></pre> <p>A configuration file may restrict this further by not including configuration information for some of these targets.</p> <p>To deploy everything that is configured for a given environment:</p> <pre><code># First do a dry run to see what would be deployed ...\netc/deploy.sh -n -f deploy/my-config.yaml -e dev\n\n# Now deploy for real ...\netc/deploy.sh -f deploy/my-config.yaml -e dev\n</code></pre> <p>Just deploy a specific component:</p> <pre><code># Deploy docs only\netc/deploy.sh -f deploy/my-config.yaml -e dev doc\n</code></pre>"},{"location":"10-installation-operation.html#lava-version-selection","title":"Lava Version Selection","text":""},{"location":"10-installation-operation.html#lambda-functions","title":"Lambda Functions","text":"<p>The <code>lambdaVersion</code> parameter in the realm CloudFormation template specifies the version to use for the lambda functions. To change versions, update the realm stack.</p> <p>The stack will look for the lambda code in <code>s3://&lt;s3CodeBucket&gt;/&lt;s3CodePrefix&gt;/_dist_/lambda/</code> where <code>s3CodeBucket</code> and <code>s3CodePrefix</code> are also stack parameters.</p>"},{"location":"10-installation-operation.html#lava-workers","title":"Lava Workers","text":"<p>The supplied <code>misc/boot/50-lava.sh</code> script is deployed into <code>s3://&lt;s3CodeBucket&gt;/&lt;s3CodePrefix&gt;/_boot_/</code> and runs when the EC2 worker instance boots.</p> <p>By default, the script will attempt to install the highest available version that matches the instance O/S type, Python version and architecture. However, a specific version can be selected by adding worker configuration entries to the realms table.</p> <p>The package selection sequence is this:</p> <ol> <li> <p>If a version is     specified in the realms table:</p> <ol> <li> <p>Look for a package with the new package naming style (i.e. version,     O/S, architecture and Python must match). If one is found, use that.</p> </li> <li> <p>Look for a package of the required version with the old naming     style. If one is found, use that.</p> </li> </ol> </li> <li> <p>If a version is not specified in the realms table:</p> <ol> <li> <p>Find the latest matching version using the new package naming     style. If one is found, use that.</p> </li> <li> <p>Find the latest version using the old package name style. If one     is found, use that.</p> </li> </ol> </li> <li> <p>Otherwise, give up and abort.</p> </li> </ol>"},{"location":"10-installation-operation.html#creating-a-lava-realm","title":"Creating a Lava Realm","text":"<p>A lava realm is created using the following process:</p> <ol> <li>Create the realms table, if it doesn't already     exist.</li> <li>Add an entry in the realms table     for the new realm.</li> <li>Use the realm CloudFormation template     to create the AWS resources for the realm.</li> </ol> <p>Once the realm is configured, there may be a few final items to perform, such as</p> <ul> <li> <p>Add a resource policy to the lava realm S3 bucket, if required.</p> </li> <li> <p>Add subscriptions to the realm SNS topic for event notifications.</p> </li> <li> <p>Configure connections and jobs for the realm.</p> </li> </ul>"},{"location":"10-installation-operation.html#creating-the-realms-table","title":"Creating the Realms Table","text":"<p>There is a single realms table per AWS account. While it can be created by the CloudFormation template that creates an individual realm, this is not recommended as it means the resulting stack cannot be deleted without impacting other lava realms.</p> <p>The recommended way to create the realms table is using the following command:</p> <pre><code># Create the realms table. ReadCapacityUnits can be adjusted now or once the\n# table is created. WriteCapacityUnits does not need to be changed.\n\naws dynamodb create-table --table-name lava.realms \\\n    --key-schema AttributeName=realm,KeyType=HASH \\\n    --attribute-definitions AttributeName=realm,AttributeType=S \\\n    --provisioned-throughput ReadCapacityUnits=3,WriteCapacityUnits=1\n</code></pre>"},{"location":"10-installation-operation.html#adding-a-new-entry-to-the-realms-table","title":"Adding a New Entry to the Realms Table","text":"<p>Each realm requires an entry in the realms table. This will look something like the following: (replace <code>&lt;..&gt;</code> with the appropriate value).</p> <pre><code>{\n  \"on_fail\": [\n    {\n      \"action\": \"sns\",\n      \"message\": \"Job {{job.job_id}}@{{realm.realm}} ({{job.run_id}}) failed: {{result.error}}\",\n      \"topic\": \"arn:aws:sns:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:lava-&lt;REALM&gt;-notices\"\n    }\n  ],\n  \"realm\": \"&lt;REALM&gt;\",\n  \"s3_key\": \"alias/lava-&lt;REALM&gt;-user\",\n  \"s3_payloads\": \"s3://&lt;lavaBucketName&gt;/payloads\",\n  \"s3_temp\": \"s3://&lt;lavaBucketName&gt;/tmp\"\n}\n</code></pre>"},{"location":"10-installation-operation.html#the-lava-realm-cloudformation-template","title":"The Lava Realm CloudFormation Template","text":"<p>Note</p> <p>Pre-built versions of the CloudFormation templates are provided as part of a release on GitHub.</p> <p>The realm CloudFormation template is located in the lava repo. See Building the CloudFormation Templates.</p> <p>If the realm is going to host any workers, the  lava lambda function code bundles must also have been built and deployed to S3.</p> <p>Note</p> <p>It is possible, and sometimes useful, to create a bare realm that will not contain any workers. In this situation, the realm consists only of the DynamoDB tables and a few other static components. A bare realm is useful where an external application needs to use the lava connector subsystem via the jinlava API but is not necessarily being orchestrated to run on a lava worker.</p> <p>The template incorporates the following resources:</p> <ul> <li>The realm specific DynamoDB tables.</li> <li>The S3 bucket for the realm that will contain job payloads and outputs.</li> <li>Some KMS keys for the realm.</li> <li>An SNS topic for notifications relating to the realm.</li> <li>Lambda functions for the S3 job trigger and dispatch helper and associated     IAM roles.</li> <li>Realm specific IAM components.</li> </ul> <p>Template parameters are described below.</p> Parameter Required Description Version Yes This is a read-only informational parameter that indicates the lava version associated with the template. autoscalingHeartbeatMinutes Yes Send auto scaling heartbeats at this frequency when workers are terminating. This tells the auto scaler that the worker is still busy finishing in-flight jobs and to give it more time before forced termination. The upper limit on the auto scaler's patience is set by the <code>workerStopMinutes</code> parameter. createRealmsTable Yes If <code>yes</code>, the realms table will be created. It's much safer to leave this set to <code>no</code> and create the realms table separately. kmsKeyAdmin Yes The IAM user name of KMS key administrator. This gets added into the resource policy of the realm's KMS keys. lambdaMetricsSchedule Yes If set to <code>ENABLED</code>, an AWS EventBridge rule fires the <code>lava-&lt;REALM&gt;-metrics</code> lambda once a minute. This lambda calculates the worker backlog metric used to manage lava worker auto scaling. lambdaTimeout Yes Timeout for the lambdas (seconds). Ignored if the lambdas are not deployed. lambdaVersion No This parameter selects which code version is deployed for the Lambda functions. If left empty, the Lambda functions are not deployed. lavaBucketName Yes The name of the bucket where the new realm will store job payloads and outputs. The template will create this bucket. logBucketName Yes Name of S3 bucket for S3 logs. readCapacityDataTables Yes Read capacity for the Dynamo DB data tables. This excludes the events table and the state table. readCapacityEventTable Yes Read capacity for the events table. readCapacityStateTable Yes Read capacity for the state table. realm Yes Realm name. s3CodeBucket Yes The name of the bucket containing the lava code bundles. s3CodePrefix Yes The prefix in the code bucket containing the lava code bundles. tmpExpiryDays Yes Expire files in the temp area of the lava bucket after this many days. workerStopMinutes Yes Allow workers this many minutes to stop gracefully. When a worker is instructed to shut down by its owning auto scaling group, it is given the specified amount of time to finish any in-flight jobs before the auto scaler forcibly terminates it. This value can be as high as 720 minutes (12 hours). The value should be selected in conjunction with the worker SQS queue visibility timeout. writeCapacityDataTables Yes Write capacity for the Dynamo DB data tables. This excludes the events table and the state table. writeCapacityEventTable Yes Write capacity for the events table. writeCapacityStateTable Yes Write capacity for the state table."},{"location":"10-installation-operation.html#creating-a-lava-worker","title":"Creating a Lava Worker","text":"<p>Info</p> <p>It is important to read all of this section before starting to create a worker.</p> <p>Each realm can have an arbitrary number of workers of different capabilities to run jobs.</p> <p>Workers can run on Linux / macOS with Python 3.9+ installed.</p> <p>There are different ways to run the lava worker. The worker can be run interactively or as a multi-threaded daemon. It is possible to run multiple workers on a single instance. </p> <p>The first worker created will typically also serve as a dispatcher for scheduled jobs. Dispatcher workers must each run in their own operating system account to ensure crontab separation.</p> <p>Creating a worker involves two distinct aspects:</p> <ol> <li> <p>Use the     lava worker CloudFormation template     to create the AWS resources required by the worker (e.g. SQS queue, EC2     components etc).</p> </li> <li> <p>Create a compute environment to run the actual lava worker. The most common     options for doing this are:</p> <ol> <li>Desktop workers</li> <li>Docker lava workers</li> <li>EC2 based workers</li> <li>Parasitic workers.</li> </ol> </li> </ol>"},{"location":"10-installation-operation.html#the-lava-worker-cloudformation-template","title":"The Lava Worker CloudFormation Template","text":"<p>Note</p> <p>Pre-built versions of the CloudFormation templates are provided as part of a release on GitHub.</p> <p>The worker CloudFormation template is located in the lava repo. See Building the CloudFormation Templates.</p> <p>The lava worker CloudFormation template is a single entity but it contains parameters (and associated resources) in three broad groups:</p> <ol> <li> <p>Core worker parameters: These are necessary for     any lava worker, unrelated to where, or whether, the worker process is     running. e.g. the worker SQS queue, queue depth alarm etc.</p> </li> <li> <p>EC2 worker parameters: These parameters are     associated with the operating resources needed for an AWS EC2 instance     running the worker process. e.g. launch configuration, auto scaling group,     IAM role for the worker etc.</p> </li> <li> <p>EC2 instance selection parameters:     These parameters control the way in which the EC2 instance type is chosen.</p> </li> </ol> <p>These are all parameters of the same template but are described separately below for clarity.</p>"},{"location":"10-installation-operation.html#core-worker-parameters","title":"Core Worker Parameters","text":"<p>The core worker parameters and associated resources are necessary for a lava worker to exist, whether or not there is an actual worker process anywhere or where it is located.</p> <p>In broad terms, these relate to resources that are not EC2 related.</p> Parameter Description Version This is a read-only informational parameter that indicates the lava version associated with the template. alarmTopic Name of an SNS topic for alarms. The topic must already exist. autoscalingControlledTermination If <code>ENABLED</code>, auto scaling controlled termination on worker nodes is enabled. Best not to fiddle with this. maxAllowedQueueDepth Create a CloudWatch alarm if queue depth exceeds this value. Set to 0 for no alarm. messageRetentionPeriod Message retention period on the worker SQS queue in seconds. Default 1 day. Must be &gt;= 1800 (30 minutes). queueDepthMinutes Minutes worker SQS queue depth exceeds <code>maxAllowedQueueDepth</code> before alarming. realm Name of the realm. realmLambdasDeployed Are the realm lambda functions deployed? This should almost always be set to <code>Yes</code>. visibilityTimeout Visibility timeout on the worker queue (seconds). Default 1 hour. worker Name of the worker."},{"location":"10-installation-operation.html#ec2-worker-parameters","title":"EC2 Worker Parameters","text":"<p>These parameters control creation of resource required to run an EC2 based lava worker. They are triggered by setting the <code>createWorkerInstance</code> parameter to <code>Yes</code>.</p> Parameter Description amiId AWS EC2 Image ID for the worker EC2 instance. This should be the ID for a lava AMI. Can be left blank if <code>createWorkerInstance</code> is <code>No</code>. autoscalingActionTopic Name of an SNS topic for reporting normal auto scaling activity. If required, the topic must already exist. createHeartBeatAlarm If <code>Yes</code> a CloudWatch alarm will be created for a loss of heartbeat messages from a worker. Set to <code>No</code> for non-EC2 based workers. createWorkerInstance If <code>Yes</code>, the required components for an EC2 based worker will be created. Set to <code>No</code> if the worker will be running on an existing compute environment (e.g. local machine or parasitic on an existing worker). dockerVolumeSize If non-zero (GB) a dedicated EBS volume for docker will be added to an EC2 based worker. This is a good idea if the worker will be running docker jobs to isolate large storage needs for docker images. keyPairName SSH key pair to assign to EC2 based instances. If left empty, no SSH key is assigned. rootVolumeSize Size in GB of the root volume on an EC2 based instance. Set to 0 for the default value associated with the AMI. This would typically be increased if not creating a dedicated temporary volume or if swapping is enabled. See also <code>tmpVolumeSize</code> and <code>swapSize</code>. secGroups A list of (existing) EC2 security groups to attach to an EC2 based instance. subnets A list of (existing) VPC subnets in which EC2 based instances can be placed. swapSize Swap size in Gibibytes (0 = no swapping). This may require a bigger root volume. See <code>rootVolumeSize</code>. tmpVolumeSize Size in GB of a dedicated EBS volume mounted on <code>/tmp</code> for EC2 based instances. Set to 0 to remove. If the worker is going to be processing disk I/O bound jobs, a larger tmp volume may help due to increased provisioned IOPS. workerBacklogScalingTarget Auto scaling worker backlog. Set to 0 to disable backlog scaling. See Lava Worker Auto Scaling. workerInstancesDesired The number of desired EC2 based instances. This is used by the auto scaling group. workerInstancesMax The maximum number of EC2 based instances the auto scaling group will provision. workerInstancesMin The minimum number of EC2 based instances the auto scaling group will provision. workerPublicIp Assign a public IP to the worker."},{"location":"10-installation-operation.html#ec2-instance-selection-parameters","title":"EC2 Instance Selection Parameters","text":"<p>The EC2 Worker Parameters described above specify how to create an EC2 based lava worker but they do not specify what instance type to use. This is done by the parameters specified below.</p> <p>Two different methods of instance type selection are supported:</p> <ol> <li> <p>Explicit instance type provisioning: This is triggered by setting the     <code>workerInstanceType</code> parameter to a single EC2 instance type (e.g.     <code>m3.large</code>).    </p> <p>The required instance type will be specified in the worker launch template. The advantage of this approach is that it is simple and direct. The disadvantage is that, if AWS has a capacity shortage for the specified instance type, the auto scaler may not be able to create a worker or fully satisfy the auto scaler's demands. The risk of this seems to increase for the larger instance types.</p> </li> <li> <p>Capability based provisioning: If <code>workerInstanceType</code> is not set, the     other parameters are used to indicate the capabilities required of the     worker instance.</p> <p>These parameters control the inclusion of a mixed instances policy in the auto scaling group definition. No instance type information is included in the launch template.  This gives the auto scaler the ability to choose any instance type that meets the capability requirements. Lava workers don't much care about instance type. They mostly care about memory, CPU and storage.</p> <p>The advantage of this approach is increased resilience in the face of AWS EC2 capacity shortages.</p> </li> </ol> Parameter Description workerAllowedInstances Comma separated list of allowed instance types (GLOBs allowed) for EC2 based workers using capability based provisioning. e.g. <code>*</code> Would allow any instance type. <code>m3.*,c3.*</code> would allow any M-series or C-series instance type that meets memory and CPU requirements. A value must be specified if <code>workerInstanceType</code> is not set. workerBurstable Include burstable instance types (e.g. T-series) for EC2 based workers using capability based provisioning. workerInstanceType If set to an AWS instance type, an EC2 based instance will use the specified type. If not set, capability provisioning will be used to select an EC2 instance type. workerLocalStorage Controls inclusion of EC2 instance types with local storage when performing capability based provisioning. workerMemoryMax Maximum memory in MiB (not MB) for EC2 instance types when performing capability based provisioning. Set to 0 to remove the upper limit. workerMemoryMin Minimum memory in MiB (not MB) for EC2 instance types when performing capability based provisioning. workerVCpuMax Maximum number of vCPUs for EC2 instance types when performing capability based provisioning. Set to 0 to remove the upper limit. workerVCpuMin Minimum number of vCPUs for EC2 instance types when performing capability based provisioning."},{"location":"10-installation-operation.html#desktop-lava-workers","title":"Desktop Lava Workers","text":"<p>Once the core worker components are created for a lava worker, it is straightforward to run the lava worker on a macOS or Linux desktop, provided the user has the appropriate IAM permissions.</p> <p>The setup will look something like this (not including installing non-lava prerequisites):</p> <pre><code># Create a virtual env to be safe\nmkdir lavaworker\ncd lavaworker\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install\npip3 install jinlava\n\n# Get help\nlava-worker --help\n\n# Run a lava worker interactively for development / debugging\nlava-worker --realm &lt;REALM&gt; --worker &lt;WORKER&gt; --level debug --dev\n</code></pre>"},{"location":"10-installation-operation.html#docker-based-lava-workers","title":"Docker Based Lava Workers","text":"<p>A straightforward way to run a lava worker on a local machine is to use one of the pre-built lava images. This will include all of the non-lava prerequisites and all of the lava utilities, include the lava worker.</p> <pre><code># First, log in to ECR then ...\ndocker pull \\\n    \"&lt;ACCOUNT_NO&gt;.dkr.ecr.ap-southeast-2.amazonaws.com/dist/lava/amzn2023/base\"\n\n# Run the container\ndocker run -it --rm \\\n    \"&lt;ACCOUNT_NO&gt;.dkr.ecr.ap-southeast-2.amazonaws.com/dist/lava/amzn2023/base\"\n\n# Get help\nlava-worker --help\n\n# Setup AWS profile then ... \n\n# Run a lava worker interactively for development / debugging\nlava-worker --realm &lt;REALM&gt; --worker &lt;WORKER&gt; --level debug --dev\n</code></pre>"},{"location":"10-installation-operation.html#ec2-based-lava-workers","title":"EC2 Based Lava Workers","text":"<p>Note</p> <p>This is the standard shared use and production configuration for a lava worker.</p> <p>Info</p> <p>The following process assumes the lava deployment artefacts and the lava EC2 AMI have already been built and deployed to S3. Ensure that matching Python versions and platform architectures are selected throughout for any given worker.</p> <p>The preferred deployment configuration for a production or shared-use lava worker is to use the lava worker CloudFormation template to create EC2 instances, based on the lava AMI, running the lava worker code bundle.</p> <p>The setup process is:</p> <ol> <li> <p>Deploy the Lava Components to S3     if not already present. These will be read by the worker EC2 at boot time.</p> </li> <li> <p>Install the worker boot scripts.     These will be read by the worker EC2 at boot time.</p> </li> <li> <p>Add    worker configuration entries    to the realms table to specify the run-time    configuration of the worker.</p> </li> <li> <p>Build the worker CloudFormation stack     with these settings:</p> <ul> <li><code>createWorkerInstance</code>=<code>Yes</code> </li> <li><code>workerInstancesDesired</code>=<code>0</code></li> <li><code>createHeartBeatAlarm</code>=<code>No</code></li> </ul> <p>If the <code>createWorkerInstance</code> parameter is set to <code>Yes</code> when deploying the lava worker CloudFormation template, additional resources are created to host a lava worker on an EC2 instance (e.g. worker IAM role, launch template, auto scaling group etc). Setting <code>workerInstancesDesired</code> to <code>0</code> still creates these components but no active EC2 instances.</p> </li> <li> <p>Update the worker IAM role if required.</p> <p>The previous step will create the required EC2 resources for the worker  but not start an EC2 instance. It will also create an IAM role <code>lava-&lt;REALM&gt;-worker-&lt;WORKER&gt;</code> . This will contain the core permissions to allow a worker to function. It may be necessary to add other, environment specific policies to this before starting an actual EC2 instance.</p> </li> <li> <p>If the worker is going to be a dispatcher, create a lavasched     job in the realm jobs table. The worker jump-start process will     automatically build a crontab schedule when the EC2 instance boots.</p> </li> <li> <p>Update the worker CloudFormation stack     with these changes:</p> <ul> <li><code>workerInstancesDesired</code>=<code>1</code></li> <li><code>createHeartBeatAlarm</code>=<code>Yes</code></li> </ul> </li> </ol> <p>An EC2 instance will be created that can be accessed via SSM session manager. It can take a few minutes for lava to install and start. The configuration is as shown below:</p> <p></p> <p>The start up process for the worker is:</p> <ol> <li> <p>The CloudFormation template configures the the worker auto scaling group to     start one or more instances.</p> </li> <li> <p>The worker auto scaling group starts an EC2 instance using the     lava AMI as a base.</p> </li> <li> <p>The EC2 instance reads boot scripts     from S3 and runs them.</p> </li> <li> <p>The boot scripts read     worker configuration     from the realms table (lava version, how many daemons to run etc.)</p> </li> <li> <p>The boot scripts read the lava code bundle from S3, install it, and configure      and start the lava worker daemons.</p> </li> </ol>"},{"location":"10-installation-operation.html#adding-worker-configuration-entries-to-the-realms-table","title":"Adding Worker Configuration Entries to the Realms Table","text":"<p>By default, the EC2-based worker boot process will attempt to install the highest available lava version that matches the host Python version and hardware architecture. It will also start a single lava worker daemon.</p> <p>The realms table entry can contain parameters under the <code>x-workers.&lt;WORKER&gt;</code> key that modify the behaviour of this process as follows. Note that a <code>&lt;WORKER&gt;</code> value of <code>_</code> in the realms table can provide defaults for all workers where a worker specific value is not provided.</p> Key Type Description daemons Integer The number of worker daemons to run. The default is 1. env Map A map of variables that will be placed in the worker's environment. threads Integer Number of threads to run per worker daemon. version String Lava version to install (e.g. 8.1.0). workers String A space separated list of workers to run. This is used for parasitic workers. <pre><code>{\n  \"realm\": \"prod01\",\n  \"x-workers\": {\n    \"core\": {\n      \"threads\": 6,\n      \"version\": \"8.1.0\"\n    },\n    \"_\": {\n      \"env\": {\n        \"AWS_MAX_ATTEMPTS\": 10,\n        \"AWS_METADATA_SERVICE_NUM_ATTEMPTS\": 10,\n        \"AWS_RETRY_MODE\": \"adaptive\",\n\n      }\n    }\n  }\n}\n</code></pre> <p>This will cause the instance to install lava version 8.1.0 on the machine and start a worker <code>lava-prod01-core</code> running 6 threads. The worker will have the three <code>AWS_*</code> environment variables set.</p>"},{"location":"10-installation-operation.html#installing-the-worker-boot-scripts","title":"Installing the Worker Boot Scripts","text":"<p>The worker boot scripts, <code>root.boot0.sh</code> and <code>root.boot.sh</code> configure the EC2 instance and install lava. The user data in the launch template will tell an instance created from a lava AMI to run the boot scripts when the worker instance boots. It will also provide the S3 location to obtain the code.</p> <p>The lava repository contains versions of these boot scripts in the <code>misc</code> directory. They can be used as is or tailored as required.</p> <p>These scripts must be placed in the same area of S3 as the lava code in sub-prefixes specifying the realm and worker names as shown in Lava Component Layout in S3.</p> <p>The supplied version of <code>root.boot.sh</code> basically just downloads and runs the boot scripts from the <code>_boot_/</code> prefix in the S3 deployment area.</p>"},{"location":"10-installation-operation.html#parasitic-lava-workers","title":"Parasitic Lava Workers","text":"<p>It is possible for multiple lava workers to share a single EC2 instance. In this case, exactly one of the workers will have an associated EC2 instance and the other parasitic workers will hitch a ride on that.</p> <p>The host worker is created first as described above.</p> <p>This is the process to add a parasitic worker:</p> <ol> <li> <p>Use the lava worker CloudFormation template     to create an additional worker with <code>createWorkerInstance</code>=<code>No</code>. </p> </li> <li> <p>Update the realm entry in the realms table to configure     the extra worker to run on the same host as the existing worker (see below).</p> </li> <li> <p>If the parasitic worker is going to be a dispatcher, create a     lavasched job in the realm jobs table. The worker     jump-start process will automatically build a crontab schedule when it the     EC2 instance boots.</p> </li> <li> <p>Reboot or replace the EC2 instance. Both workers should now start on the one     host.</p> </li> </ol> <p>The following example shows the changes in the realms table to add a <code>core-bulk</code> worker to the primary <code>core</code> worker.</p> <pre><code>{\n  \"realm\": \"prod01\",\n  \"x-workers\": {\n    \"core\": {\n      \"threads\": 6,\n      \"version\": \"8.1.0\",\n      \"workers\": \"core core-bulk\"\n    },\n    \"core-bulk\": {\n      \"daemons\": 2,\n      \"threads\": 8,\n      \"version\": \"8.1.0\"\n    },\n    \"_\": {\n      \"env\": {\n        \"AWS_MAX_ATTEMPTS\": 10,\n        \"AWS_METADATA_SERVICE_NUM_ATTEMPTS\": 10,\n        \"AWS_RETRY_MODE\": \"adaptive\"\n      }\n    }\n  }\n}\n</code></pre> <p>Note the addition of <code>\"workers\": \"core core-bulk\"</code> entry in <code>workers -&gt; core</code>. This tells the worker boot script to run an extra worker.</p> <p>The <code>\"core-bulk\": { ... }</code> section specifies the configuration of this extra (parasitic) worker.</p> <p>Points to note:</p> <ol> <li>The workers can, if required, run different versions of lava.</li> <li>The <code>\"daemons\": 2</code>element will start 2 workers named <code>lava-prod01-core-bulk</code>,     each running 8 threads.</li> <li>One worker EC2 host cannot run workers belonging to different realms.</li> </ol> <p>Its important to be aware of the following when running multiple worker daemons with the same name on the same machine:</p> <ul> <li>This configuration is experimental.</li> <li>They will all service the same SQS job queue.</li> <li>They will all emit heartbeat messages under the same name.</li> <li>They will all produce worker CloudWatch metrics for the same metrics, if that     configuration option is enabled. See Worker Metrics.</li> </ul>"},{"location":"10-installation-operation.html#installing-lava-locally","title":"Installing Lava Locally","text":"<p>Lava can be installed locally on machines with an operating system (Linux / macOS).</p> <p>There are two ways to do this:</p> <ol> <li>Local installation via pip</li> <li>Local installation via a lava install package.</li> </ol>"},{"location":"10-installation-operation.html#local-installation-via-pip","title":"Local Installation via pip","text":"<p>The lava package is available as a standard Python package that can be installed using pip. This is handy when developing lava jobs using an IDE such as PyCharm. Full API documentation is also available. </p> <pre><code>pip install jinlava\n</code></pre> <p>Warning</p> <p>Be aware that there is another, unrelated, lava package out there. Just running <code>pip install lava</code> will do the wrong thing so the lava package is bundled as <code>jinlava</code>. Python imports still use <code>import lava</code>. If you need to use both this lava and the other one, you must have a most unusual use case.</p> <p>Some optional modules are available as extras. The following modules are in the extras:</p> <ul> <li> <p>PyGreSQL (pgdb)</p> </li> <li> <p>AWS Redshift Driver</p> </li> <li> <p>Pyodbc</p> </li> </ul> <p>To install everything, including the optional extras:</p> <pre><code>pip install \"jinlava[extras]\"\n</code></pre>"},{"location":"10-installation-operation.html#local-installation-via-a-lava-install-package","title":"Local Installation via a Lava Install Package","text":"<p>First, obtain (or build) the code bundle for the target O/S and machine architecture. This will include the worker and all of the lava utilities.</p> <p>Info</p> <p>For macOS, the bundle name will look something like   <code>lava-8.0.0-darwin24-py3.11-arm64.tar.bz2</code>.</p> <p>The installation process is then:</p> <pre><code># Package name depends on lava version, O/S, Python version\n# and platform architecture.\nPKG=lava-8.0.0-darwin24-py3.11-arm64.tar.bz2\n\n# Extract the installer from the pkg.\ntar xf $PKG install.sh\n\n# Get help on the installer\n./install.sh\n\n# Do a clean install in the default location (/usr/local)\n./install.sh -c $PKG\n\n# See if it works. Make sure /usr/local/bin is in your path.\nlava-version\n</code></pre>"},{"location":"10-installation-operation.html#lava-iam-components","title":"Lava IAM Components","text":"<p>The lava CloudFormation templates will create a base set of IAM components, both for the lava workers and for users needing to interact with the lava environment.</p> <p>These components are described below.</p>"},{"location":"10-installation-operation.html#lava-iam-roles","title":"Lava IAM Roles","text":"Role Description <code>lava-&lt;REALM&gt;-dispatch-lambda</code> Service role for the dispatch helper Lambda function. <code>lava-&lt;REALM&gt;-metrics-lambda</code> Service role for the Lambda function that calculates worker backlog metrics used for lava worker auto scaling. <code>lava-&lt;REALM&gt;-s3trigger</code> Service role for the Lambda function that dispatches jobs from S3 bucket notification events. <code>lava-&lt;REALM&gt;-stop-lambda</code> Service role for the Lambda function that signals EC2-based lava workers to shutdown as part of an auto scaler scale-in process. <code>lava-&lt;REALM&gt;-worker-&lt;WORKER&gt;</code> IAM role for EC2-based workers. It will have the <code>lava-&lt;REALM&gt;-worker</code> policy attached."},{"location":"10-installation-operation.html#lava-iam-policies","title":"Lava IAM Policies","text":"Policy Description <code>lava-&lt;REALM&gt;-worker</code> Base permissions required for a worker node to function within a lava realm. <code>lava-&lt;REALM&gt;-admin</code> Permissions for an administrator for the realm. <code>lava-&lt;REALM&gt;-reader</code> Read-only access to key resources in the realm. <code>lava-&lt;REALM&gt;-operator</code> Additional permissions to that of <code>lava-&lt;REALM&gt;-reader</code>."},{"location":"10-installation-operation.html#lava-iam-groups","title":"Lava IAM Groups","text":"<p>IAM groups using the policies described above are also provided:</p> Group Name Policies <code>lava-&lt;REALM&gt;-admin</code> <code>lava-&lt;REALM&gt;-admin</code> <code>lava-&lt;REALM&gt;-reader</code> <code>lava-&lt;REALM&gt;-reader</code> <code>lava-&lt;REALM&gt;-operator</code> <code>lava-&lt;REALM&gt;-reader</code>, <code>lava-&lt;REALM&gt;-operator</code> <p>The groups have the following permissions, which are restricted to the lava realm wherever possible:</p> Resource Reader Operator Admin Lava bucket R R RW DynamoDB tables (realm specific) R R RW DynamoDB <code>realms</code> table R R R KMS key <code>user</code> Usage Usage Usage KMS key <code>sys</code> Usage Lava distro docker images in ECR R R R Payload docker images in ECR R R RW EventBridge rules R R RW Lava worker logs in CloudWatch R R Dispatcher SNS Topic Publish Publish Lava Worker SQS Queues R RW SSM Parameters RW Secrets Manager RW <p>Note</p> <p>The summary above is an approximation. Refer to the actual underlying IAM policies for specifics.</p>"},{"location":"10-installation-operation.html#the-lava-gui","title":"The Lava GUI","text":"<p>Contemporaneous with version 8.1 (K\u012blauea), a new version of the desktop lava GUI has been developed in Flet. The new GUI is a lot better than the original (which is not high praise, but it really is heaps better). The new GUI is now a separate project from the lava core.</p> <p>Lava GUI on GitHub</p> <p>The new GUI supports macOS and Windows.</p>"},{"location":"10-installation-operation.html#lava-worker-configuration","title":"Lava Worker Configuration","text":"<p>Lava worker configuration can be modified by setting a number of configuration  variables. For a configuration variable named <code>xyz</code>, lava will use the first value in the following sequence:</p> <ol> <li>An environment variable named <code>LAVA_XYZ</code></li> <li>An <code>XYZ</code> entry under the <code>config</code> map of the     realms table</li> <li>A default value as described below.</li> </ol> <p>Info</p> <p>The environment and realms table are only read when the worker starts. Changing configuration variables requires the worker to be restarted. Sorry.</p> <p>In the following, a Duration is a string in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is <code>s</code> (seconds), <code>m</code> (minutes) or <code>h</code> (hours), <code>d</code> (days) or <code>w</code> (weeks).</p> <p>A Size is a string in the form <code>nnX</code> where <code>nn</code> is a number and <code>X</code> is a case-sensitive unit specifier:</p> <ul> <li><code>B</code>: Bytes (default if no unit specified)</li> <li><code>K</code>, <code>KB</code>: Kilobytes (1000)</li> <li><code>M</code>, <code>MB</code>: Megabytes</li> <li><code>G</code>, <code>GB</code>: Gigabytes</li> <li><code>T</code>, <code>TB</code>: Terabytes</li> <li><code>P</code>, <code>PB</code>: Petabytes.</li> <li><code>KiB</code>: Kibibytes (1024)</li> <li><code>MiB</code>: Mebibytes</li> <li><code>GiB</code>: Gibibytes</li> <li><code>TiB</code>: Tebibytes</li> <li><code>PiB</code>: Pebibytes.</li> </ul> <p>A boolean is a case-insensitive string in the form:</p> <ul> <li><code>true</code>, <code>t</code>, <code>yes</code>, <code>y</code> or a non-zero integer (True)</li> <li><code>false</code>, <code>f</code>, <code>no</code>, <code>n</code> or zero (False).</li> </ul>"},{"location":"10-installation-operation.html#general-configuration-parameters","title":"General Configuration Parameters","text":"Name Type Default Description CHECK_FOR_ZOMBIES Boolean True When the worker exits, it waits for all the job threads to complete. On rare occasions, a thread may hang and delay the exit unnecessarily. If this parameter is set to True, an additional check is made a few times to see if there are any worker temporary directories still present. If there are none, then it's assumed the worker can safely exit. CONN_APP_NAME String <code>lv-{{realm}}-{{job_id}}</code> A Jinja template used to generate database client connection identifiers. It is rendered with the realm name, job ID and connection ID. See Database Client Application Identification. CW_METRICS_JOB Boolean False If True, the worker will generate CloudWatch custom metric data for all jobs. Can be overridden (to enable or disable) at the job level as described below. CW_METRICS_PERIOD Duration <code>1m</code> Period for generation of CloudWatch custom metric data for the worker itself. There is no point in setting this to anything less than 1 minute. CW_METRICS_WORKER Boolean False If True, the worker will generate CloudWatch custom metric data for the worker itself as described below. CW_NAMESPACE String <code>lava</code> The namespace used for CloudWatch custom metrics generated by the worker. DEBUG Boolean False If True, some additional information is placed in the events table for failed jobs. DISPATCHER String --&gt; Fully qualified name for the lava dispatcher executable. Defaults to <code>/usr/local/bin/lava-dispatcher</code>. EVENT_TTL Duration <code>2d</code> Time to live for records in the DynamoDB events table. HEARTBEAT_FILE String <code>.heartbeat</code> File name (relative to <code>TMPDIR</code>) that will be touched once every cycle of the worker heartbeat. This is essentially a keep-alive on <code>TMPDIR</code> so some O/S cleanup process doesn't blow it away due to inactivity. As this is part of the heartbeat process, it is only effective if the worker is running using the <code>-b</code> / <code>--heartbeat</code> option. ITERATION_MAX_DELAY Duration <code>5m</code> Maximum allowed delay when rerunning a failed job. ITERATION_MAX_LIMIT Duration 10 Maximum allowed number of run attempts for a job. JOB_LOCAL_TMPDIR Boolean True If True, the <code>TMPDIR</code> environment variable will be set for cmd, exe and pkg jobs to point into the private temporary run area for each job. If False, the system default value of <code>TMPDIR</code> is used. JUMPSTART_DELAY Duration <code>15s</code> Delay the initial scheduler jump-start process to allow the worker to fully initialise. LOGLEVEL String <code>info</code> The default logging level. PARAM_CACHE_SIZE Integer <code>100</code> Size of the cache for SSM parameters. Must be &gt; 0. PARAM_CACHE_TTL Duration <code>2m</code> SSM parameter values are only cached for the specified duration. PAYLOAD_DOWNLOADER String <code>v2</code> Selects either payload downloader version. Allowed values are <code>v1</code> (deprecated) or <code>v2</code>. See Job Payloads for more information. PAYLOAD_SETTLING_TIME Integer 1 Wait this many seconds before attempting to use a payload downloaded from S3. This helps avoid the occasional text file busy error. If this error is appearing, try setting this to 1 or 2 seconds. (Yes, it's a kludge. Sue me.) SES_CONFIGURATION_SET String None SES Configuration Set name for SES email sent by lava. SES_FROM String None Source email address for SES email sent by lava. SES_REGION String <code>us-east-1</code> AWS region for the Simple Email Service (SES). SQS_MAX_DELAY_MINS Integer <code>15</code> Maximum allowed delay for an SQS message in minutes. This is an AWS constraint. STATE_MAX_TTL Duration <code>366d</code> The maximum allowed time-to-live duration for an item in the state table. STATE_TTL Duration <code>7d</code> The default time-to-live duration for an item in the state table. STDERR_SIZE Size <code>1024</code> The number of bytes read from stderr when running jobs to include in the DynamoDB events table. The full contents of stderr are saved in S3. If &gt; 0, data is read from the start of the file. If &lt; 0, data is read from the end of the file. The latter is often more useful as that is typically where a fatal error message is found. TMPDIR String <code>/tmp/lava</code> Lava temporary directory for creating private run spaces for jobs. Not to be confused with the <code>TMPDIR</code> environment variable."},{"location":"10-installation-operation.html#configuration-for-cmd-jobs","title":"Configuration for cmd Jobs","text":"Name Type Default Description CMD_TIMEOUT Duration <code>10m</code> Run timeout for the job."},{"location":"10-installation-operation.html#configuration-for-dag-jobs","title":"Configuration for dag Jobs","text":"Name Type Default Description DAG_MAX_WORKERS Duration 4 * CPU count Maximum allowed number of worker threads for dag jobs. DAG_WORKERS Duration CPU count Default number of worker threads for dag jobs."},{"location":"10-installation-operation.html#configuration-for-db_from_s3-jobs","title":"Configuration for db_from_s3 Jobs","text":"Name Type Default Description PG_COPY_TIMEOUT Duration <code>30m</code> Timeout for individual client side copy operations for Postgres."},{"location":"10-installation-operation.html#configuration-for-docker-jobs","title":"Configuration for docker Jobs","text":"Name Type Default Description DOCKER_TIMEOUT Duration <code>10m</code> Run timeout for the container."},{"location":"10-installation-operation.html#configuration-for-exe-jobs","title":"Configuration for exe Jobs","text":"Name Type Default Description EXE_MAX_PAYLOAD_SIZE Duration <code>10M</code> Maximum payload size. EXE_TIMEOUT Duration <code>10m</code> Run timeout for the job."},{"location":"10-installation-operation.html#configuration-for-foreach-jobs","title":"Configuration for foreach Jobs","text":"Name Type Default Description FOREACH_LIMIT Integer 10 The default limit on loop iterations for a foreach job. This can be overridden in the job specification. FOREACH_MAX_LIMIT Integer 25 The maximum limit on loop iterations for a foreach job. This cannot be overridden in the job specification."},{"location":"10-installation-operation.html#configuration-for-pkg-jobs","title":"Configuration for pkg Jobs","text":"Name Type Default Description PKG_MAX_PAYLOAD_SIZE Size <code>20M</code> Maximum payload size. PKG_TIMEOUT Duration <code>10m</code> Run timeout for the job. PKG_UNPACK_TIMEOUT Duration <code>30s</code> Timeout for unpacking the job payload after downloading from S3."},{"location":"10-installation-operation.html#configuration-for-sharepoint-jobs","title":"Configuration for sharepoint Jobs","text":"<p>The <code>SP_LIST_*</code> parameters relate to conversion between SharePoint lists and columnar data files.</p> Name Type Default Description SP_LIST_DELIMITER String <code>|</code> (pipe) Single character field delimiter. SP_LIST_DOUBLEQUOTE Boolean <code>False</code> As for Python csv.writer. SP_LIST_ESCAPECHAR String None As for Python csv.writer. SP_LIST_QUOTING String <code>minimal</code> As for csv.writer <code>QUOTE_*</code> parameters (without the QUOTE_ prefix). Default <code>minimal</code> (i.e. <code>QUOTE_MINIMAL</code>). SP_LOGGING Boolean <code>False</code> Enable enhanced logging for the Sharepoint connector."},{"location":"10-installation-operation.html#configuration-for-sql-sqli-and-sqlv-jobs","title":"Configuration for sql, sqli and sqlv Jobs","text":"Name Type Default Description SQL_BATCH_SIZE Integer <code>1000</code> Number of rows to fetch in one go. SQL_DELIMITER String <code>|</code> (pipe) Single character field delimiter for data output. SQL_DIALECT String <code>excel</code> As for Python csv.writer. SQL_DOUBLEQUOTE Boolean <code>False</code> As for Python csv.writer. SQL_ESCAPECHAR String None As for Python csv.writer. SQL_MAX_PAYLOAD_SIZE Size <code>100k</code> Maximum payload size. (sql and sqlv only.) SQL_OUTPUT_SUFFIX String <code>.out</code> Suffix for output files. (sql, sqli only.) SQL_QUOTING String <code>minimal</code> As for csv.writer <code>QUOTE_*</code> parameters (without the QUOTE_ prefix). Default <code>minimal</code> (i.e. <code>QUOTE_MINIMAL</code>). <p>The following parameters are specific to sqlc  jobs:</p> Name Type Default Description SQLV_TIMEOUT Duration <code>10m</code> Run timeout for the job."},{"location":"10-installation-operation.html#configuration-for-sqlc-jobs","title":"Configuration for sqlc Jobs","text":"Name Type Default Description SQLC_MAX_PAYLOAD_SIZE Size <code>100k</code> Maximum payload size. SQLC_TIMEOUT Duration <code>10m</code> Run timeout for the job."},{"location":"10-installation-operation.html#configuration-for-the-aws-connector","title":"Configuration for the aws Connector","text":"Name Type Default Description AWS_ACCESS_KEY_CACHE_SIZE Integer <code>20</code> Size of the cache for AWS access keys. AWS_ACCESS_KEY_CACHE_TTL Duration <code>10m</code> AWS access keys are cached for the specified duration. This must be less than the duration for session credentials and should be significantly less. AWS_CONN_DURATION Duration <code>3h</code> The default session duration for AWS session credentials obtained by assuming an IAM role. This can be overridden in individual aws connections."},{"location":"10-installation-operation.html#configuration-for-the-email-connector","title":"Configuration for the email Connector","text":"Name Type Default Description EMAIL_MAX_ATTACHMENT_SIZE Size <code>2M</code> Maximum allowed size of any single attachment in bytes. EMAIL_MAX_SIZE Size <code>5M</code> Maximum allowed size of an email message in bytes. This includes the headers, body and attachments. EMAIL_MAX_ATTACHMENTS Integer <code>5</code> Maximum number of attachments per email message. Set to zero to disable attachments entirely."},{"location":"10-installation-operation.html#configuration-for-the-redshift-connector","title":"Configuration for the redshift Connector","text":"Name Type Default Description RS_PASSWORD_DURATION Duration <code>15m</code> Password validity period when obtaining temporary IAM credentials for Redshift provisioned clusters using GetClusterCredentials. <p>Note the same parameter is used for an equivalent purpose for the redshift-serverless connector.</p>"},{"location":"10-installation-operation.html#configuration-for-the-redshift-serverless-connector","title":"Configuration for the redshift-serverless Connector","text":"Name Type Default Description RS_PASSWORD_DURATION Duration <code>15m</code> Password validity period when obtaining temporary IAM credentials for Redshift Serverless clusters using GetCredentials. <p>Note the same parameter is used for an equivalent purpose for the redshift connector.</p>"},{"location":"10-installation-operation.html#lambda-function-configuration","title":"Lambda Function Configuration","text":"<p>The lambda functions use the same configuration mechanisms as described above for the lava worker with the exception that they do not read any configuration from the realms table. </p> <p>Setting configuration values to non-default values must be done by setting the corresponding <code>LAVA_*</code> environment variable on the lambda function itself.</p>"},{"location":"10-installation-operation.html#configuration-for-s3trigger","title":"Configuration for s3trigger","text":"Name Type Default Description S3TRIGGER_CACHE_TTL Duration <code>60s</code> The s3trigger lambda will cache lookup results on the s3triggers and jobs tables for the specified duration. A zero duration disables caching. S3TRIGGER_DEDUP_CACHE_SIZE Integer <code>0</code> The number of entries in the s3trigger deduplication cache. Setting this to <code>0</code> disables deduplication. S3TRIGGER_DEDUP_TTL Duration <code>30s</code> The time to live for entries in the s3trigger deduplication cache. SQS_MAX_DELAY_MINS Integer <code>15</code> Maximum allowed delay for an SQS message in minutes. This is an AWS constraint."},{"location":"10-installation-operation.html#instrumentation-and-monitoring-of-lava","title":"Instrumentation and Monitoring of Lava","text":"<p>Lava has the following instrumentation and monitoring facilities:</p> <ul> <li> <p>Lava worker and dispatcher logging.</p> </li> <li> <p>Logging to AWS CloudWatch logs for the lambda functions.</p> </li> <li> <p>AWS CloudWatch custom metrics for     lava jobs and internal worker behaviour.</p> </li> <li> <p>Predefined AWS CloudWatch alarms.</p> </li> </ul>"},{"location":"10-installation-operation.html#logging","title":"Logging","text":"<p>Both the worker and dispatcher accept a range of options to control logging.</p> <p>Capabilities include:</p> <ul> <li> <p>Generating a heartbeat message at a defined interval (worker only).</p> </li> <li> <p>Setting logging level anywhere between <code>debug</code> and <code>critical</code>.</p> </li> <li> <p>Logging to stderr, a file or designated syslog facility.</p> </li> <li> <p>Tagging log entries so they can be filtered from other unrelated logs.</p> </li> <li> <p>Logging in text or JSON format.</p> </li> </ul> <p>For more information:</p> <pre><code>lava-worker --help\nlava-dispatcher --help\n</code></pre> <p>The standard production configuration for the worker when running on the lava AMI is:</p> <ul> <li> <p>Set the logging level to <code>info</code></p> </li> <li> <p>Generate a heartbeat every 60 seconds. This is also used to trigger     heartbeat</p> </li> <li> <p>Log in JSON format to the <code>local0</code> syslog facility which is directed to the     file <code>/var/log/lava</code> on the worker machine. (The messages are also included     in the general system log in the file <code>/var/log/messages</code>.)</p> </li> <li> <p>The <code>/var/log/lava</code> log file is automatically replicated to the CloudWatch     log group <code>/var/log/lava/&lt;REALM&gt;</code>.</p> </li> </ul> <p>A typical message (although on a single line) appears thus:</p> <pre><code>{\n  \"event_source\": \"lava-worker\",\n  \"realm\": \"dev\",\n  \"worker\": \"core\",\n  \"host\": \"my-host\",\n  \"tag\": \"lava-worker\",\n  \"timestamp\": \"2023-05-11 15:04:36\",\n  \"level\": \"INFO\",\n  \"message\": \"Job cmd/hello-world (2ce3ac8e-0fc2-41f1-b908-998608a0b24b): Complete\",\n  \"thread\": \"worker-00\",\n  \"pid\": 37273,\n  \"event_type\": \"job\",\n  \"job_id\": \"test/cmd/hello-world\",\n  \"run_id\": \"2ce3ac8e-0fc2-41f1-b908-998608a0b24b\"\n}\n</code></pre> <p>The following search predicate would find this record in CloudWatch logs:</p> <pre><code>{ $.realm=dev &amp;&amp; $.job=cmd/hello-world &amp;&amp; $.run_id=2ce3ac8e-0fc2-41f1-b908-998608a0b24b }\n</code></pre> <p>The log group can also be queried using CloudWatch Log Insights. The (approximately) equivalent Insights query is:</p> <pre><code>fields @timestamp, @message, @logStream, @log\n| filter realm = \"dev\" and job_id=\"cmd/hello-world\" and run_id=\"2ce3ac8e-0fc2-41f1-b908-998608a0b24b\"\n| sort @timestamp desc\n</code></pre> <p>As is usual for Lambda functions, the lava lambdas also log directly to CloudWatch.</p>"},{"location":"10-installation-operation.html#heartbeats","title":"Heartbeats","text":"<p>When used with the worker startup script provided for use in production deployments, the lava worker emits a heartbeat message to syslog every 60 seconds. These records propagate to the CloudWatch log group <code>/var/log/messages</code>. A log metric filter on this group is used as the basis of a CloudWatch Metric which underpins an alarm in the event of loss of heartbeat. This is all configured via the worker CloudFormation stack.</p> <p>As of v7.1.0 (Pichincha), the heartbeat message also contains additional worker health information. A typical heartbeat message (although on a single line) appears thus:</p> <pre><code>{\n  \"event_source\": \"lava-worker\",\n  \"realm\": \"dev\",\n  \"worker\": \"core\",\n  \"host\": \"my-host\",\n  \"tag\": \"lava-worker\",\n  \"timestamp\": \"2023-05-11 18:09:23\",\n  \"level\": \"INFO\",\n  \"message\": \"heartbeat realm=dev worker=core\",\n  \"thread\": \"heartbeat\",\n  \"pid\": 37273,\n  \"event_type\": \"heartbeat\",\n  \"sqs\": {\n    \"messages\": 0,\n    \"notvisible\": 0\n  },\n  \"internal\": {\n    \"qlen\": 0\n  },\n  \"threads\": {\n    \"event\": \"OK\",\n    \"worker-00\": \"OK\",\n    \"worker-01\": \"OK\",\n    \"heartbeat\": \"OK\",\n    \"metrics\": \"OK\"\n  }\n}\n</code></pre> <p>The various threads perform the following functions:</p> Thread Description event Send events to the DynamoDB events table. heartbeat Emit heartbeat messages. metrics Send metric data to CloudWatch. worker-* Run lava jobs."},{"location":"10-installation-operation.html#cloudwatch-metrics","title":"CloudWatch Metrics","text":"<p>The lava worker can generate CloudWatch custom metrics for individual jobs and for the worker itself.</p>"},{"location":"10-installation-operation.html#job-metrics","title":"Job Metrics","text":"<p>Job metrics can be enabled at the realm level or at the individual job level. The following process is used to determine if metric data will be produced:</p> <ol> <li> <p>If the <code>cw_metrics</code> field is set in the     job specification, that value is used to     either enable or disable metric data generation.</p> </li> <li> <p>Otherwise, the value specified at the     worker or realm level via the      CW_METRICS_JOB parameter     is used.</p> </li> </ol> <p>If enabled, the following job metrics are produced for each job:</p> Metric Dimensions Description JobFailed Realm, Job <code>0</code> if the job succeeded and <code>1</code> if it failed. RunDelay Realm, Worker The time in seconds between job dispatch and job start. This should normally be under 10 seconds. Values above this may indicate the worker is overloaded. RunTime Realm, Job The runtime in seconds of the job from when it started to the completion of any post-job actions."},{"location":"10-installation-operation.html#worker-metrics","title":"Worker Metrics","text":"<p>Worker metrics can be enabled at the  worker or realm level via the  CW_METRICS_WORKER parameter.</p> <p>If enabled, the following worker metrics are produced:</p> Metric Dimensions Description MaxRss Realm, Worker, Instance The maximum resident set size for the main worker process. See getrusage(2) for more information. PercentDiskUsed Realm, Worker, Instance This is a deprecated name for <code>PercentTmpDiskUsed</code>. It will be removed in a future release. PercentDockerDiskUsed Realm, Worker, Instance The percentage of disk space used for the docker volume on the worker, if present. PercentMemUsed Realm, Worker, Instance The percentage of memory used on the worker measured as the total physical memory minus the memory that can be given instantly to processes without the system going into swap. See psutil.virtual_memory(). PercentSwapUsed Realm, Worker, Instance The percentage of swap memory used on the worker. See psutil.virtual_memory(). PercentTmpDiskUsed Realm, Worker, Instance The percentage of disk space used for the lava temporary area on the worker. WorkerThreadsAlive Realm, Worker, Instance The number of job worker threads that are alive. The worker thread pool size is dependent on the configuration of a particular worker daemon. This is typically controlled in the realms table. WorkerThreadsDead Realm, Worker, Instance The number of job worker threads that have died. If not enough worker threads are active, job delays can increase."},{"location":"10-installation-operation.html#cloudwatch-alarms","title":"CloudWatch Alarms","text":"<p>The CloudFormation worker template optionally creates the following CloudWatch alarms:</p> <ul> <li> <p>A heartbeat alarm if the worker heartbeat message is not detected in the     <code>/var/log/messages</code> CloudWatch log group for 5 minutes. This relies on the     worker being configured to generate a heartbeat message every 60 seconds and     the node being configured to replicate syslog messages to CloudWatch.</p> </li> <li> <p>An SQS queue depth alarm if the <code>ApproximateNumberOfMessagesVisible</code> metric     exceeds a value specified as a CloudFormation stack parameter. This alarm     can indicate a dead or overloaded worker.</p> </li> </ul> <p>The target for the alarms is an SNS topic specified as a CloudFormation stack parameter.</p>"},{"location":"10-installation-operation.html#getting-internal-worker-state-information","title":"Getting Internal Worker State Information","text":"<p>It is possible to get the worker to reveal some of its internal state by sending it a SIGUSR1 (signal number 30). The information is written to the logger. Typically, this is sent to syslog.</p>"},{"location":"10-installation-operation.html#maintaining-dynamodb-table-entries","title":"Maintaining DynamoDB Table Entries","text":"<p>Lava comes with a number of mechanisms to assist with maintaining the health of the entries in the DynamoDB tables. These include:</p> <ul> <li> <p>The lava job framework which provides a     standardised template for creating, configuring and deploying lava jobs and     associated components. It also provides support for automatic generation of     checksums on table entries and     configuration drift detection.</p> </li> <li> <p>Deep Schema validation via the lava-schema     utility.</p> </li> <li> <p>Bad practice detection via the lava-check utility.</p> </li> <li> <p>Checksum generation and validation for DynamoDB table entries via the     lava-checksum utility and the     lava job framework.</p> </li> </ul>"},{"location":"10-installation-operation.html#backing-up-lava-configuration","title":"Backing Up Lava Configuration","text":"<p>When the lava DynamoDB tables are created using the realm CloudFormation template, point-in-time recovery is configured for the main configuration tables. DynamoDB maintains continuous backups of the tables for the last 35 days.</p> <p>Lava also comes with two additional utilities to facilitate bulk extraction and backup of data from the lava tables.</p> <ul> <li> <p>lava-dump performs a bulk extract of data from a     single table to a local directory. It can extract all entries with keys that     match any of a list of GLOB style patterns. By default, all entries are     extracted.</p> </li> <li> <p>lava-backup performs a complete extract of all of     the configuration tables for a given realm and stores the result in a zip     file, either locally or in AWS S3. It can be run as a lava     cmd job if required.</p> </li> </ul>"},{"location":"10-installation-operation.html#lava-worker-management","title":"Lava Worker Management","text":""},{"location":"10-installation-operation.html#lava-worker-auto-scaling","title":"Lava Worker Auto Scaling","text":"<p>Lava workers created using the provided worker CloudFormation template sit in an auto scaling group with a set of supporting resources to assist with horizontal capacity scaling and controlled worker node termination.</p> <p>The auto scaling architecture is shown below.</p> <p></p> <p>The key components are:</p> <ol> <li> <p>The EC2 auto scaling group itself, named <code>lava-&lt;REALM&gt;-&lt;WORKER&gt;</code>.       The minimum, preferred and maximum instance counts are specified as     parameters in the worker CloudFormation stack. Workers that act as schedule     based dispatchers must have all values set to 1. Other workers can have     whatever is needed noting that the auto scaling can create that many     instances, so be reasonable. Also note that if auto scaling is enabled, the     minimum should be 1 or the auto scaler will scale down to 0 and it will     never scale up.</p> </li> <li> <p>A lambda function, <code>lava-&lt;REALM&gt;-metrics</code>.       This is triggered by an Amazon EventBridge schedule every minute to generate     a metric that is the worker SQS job queue depth per in-service instance in     the auto scaling group. This metric is used to drive the auto scaling     process using a TargetTracking policy.  This is referred to as the     worker backlog.</p> </li> <li> <p>An EC2 auto scaling TargetTracking policy.       This is controlled by the worker backlog metric to cause the worker fleet to     scale out or in as appropriate. The target value for auto scaling purposes     is specified in the worker CloudFormation stack. See Setting the Auto     Scaling Target     Value.</p> </li> <li> <p>An EC2 auto scaling lifecycle hook for terminating nodes.       This will send an appropriate message to the Amazon EventBridge default bus.</p> </li> <li> <p>An AWS EventBridge rule, <code>lava-&lt;REALM&gt;-&lt;WORKER&gt;-terminating</code>.       This detects the lifecycle hook message and triggers the <code>lava-&lt;REALM&gt;-stop</code>     lambda function, providing it with the lifecycle event details.</p> </li> <li> <p>The <code>lava-&lt;REALM&gt;-stop</code> lambda function.       This uses AWS Systems Manager <code>RunCommand</code> to run the     lava-stop     utility on the instance. This will do a controlled shutdown of the lava     worker daemons and wait for them to complete. It also inhibits further     scheduled job dispatches during the shutdown process. Auto scaling     lifecycle hook heartbeat messages will be issued periodically until the      daemons stop, at which point a lifecycle completion message will be sent.     If the daemons fail to stop in a reasonable period of time, they are killed.</p> </li> </ol>"},{"location":"10-installation-operation.html#setting-the-auto-scaling-target-value","title":"Setting the Auto Scaling Target Value","text":"<p>The lava worker EC2 auto scaling process uses a TargetTracking policy based on a custom metric referred to as the worker backlog. This is defined as the <code>ApproximateNumberOfMessagesVisible</code> for the worker job queue divided by the number of instances in the <code>InService</code> state in the work auto scaling group.</p> <p>In short, the worker backlog is the number of jobs waiting per worker instance.</p> <p>This follows the pattern recommended by AWS.</p> <p>The metric is calculated every minute by the <code>lava-&lt;REALM&gt;-metrics</code> lambda function.</p> Metric Name Namespace Dimensions Unit WorkerBacklog Lava Realm, Worker None <p>The crucial aspect of the auto scaling process is the selection of the target value for the <code>WorkerBacklog</code> metric.</p>"},{"location":"10-installation-operation.html#method-1-aws-recommended-approach","title":"Method 1 - AWS Recommended Approach","text":"<p>To do this, AWS recommends the following process:</p> <ol> <li> <p>Estimate the acceptable latency for a job to run.</p> </li> <li> <p>Estimated the average processing time (i.e. average job run time).</p> </li> <li> <p>Divide the latency by the average run time to get the target value.</p> </li> </ol> <p>For example, if the acceptable latency is 10 minutes (600 seconds) and the average job run time is 20 seconds, the target value would be 30. So if the queue gets 300 messages, it will start 10 instances. This may, or may not, be what you want to happen.</p> <p>Note</p> <p>The number of worker instances is always capped by the maximum instance count of the auto scaling group, as specified in the lava worker CloudFormation stack.</p>"},{"location":"10-installation-operation.html#method-2-guess","title":"Method 2 - Guess","text":"<p>The trouble with method 1 is that lava jobs can vary widely in run time, from seconds to hours. So the average processing time may not be a reliable data point.</p> <p>An alternative approach is to disable auto scaling initially (using parameters in the worker CloudFormation stack) and monitor the SQS queue depth for a period of time. Then take an educated guess.</p> <p>Tip</p> <p>It's probably better to set the target too high rather than too low, initially.</p>"},{"location":"10-installation-operation.html#stopping-the-worker-daemon","title":"Stopping the Worker Daemon","text":"<p>As of version 5.1.0 (Tungurahua), the preferred mechanism to stop the worker daemon is to send it one of the following signals to initiate a controlled shutdown:</p> <ul> <li>SIGHUP (signal number 1)</li> <li>SIGINT (signal number 2)</li> </ul> <p>The worker will complete any in-flight jobs before terminating.</p> <p>A second signal will cause a hard shutdown, causing in-flight jobs to be terminated and, possibly, resubmitted by SQS once the queue visibility timeout expires.</p>"},{"location":"10-installation-operation.html#aws-systems-manager-support","title":"AWS Systems Manager Support","text":"<p>As of lava version 6.2.0 (Reventador), a number AWS Systems Manager command documents are provided to assist with performing operations on EC2 based worker nodes. These are deployed as part of the lava-common.cfn.json CloudFormation stack.</p>"},{"location":"10-installation-operation.html#lava-rebootworkerinstance","title":"lava-RebootWorkerInstance","text":"<p>This command document performs the following steps:</p> <ol> <li> <p>Perform a yum security update.</p> </li> <li> <p>Stop any further scheduled dispatches from the instance.</p> </li> <li> <p>Perform a controlled stop of any lava worker daemons.</p> </li> <li> <p>Reboot the instance.</p> </li> </ol> <p>When the instance reboots, the dispatch blockage will be removed and the worker daemons will be restarted.</p>"},{"location":"10-installation-operation.html#lava-securityupdate","title":"lava-SecurityUpdate","text":"<p>This command document is a more sophisticated version of lava-RebootWorkerInstance designed specifically to perform yum security updates. It is less disruptive in that it will not reboot a worker instance unless it is necessary.</p> <p>It performs the following steps:</p> <ol> <li> <p>Check if the instance has been up for at least a specified period of time.</p> </li> <li> <p>Check if any security updates are pending. If not, exit.</p> </li> <li> <p>Apply the security updates with <code>yum update --security</code>.</p> </li> <li> <p>Check if a reboot is required. If not, exit.</p> </li> <li> <p>Signal the worker daemons to stop.</p> </li> <li> <p>Wait a specified period of time for the worker daemons to finish any     in-flight jobs and stop.</p> </li> <li> <p>Kill any worker daemons still running.</p> </li> <li> <p>Reboot the instance.</p> </li> </ol> <p>The command document accepts the following parameters:</p> Parameter Default Description ExecutionTimeout <code>3600</code> Execution timeout in seconds. The <code>ExecutionTimeout</code> must exceed the <code>Wait</code> duration by enough to allow the patching activity and a reboot. LogLevel <code>info</code> Logging level. Allowed values are <code>debug</code>, <code>info</code>, <code>warning</code>. MinUpDays <code>0</code> Skip the update process if the EC2 instance hasn't been up for this many days. Signal <code>SIGHUP</code> Signal the worker to stop (if necessary) with the specified signal. Allowed values are <code>SIGHUP</code> and <code>SIGKILL</code>. See Stopping the Worker Daemon. Wait <code>15m</code> Wait for the specified duration for lava workers to stop voluntarily before killing them. <p>The command document sends status messages to Amazon EventBridge as part of the process. These can be captured using standard EventBridge mechanisms to send notifications to system operators or trigger other automated action, as required. The messages look like this:</p> <pre><code>{\n  \"version\": \"0\",\n  \"id\": \"a1ce8e44-dada-d265-2b9e-beed76ab493b\",\n  \"detail-type\": \"Lava Worker Instance Patching Notification\",\n  \"source\": \"lava\",\n  \"account\": \"123456789123\",\n  \"time\": \"2023-05-20T08:16:36Z\",\n  \"region\": \"ap-southeast-2\",\n  \"resources\": [],\n  \"detail\": {\n    \"instance-id\": \"i-082cafe1b7811a47a\",\n    \"instance-name\": \"lava-dev0-core\",\n    \"info\": \"Rebooting after security patching\"\n  }\n}\n</code></pre> <pre><code>{\n  \"version\": \"0\",\n  \"id\": \"79d96f65-1be1-50b4-fd4c-d0ae39981eb7\",\n  \"detail-type\": \"Lava Worker Instance Patching Notification\",\n  \"source\": \"lava\",\n  \"account\": \"123456789123\",\n  \"time\": \"2023-05-20T08:18:00Z\",\n  \"region\": \"ap-southeast-2\",\n  \"resources\": [],\n  \"detail\": {\n    \"instance-id\": \"i-082cafe1b7811a47a\",\n    \"instance-name\": \"lava-dev0-core\",\n    \"info\": \"Reboot complete\"\n  }\n}\n</code></pre>"},{"location":"10-installation-operation.html#lava-stopworkerdaemons","title":"lava-StopWorkerDaemons","text":"<p>This command document performs the following steps:</p> <ol> <li> <p>Optionally, stop any further scheduled dispatches from the instance.</p> </li> <li> <p>Perform a controlled stop of any lava worker daemons.</p> </li> </ol> <p>The instance is not rebooted. Further action (e.g. to restart lava worker daemons and re-enable dispatches) requires operator intervention.</p>"},{"location":"10-installation-operation.html#security-patching-of-lava-ec2-workers","title":"Security Patching of Lava EC2 Workers","text":"<p>The lava-SecurityUpdate SSM command document provides a convenient mechanism to perform controlled security updates on EC2 based lava workers.</p> <p>This command document can be scheduled by lava itself via a standard job. The following example shows how this can be done for a given target realm (which can be different from the realm initiating the command).</p> <pre><code>{\n  \"description\": \"Security patching for lava nodes\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"lava/admin/secupdate/&lt;REALM&gt;\",\n  \"owner\": \"Deus ex machina\",\n  \"parameters\": {\n    \"vars\": {\n      \"realm\": \"dev\",\n      \"up_days\": 7,\n      \"wait_mins\": 60\n    }\n  },\n  \"payload\": \"aws ssm send-command --document-name \\\"lava-SecurityUpdate\\\" --targets '[{\\\"Key\\\":\\\"tag:LavaRealm\\\",\\\"Values\\\":[\\\"{{vars.realm}}\\\"]}]' --parameters '{\\\"Wait\\\":[\\\"{{vars.wait_mins}}m\\\"],\\\"MinUpDays\\\":[\\\"{{vars.up_days}}\\\"],\\\"ExecutionTimeout\\\":[\\\"{{(vars.wait_mins+15)*60}}\\\"]}' --cloud-watch-output-config '{\\\"CloudWatchOutputEnabled\\\":true,\\\"CloudWatchLogGroupName\\\":\\\"lava\\\"}'\",\n  \"schedule\": \"0 19 * * Sat,Sun\",\n  \"type\": \"cmd\",\n  \"worker\": \"core\"\n}\n</code></pre> <p>The IAM setup for this to work requires that the lava worker can run <code>ssm:SendCommand</code> for</p> <ul> <li>The <code>lava-*</code> command documents; and</li> <li>For all EC2 instances with specified values of the <code>LavaRealm</code> tag.</li> </ul> <p>The IAM policy for the worker will include elements like these:</p> <pre><code>{\n  \"Sid\": \"RunLavaCommandDocs\",\n  \"Action\": \"ssm:SendCommand\",\n  \"Effect\": \"Allow\",\n  \"Resource\": [\n    \"arn:aws:ssm:ap-southeast-2:123456789123:document/lava-*\"\n  ]\n},\n{\n  \"Sid\": \"RunLavaCommandInstances\",\n  \"Action\": \"ssm:SendCommand\",\n  \"Condition\": {\n    \"StringLike\": {\n      \"ssm:resourceTag/LavaRealm\": \"*\"\n    }\n  },\n  \"Effect\": \"Allow\",\n  \"Resource\": [\n    \"arn:aws:ec2:*:*:instance/*\"\n  ]\n},\n</code></pre>"},{"location":"10-installation-operation.html#the-lava-ec2-ami","title":"The Lava EC2 AMI","text":"<p>The lava worker utilises a suite of external capabilities on the host platform. Examples include:</p> <ul> <li>A stable and predictable Linux O/S</li> <li>A compatible version of Python</li> <li>Docker</li> <li>The Postgres, MySQL and Oracle CLI binaries</li> <li>The Oracle driver and SQL*Plus binary</li> <li>UnixODBC and FreeTDS for accessing MSSQL databases</li> <li>AWS RDS and Redshift trust certificates</li> <li>AWS SSM agent for host maintenance</li> <li>AWS CloudWatch logs agent</li> <li>Some of the platform dependent Python packages (e.g. psutil).</li> </ul> <p>... and, of course, ...</p> <ul> <li>The lava worker code bundle.</li> </ul> <p>The lava EC2 AMI incorporates all of the required components, except for the lava code bundle itself, which is loaded from S3 at boot time. This makes it easier to roll the deployed lava version forward/backward just by changing an entry in the realms table and rebooting.</p> <p>The lava AMI is based on Amazon Linux 2023. X86 and Graviton EC2 instances are supported.</p>"},{"location":"10-installation-operation.html#lava-ami-names","title":"Lava AMI Names","text":"<p>Lava AMI names look like so:</p> <pre><code>lava-8.1.0-amzn2023-py3.11-arm64-2025-07-02T08-43-48Z\n     --+-- ---+---- --+--- --+-- ----------+---------\n       |      |       |      |             |\n       |     O/S      | Architecture       |\n       |      |       |                    |\n     lava     |    Python                Build\n    Version   |    Version               Date\n              |       |\n              +---+---+\n                  |\n               Runtime\n</code></pre> <p>Note</p> <p>With AMI names, the architecture for ARM is always <code>arm64</code>, never <code>aarch64</code>. Lava worker code bundles for ARM can have either <code>arm64</code> or <code>aarch64</code>, depending on how the target operating system identifies its platform. You say tomato ... (Google it Grasshopper).</p>"},{"location":"10-installation-operation.html#lava-ami-architecture","title":"Lava AMI Architecture","text":"<p>The lava AMI can be built for different CPU architectures. Typically this is one of <code>x86_64</code> or <code>arm64</code>.</p> <p>This can affect lava job payloads for pkg jobs. If the payload contains binary components, they may have a dependency on a particular machine architecture.</p>"},{"location":"10-installation-operation.html#lava-ami-versions","title":"Lava AMI Versions","text":"<p>Lava AMI versions are derived from the version number of a given lava release (e.g. v8.1.0). While there is generally not a hard binding between the lava worker version and the AMI version, its best to keep them as close to in-sync as possible.</p>"},{"location":"10-installation-operation.html#python-versions-in-the-lava-ami","title":"Python Versions in the Lava AMI","text":"<p>The lava AMI can be built with different Python versions. This can affect lava job payloads for pkg jobs. If the payload contains Python components, they may have a dependency on a particular version of Python.</p> <p>The Python version for a lava AMI is also indicated in the <code>PYTHON_VERSION</code> tag on the AMI, and also in the AMI name.</p> <p>Tip</p> <p>If possible, use the <code>PYTHON_VERSION</code> tag in preference to the AMI name if checking for Python version in case of future name structure changes.</p>"},{"location":"10-installation-operation.html#building-the-lava-ami","title":"Building the Lava AMI","text":"<p>The lava AMI is built using packer. Most of the components for the build are in the <code>ami</code> directory in the lava repo. Some components (e.g. large 3rd party code bundles like the Oracle drivers) must be loaded to a specified location in S3 prior to the build. The build process will do this for you. This is done to speed up the build process as it avoids the need to transfer these to the build instance at build time.</p> <p>The build process leaves the following artefacts on the AMI in <code>~ec2-user/packer</code>:</p> <ul> <li> <p>The build components</p> </li> <li> <p>The detailed log from the build.</p> </li> </ul> <p>The packer executable is also installed.</p> <p>In theory, an instance based on the lava AMI can be used to rebuild the AMI itself.</p> <p>The essence of the process to build the AMI is:</p> <pre><code># Make sure the lava virtualenv is activated first!\n\n# From the top of the lava source repo...\ncd ami\n\n# Get some help, just in case...\nmake help\n\n# Build the AMI...\nmake ami param=value ...\n</code></pre> <p>This will first sync the required S3 based components from the local filesystem to S3 and then initiate the packer build. A number of tags are added to the resultant image to record things such as the corresponding lava version and the installed Python version.</p> <p>The trick to the process is getting the build parameters right for the target environment. These are either specified on the make command line with the <code>param=value</code> arguments (the hard way) or obtained from a configuration file (the easy way).</p> Parameter Required Description ami_id No ID of the base AMI. If not specified, the <code>ami_ref</code> is read from <code>ami-build.yaml</code>, which is then used to lookup the AMI ID. Don't mess with this unless you really know what you're doing. ami_ref No The <code>ami_ref</code> is the tail of an AWS SSM parameter name containing the ID of the base AMI. AWS provides a bunch of these parameters to simplify finding common AMIs. e.g. <code>al2023-ami-kernel-default-x86_64</code>. See https://docs.aws.amazon.com/linux/al2023/ug/ec2.html. If not specified, the <code>os</code> is looked up in <code>ami-build.yaml</code> to obtain the value. arch No The CPU architecture. This must be one of <code>x86_64</code> (default) or <code>arm64</code>. config No Name of a YAML configuration file (in lava deploy format) containing values for any or all of the required parameters. A configuration file can contain separate configurations for different environments (e.g. dev, prod etc.). If specified, the <code>env</code> parameter is also required. The AMI configuration parameters are specified under the <code>&lt;ENV&gt; -&gt; ami</code> key. A sample configuration file can be found in <code>deploy/sample.yaml</code>. env No The environment identifier within a specified configuration file. os Yes The O/S type of the AMI to be built. Allowed values are specified in <code>ami-build.yaml</code> and must correspond to a builder in the <code>os</code> directory. Currently supported values are <code>amzn2</code> (Amazon Linux 2) and <code>amzn2023</code> (Amazon Linux 2023). instance_type No The AWS EC2 build instance type. This must match the CPU architecture specified by <code>arch</code>. e.g. If the <code>arch</code> is <code>arm64</code>, a Graviton build instance must be selected. If not specified, the value is read from <code>ami-build.yaml</code>. ip Yes IP address type for the packer build instance. Either <code>public</code> or <code>private</code> python_version Yes The Python version to build from source for the AMI (e.g. <code>3.11.12</code>). s3bucket Yes Name of the S3 bucket containing extra build resources (e.g. Oracle client binaries). s3prefix Yes The S3 prefix containing extra build resources (e.g. Oracle client binaries). sg Yes Security group ID (or a comma separated list of IDs) for the packer build instance. subnet Yes Subnet ID for the packer build instance. tz Yes Timezone to set in instances created from the lava AMI (e.g. <code>Australia/Sydney</code>)"},{"location":"10-installation-operation.html#the-hard-way-specifying-build-parameters-manually","title":"The Hard Way - Specifying Build Parameters Manually","text":"<p>The required configuration parameters can all be specified as part of the make command line, like so:</p> <pre><code>make ami \\\n    ami_id=\"ami-0043df2e553ad12b6\" \\\n    ip=\"public\" \\\n    python_version=\"3.11.12\" \\\n    s3bucket=\"my-artefact-bucket\" \\\n    s3prefix=\"lava/ami/\" \\\n    sg=\"sg-004d25956a3bf5bd4\" \\\n    subnet=\"subnet-f6b6d681\" \\\n    tz=\"Australia/Sydney\"\n</code></pre> <p>This gets old very quickly when building for multiple environments.</p>"},{"location":"10-installation-operation.html#the-easy-way-reading-build-parameters-from-a-configuration-file","title":"The Easy Way - Reading Build Parameters from a Configuration File","text":"<p>All of the mandatory build parameters can be placed in a YAML configuration file (let's call it <code>config.yaml</code>) formatted like so:</p> <pre><code># This is the config for the \"dev\" environment. The file can contain\n# multiple environments.\ndev:\n  # The underscore key is for general config\n  _:\n    # If other S3 location keys don't start with s3:// this base is used.\n    # Don't add trailing / here.\n    s3base: my-bucket/a/prefix\n\n  # Parameters for packer when building the lava ami.\n  ami:\n    # Location of additional resources for building the lava AMI.\n    s3: lava/ami/\n    # Public or private IP address for the build\n    ip: public\n    # Build instance subnet\n    subnet: subnet-f6b6d681\n    # Security groups -- this must be a comma separated list (no spaces)\n    sg: sg-004d25956a3bf5bd4\n    # Timezone set on instances created from the AMI.\n    tz: Australia/Sydney\n    # Python version to build from source\n    python:\n      version: \"3.11.12\"\n</code></pre> <p>The configuration file is in lava standard deploy format and may also contain other parameters for deploying the lava code bundles to S3 etc. A sample version is provided in <code>deploy/sample.yaml</code>.</p> <p>The AMI build process then becomes:</p> <pre><code>make ami config=config.yaml env=dev\n</code></pre> <p>It is still possible to override individual parameters like so:</p> <pre><code>make ami config=config.yaml env=dev python_version=3.11.11\n</code></pre>"},{"location":"10-installation-operation.html#user-data-for-the-lava-ami","title":"User Data for the Lava AMI","text":"<p>When a lava AMI instance boots, the normal Linux boot process will eventually get around to running <code>/etc/rc.local</code>. At the end of this, a lava AMI specific process runs all of the scripts in the directory <code>/usr/local/etc/rc.d</code>. A number of these scripts examine the EC2 instance user data, expecting to find a JSON formatted object which they search for keys that provide configuration instructions.</p> <p>The following keys are currently supported in the user data JSON. All of the top level keys are optional. If not present, the relevant script will take no action.</p> Key Sub-key Description crontab <code>&lt;USER_NAME&gt;</code> A dictionary of Linux instance user names. The value for each user name is the name of an S3 object containing the crontab for that user. shell - A string (or list of strings) containing a command (or list of commands) to run. This runs after all the other startup scripts have run. This is used by the lava worker CloudFormation template to invoke the lava installer to download, install and run the lava worker code on boot. shell0 - Same as the <code>shell</code> key except this runs before any of the other scripts. swap size If present and non-zero, swapping is enabled. The value is the swap file size in Gibibytes (1024 * 1024 * 1024 bytes). The swap file is on the root volume so there must be adequate space to hold it. More information on swap space and swap file size selection. swap file Name of the swap file. It must be on the root volume. The default is <code>/swapfile</code>. <p>For example, the following user-data will cause a lava AMI based instance to:</p> <ul> <li> <p>Install a crontab file for users <code>ec2-user</code> and <code>root</code>.</p> </li> <li> <p>Enable swapping with swap file of 2GiB.</p> </li> <li> <p>Run a couple of shell commands.</p> </li> </ul> <pre><code>{\n  \"swap\": {\n    \"size\": 2,\n    \"file\": \"/swap_till_you_drop\"\n  },\n  \"crontab\": {\n    \"ec2-user\": \"s3://mybucket/ec2config/latest/ec2-user.cron\",\n    \"root\": \"s3://mybucket/ec2config/latest/root.cron\"\n  },\n  \"shell\": [\n    \"echo Hello world\",\n    \"echo Resistance is futile &gt; /tmp/borg\"\n  ]\n}\n</code></pre>"},{"location":"10-installation-operation.html#configuring-a-lava-worker-ec2-instance-to-use-a-lava-ami","title":"Configuring a Lava Worker EC2 Instance to Use a Lava AMI","text":"<p>The AMI used by an EC2 based lava worker is specified as a parameter in the worker CloudFormation template.</p> <p>The template creates the necessary EC2 launch template and auto scaler and also ensures the instance user data is correctly constructed to install and start the lava worker(s) on boot.</p> <p>To select or change the AMI, simply update the relevant worker CloudFormation stack. While this can be done from the AWS console, it is more convenient to use the lava-ami utility which displays the available, lava compatible, AMIs and the AMI specified in each lava worker CloudFormation stack. It also provides an update mode that allows a (more or less) interactive process to select and apply a different AMI to one or more lava worker stacks.</p>"},{"location":"11-lava-job-framework.html","title":"The Lava Job Framework","text":"<p>Building and deploying simple lava jobs is fairly straightforward. For more complex requirements, a lava solution may require multiple jobs with associated JSON job specifications, connectors and S3 triggers to be tailored and deployed across multiple lava realms (e.g. dev, test and production). Job payloads also need to be assembled into packages or docker containers and deployed. This can be manually intensive and error prone.</p> <p>The lava job framework provides a suggested way of structuring, building and deploying lava jobs. Its use is completely optional.</p> <p>The framework provides the following advantages over hand-crafting a complex lava solution.</p> <ul> <li> <p>Job, connection and s3 trigger specifications can be specified in either     YAML or JSON in a realm / environment independent way. YAML is easier to     read, write and annotate than JSON. YAML formatted samples are provided in     the Lava Job Framework Samples     section.</p> </li> <li> <p>Environment specific information is managed in separate, extensible     configuration files.</p> </li> <li> <p>The deployable job, connection and s3 trigger specifications and the job     payloads can be generated and deployed for a target environment in a single     step.</p> </li> <li> <p>The framework can automatically build and deploy single file     exe, sql     and sqlc payloads, multi-file     pkg payloads and images for     docker jobs. It could be readily     integrated into a CI/CD pipeline.</p> </li> </ul> <p>The framework uses a combination of the following common tools:</p> <ul> <li> <p>cookiecutter</p> </li> <li> <p>GNU make</p> </li> <li> <p>Jinja rendering</p> </li> <li> <p>The AWS CLI.</p> </li> </ul> <p>It works on Linux and macOS. It can also run inside a docker container, with some limitations. It is not supported on DOS.</p>"},{"location":"11-lava-job-framework.html#usage-guidelines","title":"Usage Guidelines","text":"<p>A key goal of the framework is to facilitate the separation of environment specific configuration details from the structure and logic of a lava based solution. When developing lava solution components it is critical to properly parameterise the various components to allow the same solution to be rapidly migrated from one environment to another (e.g. dev to prod).</p> <p>The following guidelines should be considered:</p> <ol> <li> <p>Become familiar with basic Jinja     syntax. Jinja is used extensively in lava and the job framework.</p> </li> <li> <p>Never hard-code any environment specific information into source code.     These details should be properly parameterised and values provided by the     environment configuration file at build time. Typical environment specific     information includes S3 bucket names, database identifiers, schema names,     host names and addresses etc.</p> </li> <li> <p>Complete solution design, implementation and testing in a single development     environment. Once done, the configuration file can be cloned for other     environments and the parameters adjusted appropriately. Building for these     other environments is then a quick and easy process.</p> </li> </ol>"},{"location":"11-lava-job-framework.html#getting-started","title":"Getting Started","text":"<p>Ensure you have GNU make and Python 3.9+ installed. Make will be preinstalled on many Linux systems, or will be available in the distro package repos. For macOS, install the Xcode developer tools.</p> Recommended ApproachLegacy Approach <p>New in v8.2 (K\u012blauea)</p> <p>The recommended approach to creating a new lava framework project is using the lava-new utility.</p> <pre><code># Install the jinlava package if not already installed.\npip install --user jinlava\n\n# Create a new lava framework project.\nlava-new my-project-directory\n</code></pre> <p>The lava framework itself is packaged as a zip file: <code>lava-job-framework-&lt;VERSION&gt;.zip</code>. If this is not available, it can be built by cloning the lava repo and running <code>make tools</code>. The zip file will be placed in the <code>dist/dev-tools</code> directory.</p> <p>Note</p> <p>This was the approach required prior to v8.2 (K\u012blauea). It can still be used provided you have access to the lava framework cookiecutter bundle.</p> <pre><code># Install cookiecutter \npip install --user --upgrade cookiecutter\n\n# Install the AWS CLI, if not installed.\npip install --user --upgrade awscli\n\n# Create a new lava project. Cookiecutter will issue prompts for a few\n# configuration parameters. The parameters \"project_name\" and \"project_dir\"\n# are particularly important. The others can easily be changed later.\n\ncookiecutter lava-job-framework-&lt;VERSION&gt;.zip\n</code></pre> <p>Either approach will prompt the user for some basic configuration options and then create the project structure in the specified directory. It should look like this (items in angled brackets refer to the values provided in response to cookiecutter prompts):</p> <pre><code>&lt;project_dir&gt;/\n        +--&gt; Makefile               # Master make file. Try \"make help\".\n        +--&gt; bin/                   # Miscellaneous utilities.\n        +--&gt; config/                # Config files - one per environment.\n        |       +--&gt; &lt;env&gt;.yaml     # Initial config built by cookiecutter.\n        +--&gt; etc/                   # Miscellaneous support files.\n        +--&gt; lava-connections/      # Lava connection specifications.\n        +--&gt; lava-jobs/             # Lava job specifications.\n        +--&gt; lava-payloads/         # Lava job payloads.\n        +--&gt; lava-rules/            # Amazon EventBridge specifications.\n        +--&gt; lava-triggers/         # Lava s3trigger specifications.\n        +--&gt; misc/                  # Non-deployable job related components.\n</code></pre> <p>To complete the setup:</p> <p><pre><code># cd to the new project directory\ncd &lt;project_dir&gt;\n\n# Initialise the project environment. This will create a virtualenv and install\n# a bunch of required components. This can be safely rerun at any time.\nmake init\n\n# Activate the virtual environment\nsource venv/bin/activate\n</code></pre> If the project requires any non-standard Python packages, create a suitable <code>requirements.txt</code> file at the root of the project directory before running <code>make init</code>. The packages required by the framework itself are already covered in <code>etc/requirements.txt</code>.</p> <p>The project is now ready to start creating the various lava components.</p>"},{"location":"11-lava-job-framework.html#jinja-rendering-of-lava-framework-components","title":"Jinja Rendering of Lava Framework Components","text":"<p>It is very important to create the lava job framework component specifications in a way that is environment independent. This allows the same specification file to generate deployable components for multiple target environments (e.g. development, testing, production). The framework achieves this by placing all environment specific parameters into a YAML configuration file in the <code>config</code> directory and using Jinja rendering to inject the environment parameters into the specifications at build/deploy time.</p> <p>The cookiecutter will create an initial skeleton environment configuration file that can be tailored as needed and copied as the basis for other environments. Apart from a small number of variables required for the correct operation of the project framework, configuration files have no predefined structure. The file can contain whatever other variables are required for a given project.</p> <p>Info</p> <p>Parameters in the configuration file should not use the <code>lava</code> key. This is reserved for use by lava itself.</p> <p>Because lava specifications can contain Jinja markup intended for lava itself, this build/deploy time rendering must use non-standard Jinja delimiters to avoid a clash. For build/deploy time parameter injection, use the following Jinja delimiters.</p> <ul> <li> <p><code>&lt;{...}&gt;</code> instead of <code>{{...}}</code></p> </li> <li> <p><code>&lt;#...#&gt;</code> instead of <code>{#...#}</code></p> </li> <li> <p><code>&lt;%...%&gt;</code> instead of <code>{% %}</code></p> </li> </ul>"},{"location":"11-lava-job-framework.html#built-in-rendering-variables","title":"Built-in Rendering Variables","text":"<p>In addition to the parameters defined in the environment configuration file, the following variables are also made available to the renderer.</p> Name Type Description env str The name of the configuration file (without the <code>.yaml</code> suffix). jinja.ctime str Current local date/time in ctime(3) format. jinja.datetime str Current local date/time in YYYY-mm-dd HH:MM:SS format. jinja.iso_datetime str Current date/time in ISO8601 format. jinja.prog str Name of the rendering program. jinja.templates list[str] A list of the files being rendered. For framework files, <code>jinja.templates[0]</code> will be the name of the YAML source file relative to the enclosing <code>lava-*</code> directory. jinja.user str Current user name. jinja.utc_ctime str Current UTC date/time in ctime(3) format. jinja.utc_datetime str Current UTC date/time in YYYY-mm-dd HH:MM:SS format. lava.aws.account str The AWS account ID. lava.aws.arn() function Helper function to assist with constructing AWS ARNs for a limited range of AWS resource types. See below. lava.aws.ecr_uri str The base URI for the ECR registry. The repository name needs to be appended to get the repository URI. lava.aws.region str The AWS region (e.g. <code>ap-southeast-2</code>). lava.aws.user str The AWS user or role name (e.g. <code>user/fred</code>). lava.dag() function A helper function for building DAG payloads. lava.realm dict[str,*] The realms table entry for the target realm. e.g. <code>&lt;{ lava.realm.s3_temp.split('/')[2] }&gt;</code> is the realm temp bucket. <p>The <code>lava.arn(service, resource)</code> function is a helper function to generate AWS ARNs. Mostly these are required for specifying targets for Amazon EventBridge Rules. The allowed values for the <code>service</code> and <code>resource</code> arguments are:</p> service resource iam-role The name of an IAM role. lambda-function The name of an AWS Lambda function. log-group The name of a CloudWatch log group. sns-topic The name of an SNS topic. sqs-queue The name of an SQS queue. <p>For example, this will generate the ARN for the s3trigger Lambda function for the realm:</p> <pre><code>&lt;{ lava.aws.arn('lambda-function', 'lava-' + realm + '-s3trigger') }&gt;\n</code></pre>"},{"location":"11-lava-job-framework.html#jinja-template-factorisation","title":"Jinja Template Factorisation","text":"<p>In more complex projects, it is not uncommon to have several jobs or triggers containing similar or repeated material. This material can be factored out into reusable sub-templates that are included into the main component at build time.</p> <p>If these reusable sub-templates are located in one of the <code>lava-*</code> directories and have names ending in <code>.yaml</code> or <code>.json</code>, they must have names starting with an underscore, or be in a subdirectory with a name starting with underscore, otherwise the framework will attempt to process them as a complete specification in their own right.</p> <p>Jinja provides several mechanisms to facilitate such factorisation:</p> <ol> <li> <p>Template inheritance</p> </li> <li> <p>Template inclusion</p> </li> <li> <p>Template import</p> </li> <li> <p>Template self-configuration</p> </li> </ol>"},{"location":"11-lava-job-framework.html#jinja-template-inheritance","title":"Jinja Template Inheritance","text":"<p>Template inheritance allows construction of a base skeleton template that contains common elements and defines blocks that child templates can override.</p> <p>It is the most complex of the factorisation methods. Refer to the Jinja documentation for details.</p>"},{"location":"11-lava-job-framework.html#jinja-template-inclusion","title":"Jinja Template Inclusion","text":"<p>The Jinja include statement is useful to include a sub-template and return the rendered contents of that file into the current namespace. These sub-templates are rendered using the environment configuration file in exactly same way as the main component files. They can also receive variable values that are set in the parent file.</p> <p>The sub-templates can be placed in a subdirectory of the relevant lava job framework directory or in a common area elsewhere in the project tree. YAML files that are not full specification files must have names beginning with underscore or be in a subdirectory with a name beginning with an underscore if located within one of the <code>lava-*</code> directories.</p> <p>For example, consider the following s3trigger specification.</p> <pre><code>description: \"Process data received from source_a\"\ntrigger_id: \"&lt;{ prefix.s3trigger }&gt;/source_a\"\n\nenabled: true\n\njob_id: \"&lt;{ prefix.job }&gt;/process/source_a\"\n\nbucket: \"&lt;{ s3.bucket }&gt;\"\nprefix: \"source_a\"\n\nparameters:\n  vars:\n    bucket: \"{{ bucket }}\"\n    key: \"{{ key }}\"\n</code></pre> <p>This is fine if there is only a single <code>source_a</code> that needs to be handled. If new sources are added that have similar processing, the s3trigger specification will be copied multiple times with common material.</p> <p>An alternative approach is to create a new directory <code>lava-triggers/_common</code> containing the following file <code>whatever.yaml</code>. This relies on the main template to set the <code>source</code> variable.</p> <pre><code>description: \"Process data received from &lt;{ source }&gt;\"\ntrigger_id: \"&lt;{ prefix.s3trigger }&gt;/&lt;{ source }&gt;\"\n\nenabled: true\n\njob_id: \"&lt;{ prefix.job }&gt;/process/&lt;{ source }&gt;\"\n\nbucket: \"&lt;{ s3.bucket }&gt;\"\nprefix: \"&lt;{ source }&gt;\"\n\nparameters:\n  vars:\n    bucket: \"{{ bucket }}\"\n    key: \"{{ key }}\"\n</code></pre> <p>The main s3trigger specification then becomes:</p> <pre><code># Set the source for the sub-template\n# &lt;% set source='source_a' %&gt;\n\n# Load the sub-template\n# &lt;% include '_common/whatever.yaml' %&gt;\n</code></pre>"},{"location":"11-lava-job-framework.html#jinja-template-import","title":"Jinja Template Import","text":"<p>Jinja2 allows variables (and macros) to be imported from other templates using the import statement. This process is broadly similar to Python imports.</p> <p>Imported templates don\u2019t have access to the current template variables, just the globals.</p> <p>For example, consider the following file <code>vars.jinja</code>:</p> <pre><code>&lt;% set bucket='my-bucket' %&gt;\n</code></pre> <p>This can be used in YAML template thus:</p> <pre><code># &lt;% from 'vars.jinja' import bucket %&gt;\n\nbucket: \"&lt;{ bucket }&gt;\"\n</code></pre> <p>Alternatively:</p> <pre><code># &lt;% import 'vars.jinja' as v %&gt;\n\nbucket: \"&lt;{ v.bucket }&gt;\"\n</code></pre> <p>Note that because <code>vars.jinja</code> does not end in <code>.yaml</code>, the framework will not confuse it with a specification file.</p>"},{"location":"11-lava-job-framework.html#jinja-template-self-configuration","title":"Jinja Template Self-Configuration","text":"<p>The Jinja rendering process is aware of the name of the source file being rendered and makes this name available for use in the rendering process as the expression <code>jinja.templates[0]</code>. For example, if the source file is <code>lava-jobs/dir/file.yaml</code>, this expression will have the value <code>dir/file.yaml</code>.</p> <p>This allows the contents of the created DynamoDB object to be dependent on the name of the file.</p> <p>Here is a simple example of how this can be used to create a generic job that avoids embedding specific configuration details.</p> <pre><code># Assume the name of this file is lava-jobs/my-db/my-schema/my-table/count.yaml\n\n# Extract database, schema and table names:\n# &lt;% set db=jinja.templates[0].split('/')[0] %&gt;\n# &lt;% set schema=jinja.templates[0].split('/')[1] %&gt;\n# &lt;% set table=jinja.templates[0].split('/')[2] %&gt;\n\n# Now we can use these in our job spec\n\ndescription: \"Count rows in &lt;{ schema }&gt;.&lt;{ table }&gt; in database &lt;{ db }&gt;\"\njob_id: \"&lt;{ prefix.job }&gt;/count/&lt;{ db }&gt;/&lt;{ schema }&gt;.&lt;{ table }&gt;\"\ntype: sqli\nowner: \"&lt;{ owner }&gt;\"\n\ndispatcher: \"&lt;{ dispatcher.main }&gt;\"\nworker: \"&lt;{ worker.main }&gt;\"\nenabled: true\n\npayload: \"SELECT count(*) FROM &lt;{ schema }&gt;.&lt;{ table }&gt;\"\nparameters:\n  # Lookup a table in the config file to convert db to a connector ID\n  conn_id: \"&lt;{ db_conn_table[db] }&gt;\"\n</code></pre> <p>This is probably overkill for handling a single table in a single database. However, if the same action is required for multiple tables, the same specification can be copied without modification, provided the file naming structure is setup correctly. Alternatively, symlinks can be used to avoid multiple copies of the same specification. Like so:</p> <pre><code>lava-jobs\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 _common\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 count.yaml\n\u251c\u2500\u2500 db1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 schema1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 table1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 count.yaml -&gt; ../../../_common/count.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 schema2\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 table2\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 count.yaml -&gt; ../../../_common/count.yaml\n\u2514\u2500\u2500 db2\n    \u2514\u2500\u2500 schema3\n        \u2514\u2500\u2500 table3\n            \u2514\u2500\u2500 count.yaml -&gt; ../../../_common/count.yaml\n</code></pre>"},{"location":"11-lava-job-framework.html#creating-dynamodb-items","title":"Creating DynamoDB Items","text":"<p>The <code>lava-connections</code>, <code>lava-jobs</code> and <code>lava-triggers</code> directories will contain the source for the lava specification components for the project. The source can be either JSON (<code>*.json</code>) or YAML (<code>*.yaml</code>) files that will be converted into JSON and pushed to the appropriate DynamoDB table as part of the deployment process. It is strongly recommended to use YAML as it is easier to write, read and annotate with comments.</p> <p>There are samples for the various DynamoDB table entries available in the Lava Job Framework Samples section.</p> <p>Existing lava configuration entries can be imported from the DynamoDB tables using the lava-dump utility. The resulting files will need to be manually edited to remove realm specific settings and move those to the environment configuration file(s).</p>"},{"location":"11-lava-job-framework.html#jinja-rendering-of-dynamodb-items","title":"Jinja Rendering of DynamoDB Items","text":"<p>Jinja rendering of lava framework components as part of the build and deploy process is supported. See Jinja Rendering of Lava Framework Components.</p>"},{"location":"11-lava-job-framework.html#conditional-deployment-of-dynamodb-items","title":"Conditional Deployment of DynamoDB Items","text":"<p>In most cases, exactly the same inventory of components should be deployed to all target environments, although the contents may be environment specific. In some, limited, circumstances, some components may not need to be deployed to some target environments.</p> <p>The lava framework will skip deployment of a component if the built JSON component contains only a <code>null</code> object. This is achieved by wrapping the YAML source for the object in a Jinja conditional block like so:</p> <pre><code># &lt;% if env in ('dev', 'uat') %&gt;\ndescription: Conditional job\njob_id: maybe_yes_maybe_no\ntype: etc ...\n# &lt;% endif %&gt;\n</code></pre> <p>When this job is built for the <code>dev</code> or <code>uat</code> environments, the resulting json object will be non-null and hence the job will be installed. when it is built for the <code>prod</code> environment, the contents will generate a <code>null</code> JSON object  which will be skipped during installation.</p> <p>The conditional logic can make use of any of the configuration information made available when rendering the item, including the contents of the environment configuration file.</p>"},{"location":"11-lava-job-framework.html#examples","title":"Examples","text":"<p>The following example shows an sql job specification:</p> <pre><code># --------------------------------------\ndescription: Sample SQL job\ndispatcher: &lt;{ dispatcher.none }&gt;\nenabled: true\njob_id: &lt;{ prefix.job }&gt;/simple-sql\nowner: &lt;{ owner }&gt;\npayload: &lt;{ prefix.payload }&gt;/simple.sql\ntype: sql\nworker: &lt;{ worker.main }&gt;\n\n# Get the name of the job source file relative to lava-jobs dir.\nx-srcfile: &lt;{ jinja.templates[0] }&gt;\n\n# --------------------------------------\n# Post job actions\n# &lt;% if on_fail %&gt;\non_fail: &lt;{ on_fail }&gt;\n# &lt;% endif %&gt;\n\n# &lt;% if on_success %&gt;\non_success: &lt;{ on_success }&gt;\n# &lt;% endif %&gt;\n\n# --------------------------------------\nparameters:\n  conn_id: &lt;{ conn.mydb }&gt;\n  vars:\n    schema_name: &lt;{ schema.staging }&gt;\n</code></pre> <p>All of the values delimited by <code>&lt;{...}&gt;</code>, <code>&lt;%...%&gt;</code> will be obtained from whichever environment configuration file is used at build/deploy time.</p> <p>If the configuration file is:</p> <pre><code># --------------------------------------\n# Lava environment configuration file\n\nrealm: \"user01\"\nprefix:\n  job: \"app/demo\"\n  payload: \"app/demo\"\n  s3trigger: \"app/demo\"\nowner: \"Fred\"\nworker:\n  main: \"core\"\ndispatcher:\n  main: \"Sydney\"\n  none: \"--\"\nschedule:\n  main: \"--\"\n\n# --------------------------------------\n# Connections\nconn:\n  mydb: redshift/dev\n\n# --------------------------------------\n# Post-job actions These can be safely removed if not needed.\non_fail:\n  - action: email\n    to: fred@somewhere.com\n    subject: \"ALARM: Job={{job.job_id}}@{{realm.realm}}\"\n    message: \"Run {{job.run_id}}: {{result.error}}\"\n\n# --------------------------------------\n# Custom variables.\n\nschema:\n  staging: public\n</code></pre> <p>The final job will look like this:</p> <pre><code>{\n    \"description\": \"Sample SQL job\",\n    \"dispatcher\": \"--\",\n    \"enabled\": true,\n    \"job_id\": \"app/demo/simple-sql\",\n    \"on_fail\": [\n        {\n            \"action\": \"email\",\n            \"message\": \"Run {{job.run_id}}: {{result.error}}\",\n            \"subject\": \"ALARM: Job={{job.job_id}}@{{realm.realm}}\",\n            \"to\": \"fred@somewhere.com\"\n        }\n    ],\n    \"owner\": \"Fred\",\n    \"parameters\": {\n        \"conn_id\": \"redshift/dev\",\n        \"vars\": {\n            \"schema\": \"public\"\n        }\n    },\n    \"payload\": \"app/demo/simple.sql\",\n    \"type\": \"sql\",\n    \"worker\": \"core\"\n}\n</code></pre>"},{"location":"11-lava-job-framework.html#creating-amazon-eventbridge-rules","title":"Creating Amazon EventBridge Rules","text":"<p>Lava provides support for triggering jobs from Amazon EventBridge rules via a number of mechanisms:</p> <ul> <li> <p>Using the lava dispatch helper.</p> </li> <li> <p>Using Amazon S3 Event Notifications with Amazon EventBridge.</p> </li> </ul> <p>Also, a project may need to create EventBridge rules to interact with other non-lava elements in the environment.</p> <p>In each case, EventBridge rules with suitable targets need to be created. The lava job framework supports this with rule specifications placed in the <code>lava-rules</code> directory.</p> <p>Sorry, I couldn't resist.</p>"},{"location":"11-lava-job-framework.html#anatomy-of-eventbridge-rules","title":"Anatomy of EventBridge Rules","text":"<p>Note</p> <p>This explanation is for general information only and many details are omitted. Consult AWS documentation for full details.</p> <p>EventBridge rules are attached to an event bus (typically the <code>default</code> bus) and contain the following key components:</p> <ul> <li> <p>A rule name.</p> </li> <li> <p>A description.</p> </li> <li> <p>An optional event pattern that is matched against incoming events by     EventBridge at runtime to determine if the rule should fire or not.</p> </li> <li> <p>An optional schedule that specifies a cron style schedule or repetition     frequency for the rule to fire.</p> </li> <li> <p>Targets for the rule and a definition of what data to send to the targets.     A range of target types are supported, including Lambda functions,      CloudWatch log groups, SNS topics and SQS queues. While targets are optional,     not having any is pretty pointless.</p> </li> <li> <p>Tags for the rule.</p> </li> </ul> <p>For the lava job framework, these elements are defined in a rule specification file.</p>"},{"location":"11-lava-job-framework.html#rule-specification-files","title":"Rule Specification Files","text":"<p>Rule specification files are YAML (or JSON, if you must) formatted and placed in the <code>lava-rules</code> directory. These files are Jinja rendered against the specified environment configuration file as for other YAML job framework components and deployed to EventBridge by the lava job framework.</p> <p>A sample rule specification file is provided here.</p> <p>Each file has the following keys:</p> Key Type Required Description description str Yes A short description of the rule. enabled Boolean No Whether or not the rule is enabled. Defaults to <code>false</code> event_bus_name str No The event bus name. The default is <code>default</code>. event_pattern dict No The pattern used to select which events trigger the rule. See the AWS documentation for details. owner str Yes Name or email address of the rule owner. This will be added as a tag on the rule when deployed. role_arn str No The ARN of the IAM role associated with the rule. See the AWS documentation for details. rule_id str Yes The rule name. This must be of the form <code>lava.&lt;REALM&gt;.*</code>. schedule_expression str No A cron style schedule or repetition frequency for the rule to fire. See the AWS documentation for details. tags dict No A dictionary of key/value pairs that will be added as tags to the rule. These are additional to the <code>owner</code> and control tags added by the lava framework. targets list No A list of targets for the rule. If omitted, the rule may fire but nothing will happen. See Specifying Rule Targets."},{"location":"11-lava-job-framework.html#specifying-rule-targets","title":"Specifying Rule Targets","text":"<p>A rule target is a resource to which EventBridge sends an event message when a rule fires. Rules can have zero or more targets. Consult the AWS documentation for details.</p> <p>Rule specification files may contain the <code>targets</code> key which is a list of targets for the rule. Each entry in the list specifies the resource or endpoint and any additional parameters required for that endpoint.</p> <p>The format for each entry in the <code>targets</code> list can be either:</p> <ol> <li> <p>The ARN of a target resource.</p> </li> <li> <p>A full target specification using the structure specified for a target in     the boto3 EventBridge     put_targets     function (camel case and all). </p> </li> </ol> <p>In the first case, the incoming event is forwarded, unmodified, to the resource specified by the ARN. This is suitable for using EventBridge to trigger lava jobs from S3 events, among other uses. The lava job framework provides  Jinja helper functions to assist with constructing ARNs.</p> <p>In the second case, the specification provides full control over the target configuration, including the nature of the event message being sent.</p>"},{"location":"11-lava-job-framework.html#example-rule-specification-file","title":"Example Rule Specification File","text":"<p>The following example is typical of one used to send an S3 bucket event to the realm s3trigger lambda function to dispatch a lava job. It also logs the event to CloudWatch logs.</p> <pre><code># rule_id becomes the rule name\nrule_id: \"&lt;{ prefix.rule }&gt;.s3-rule-example\"\n\n# If you forget this, your rule is disabled.\nenabled: true\n\nowner: Fred\ndescription: A sample rule\n\ntags:\n  project: my-great-project\n\n# This will capture object creation in s3://my-bucket/an/interesting/prefix\nevent_pattern:\n  detail:\n    bucket:\n      name:\n        - my-bucket\n    object:\n      key:\n        - prefix: an/interesting/prefix\n  detail-type:\n    - Object Created\n  source:\n    - aws.s3\n\ntargets:\n\n  # Construct the ARN for the realm s3trigger lambda\n  - &lt;{ lava.aws.arn('lambda-function', 'lava-' + realm + '-s3trigger') }&gt;\n  # Let's log messages in CloudWatch logs\n  - &lt;{ lava.aws.arn('log-group', '/aws/events/lava') }&gt;\n\n  # This does exactly the same as the previous targets using the full target\n  # format. Don't do both or s3trigger will get 2 events sent\n  - Id: trigger-me\n    Arn: &lt;{ lava.aws.arn('lambda-function', 'lava-' + realm + '-s3trigger') }&gt;\n  - Id: log-me\n    Arn: &lt;{ lava.aws.arn('log-group', '/aws/events/lava') }&gt;\n</code></pre>"},{"location":"11-lava-job-framework.html#creating-payloads","title":"Creating Payloads","text":"<p>Some job types, such as cmd, dag and sqli, have the payload fully contained within the job specification.</p> <p>For other job types, such as  exe, pkg and sql, the payload is external to the job specification, which references the payload content (e.g. as a code bundle in S3 or a docker image repository). For these, the <code>lava-payloads</code> directory will contain the source for the lava payloads for the project. The framework currently supports automated build for the following external payload types:</p> <ul> <li> <p>Python scripts (<code>*.py</code>)</p> </li> <li> <p>Jupyter notebooks (<code>*.ipynb</code>)</p> </li> <li> <p>Shell scripts (<code>*.sh</code>)</p> </li> <li> <p>SQL scripts (<code>*.sql</code>)</p> </li> <li> <p>Packages for     pkg jobs (<code>*.pkg/</code>).</p> </li> <li> <p>Docker images for     docker jobs (<code>*.docker/</code>).</p> </li> <li> <p>Resource directories     (<code>*.rsc</code> and <code>*.raw</code>).</p> </li> </ul>"},{"location":"11-lava-job-framework.html#resource-directories","title":"Resource Directories","text":"<p>Directories directly under <code>lava-payloads</code> with names ending in <code>.rsc</code> or <code>.raw</code> are static resource directories that contain no active job components but are uploaded to the payloads area in S3 for consumption by lava jobs as required.</p> <p>Directories ending in <code>.rsc</code> will have the contents Jinja rendered at build/deploy time using the specified environment configuration file.</p> <p>Directories ending in <code>.raw</code> are not Jinja rendered.</p> <p>In either case, the directory structure is replicated in the project payload area in S3 under the <code>prefix.payload</code> item from the environment configuration. Symbolic links are followed as part of the process.</p> <p>Note that the lava worker will completely ignore these areas in S3. It is up to individual jobs to download the contents as required. For situations where static resources need to be accessed locally by a job, it may be more appropriate to place them directly in the <code>.pkg</code> or <code>.docker</code> directory so that they are included in the job payload.</p> <p>An element <code>my-file</code> from a resource directory <code>xyz.rsc</code> can be referenced in a job specification thus:</p> <pre><code>{{ realm.s3_payloads }}/&lt;{ prefix.payload }&gt;/xyz.rsc/my-file\n</code></pre>"},{"location":"11-lava-job-framework.html#dag-payloads","title":"DAG Payloads","text":"<p>The payload for dag jobs is a map representing job dependencies. The details can be included directly in the job specification.  The job framework also provides support for generating this map at build time via the following:</p> <ul> <li> <p>The lava-dag-gen utility which is     provided in the job framework <code>bin</code> directory.</p> </li> <li> <p>A Jinja function, <code>lava.dag()</code>, that calls this utility to generate and     interpolate a DAG payload at build time.</p> </li> </ul> <p>Note</p> <p>The lava framework cannot easily tell if a job using the <code>lava.dag()</code> function needs to be rebuilt as it may depend on external data. Hence, the framework will always rebuild job specifications that use this function.</p> <p>This following example shows how to use the Jinja function:</p> <pre><code>description: A daggy job\n\ntype: dag\n\njob_id: \"&lt;{ prefix.job }&gt;/dag/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\nenabled: true\nowner: \"&lt;{ owner }&gt;\"\n\nparameters:\n  workers: 2\n\n# Generate the dag payload by reading the first tab in Excel file dag.xlsx\npayload:  \"&lt;{ lava.dag('dag.xlsx'}&gt;\"\n</code></pre> <p>The first (positional) argument to the <code>lava.dag()</code> function corresponds to the <code>source</code> argument of the lava-dag-gen utility.</p> <p>The <code>lava.dag()</code> function also supports keyword arguments that match the <code>--option value</code> command line options of the  lava-dag-gen utility, although not all of these are useful in a lava framework job specification.</p> <p>The following example shows how to generate the dag payload by reading dependencies from a database using a lava connector:</p> <pre><code>description: A daggy job\n\ntype: dag\n\njob_id: \"&lt;{ prefix.job }&gt;/dag/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\nenabled: true\nowner: \"&lt;{ owner }&gt;\"\n\nparameters:\n  workers: 2\n\n# Generate the dag payload by reading a database table. Note that the realm\n# value from the framework configuration file is used.\npayload: \"&lt;{ lava.dag('a_conn_id', group='a_batch', table='a_schema.dags', realm=realm) }&gt;\"\n</code></pre> <p>Note that the <code>lava.dag()</code> function actually returns a JSON formatted string. This works in a YAML source file because valid JSON is also valid YAML. Neat eh?</p> <p>Info</p> <p>Using the <code>lava.dag()</code> function with a lava database connector requires that the lava package is installed in the framework virtual environment.</p>"},{"location":"11-lava-job-framework.html#docker-payloads","title":"Docker Payloads","text":"<p>Warning</p> <p>Lava version 8.1 (K\u012blauea) introduced some important changes in this area. It is essential to read Backward Compatibility Notes for Docker Payloads if running an earlier version.</p> <p>Directories directly under <code>lava-payloads</code> with names ending in <code>.docker</code> are assumed to contain the code for lava docker jobs.</p> <p>The build process is essentially:</p> <ol> <li> <p>Create a clean copy of the source tree.</p> </li> <li> <p>Any files in the <code>env/</code> directory of the source tree are Jinja rendered     using the environment configuration file. This provides one possible     mechanism to include environment specific information in the build.</p> </li> <li> <p>Any Jupyter notebooks (<code>*.ipynb</code>) are converted to Python.</p> </li> <li> <p>If the source directory already contains a <code>Dockerfile</code>, that will be Jinja     rendered using the environment configuration file and used to build the     image.</p> </li> <li> <p>If the source directory does not contain a <code>Dockerfile</code>, a     default one is used.</p> </li> </ol> <p>Info</p> <p>These components are placed in the container in <code>/lava</code> and owned by the user <code>lava</code>. However, by default, the container will be run with the effective user ID of the lava worker. This is required so that any items left by the container in the $LAVA_TMP area can be read by the worker. Take care when building containers to account for the different user IDs at build and run time.</p> <p>The install process will create an appropriate ECR repo and push the image. The uninstall process will delete the ECR repo.</p>"},{"location":"11-lava-job-framework.html#the-default-dockerfile","title":"The Default Dockerfile","text":"<p>The default <code>Dockerfile</code> supplied with the framework should suffice in most cases. It effectively emulates the packaging process for pkg payloads but builds a docker image instead of a zip file.</p> <p>The payload files are installed in the <code>/lava</code> directory in the container. The files are owned by root and are globally readable inside the container. Any files that are user executable in the source directory are made globally executable inside the container.</p> <p>Info</p> <p>The <code>/lava</code> directory is not added to any <code>*PATH</code> environment variables by default.</p> <p>If the root directory of the source tree contains a <code>requirements.txt</code> file, then Python modules listed therein, including any dependencies, are installed as part of the image build. If the root directory contains a <code>requirements-nodeps.txt</code> file, then Python modules listed therein, excluding any dependencies, are included.</p> <p>If the default <code>Dockerfile</code> is not adequate, a custom one can be created. A simple <code>Dockerfile</code> might look something like the following, but keep in mind the runtime configuration to ensure permissions are set correctly inside the container when building the image.</p> <pre><code>FROM ghcr.io/jin-gizmo/lava/amzn2023/base\n\n# Copy our code into the image\nCOPY * /install/\n\n# Point at the right pip repo. The Makefile will supply the value.\nARG PIP_INDEX_URL\nENV PIP_INDEX_URL $PIP_INDEX_URL\n\nRUN \\\n    cd /install ; \\\n    echo My code is here ; \\\n    ls -lR : \\\n    python3 -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"11-lava-job-framework.html#docker-platform-architecture-selection","title":"Docker Platform Architecture Selection","text":"<p>As of version 8.1 (K\u012blauea), the lava job framework supports building docker payload images for a specific target platform architecture.</p> <p>Info</p> <p>Currently, the capability to generate cross-platform images is only supported when using Docker Desktop with multi-platform support enabled.</p> <p>Image platform selection is controlled by the <code>docker-&gt;platform</code> key in the environment configuration file. This key may have one of the following values.</p> Docker platform Description <code>host</code> Use the default behaviour of the build host docker platform. The platform selected will be dependent on some combination of the architecture of the base image and the build host, as is usual for docker. <code>linux/amd64</code> Build an image for x86_64 platforms. <code>linux/arm</code> Build an image for ARM platforms, such as Mac M series and AWS Graviton. unspecified Build an image for x86_64 platforms. <p>For a cross-platform build to work as expected, the base image must either be a multi-platform image or have itself been built for the target platform. Most standard operating system base images, such as Amazon Linux 2023 and Ubuntu  Linux are multi-platform. As of version 8.1 (K\u012blauea), the lava docker images are also multi-platform.</p>"},{"location":"11-lava-job-framework.html#compatibility-notes-for-docker-payloads","title":"Compatibility Notes for Docker Payloads","text":"<p>This is a bit complicated but please bear with me ...</p> <p>To understand platform compatibility when deploying a docker payload, the fundamental principle is that the docker image must contain a platform version that matches the host running the lava worker.</p> <p>If lava workers are being run on x86 AWS EC2 instances (<code>linux/amd64</code> in docker terminology), job payload docker images must be, or contain, a <code>linux/amd64</code> version.</p> <p>This, in turn, implies that the base image for the payload is either:</p> <ol> <li> <p>A single platform <code>linux/amd64</code> image; or</p> </li> <li> <p>A multi-platform image that</p> <ul> <li>includes a <code>linux/amd64</code> platform version; and</li> <li>the build process, implicitly or explicitly, directs the use of the     <code>linux/amd64</code> platform version.</li> </ul> </li> </ol> <p>If every machine in the dev / build / run chain is x86, no problems. That was the world view for lava versions prior to version 8.1. The lava docker images, commonly used as payload base images, were built only for x86. Any derived images would inevitably be x86.</p> <p>Unfortunately, if a multi-platform base image, such as any of the common operating system base images, was used on a M-series Mac build machine, the result would be an ARM (<code>linux/arm64</code>) payload which would not run on an x86 AWS EC2 worker. The lava job framework provided no way to specify what output architecture was required.</p> <p>It also meant that the lava docker images could not run on ARM machines, except under emulation.</p> <p>Lava version 8.1 (K\u012blauea) introduced some key changes in this area:</p> <ol> <li> <p>The lava docker    images are multi-platform images supporting x86    (<code>linux/amd64</code>) and ARM (<code>linux/arm64</code>).</p> </li> <li> <p>The lava job framework includes the ability to explicitly specify the target     platform for docker payloads, rather than relying on some implicit     combination of the platform types available in the base image and the     platform type of the build host.</p> </li> </ol> <p>So far, so good.</p> <p>New projects using the v8.1 lava job framework allow the user to control the target platform using the <code>docker-&gt;platform</code> key in the environment configuration file. It defaults to <code>linux/amd64</code>. This should work fine on x86 and M-series Mac build machines using Docker Desktop with emulation.</p> <p>What happens when working with existing projects using an older version of the lava job framework? I hear you ask. It depends:</p> <ol> <li> <p>Existing, deployed docker payloads and projects without docker payloads.   No impact.</p> </li> <li> <p>Rebuilding and deploying docker payloads from an x86 build host.   No impact.</p> </li> <li> <p>Rebuilding and deploying from an ARM build host (e.g. M-series Mac)   This (probably) would have worked prior to v8.1. Now, it will not. The lava    job framework version must be updated to v8.1 (or later). See Updating the    Framework in an Existing    Project. The    <code>docker-&gt;platform</code> key should be added to the <code>config/*.yaml</code> files, but will    default to <code>linux/amd64</code> if not present.</p> </li> </ol>"},{"location":"11-lava-job-framework.html#exe-payloads","title":"Exe Payloads","text":"<p>Single file Python and Shell scripts directly under <code>lava-payloads</code> are copied as is when deployed.</p> <p>SQL scripts are Jinja rendered at build/deploy time using the specified environment configuration file, in the same way as the DynamoDB table specifications.</p> <p>Jupyter notebooks are converted to Python scripts for deployment.</p>"},{"location":"11-lava-job-framework.html#pkg-payloads","title":"Pkg Payloads","text":"<p>Directories directly under <code>lava-payloads</code> with names ending in <code>.pkg</code> are assumed to contain the code for lava pkg jobs.</p> <p>The build process is essentially:</p> <ol> <li> <p>Create a clean copy of the source tree.</p> </li> <li> <p>Any files in the <code>env/</code> directory of the source tree are Jinja rendered     using the environment configuration file. This provides one possible     mechanism to include environment specific information in the build.</p> </li> <li> <p>Any Jupyter notebooks (<code>*.ipynb</code>) are converted to Python.</p> </li> <li> <p>If the root directory of the source tree contains a     <code>requirements.txt</code> file, then Python modules listed therein, including any     dependencies, are included.</p> </li> <li> <p>If the root directory contains a <code>requirements-nodeps.txt</code> file, then Python     modules listed therein, excluding any dependencies, are included.</p> </li> <li> <p>Zip up everything and place it in the <code>dist</code> area of the project.</p> </li> </ol>"},{"location":"11-lava-job-framework.html#miscellaneous-components","title":"Miscellaneous Components","text":"<p>Lava jobs sometimes require other components that may, or may not, be deployed as part of a job but which don't naturally belong in the lava payloads area in S3.</p> <p>For example, jobs may require some tables to be pre-created before the job runs. The SQL to create the tables would be one such miscellaneous component. Another example might be JSONPath files for a Redshift COPY operation for JSON data.</p> <p>These components can be placed in the <code>misc</code> (miscellaneous) directory.</p> <p>Any SQL scripts (<code>*.sql</code>) placed in the <code>misc</code> directory are Jinja rendered at build/deploy time into the <code>dist</code> directory using the specified environment configuration file, in the same way as the DynamoDB table specifications.</p> <p>By default, no other build or installation action is performed for anything in the <code>misc</code> directory.</p> <p>Info</p> <p>Do not edit <code>misc/Makefile</code> as this file will be replaced in the event of a framework update.</p> <p>If some additional build or installation action is required, the appropriate means to achieve this is to create a custom makefile <code>Makefile.local</code>. This will be detected by the framework and invoked. This makefile must implement the following targets, although they don't have to do anything if not required:</p> <ul> <li>dist</li> <li>pre-install</li> <li>install</li> <li>uninstall.</li> </ul> <p>The recommended approach is to copy the file <code>misc/Makefile.local.sample</code> to <code>misc/Makefile.local</code> and customise as required.</p>"},{"location":"11-lava-job-framework.html#building-the-deployable-components","title":"Building the Deployable Components","text":"<p>Once the lava components are created, the installable components are created thus:</p> <pre><code># cd to the project root directory then ...\n\n# Activate the virtualenv\nsource venv/bin/activate\n\n# Build the lava artefacts\nmake dist env=&lt;ENV&gt;\n</code></pre> <p>The value of the <code>env</code> parameter must correspond to one of the environment configuration YAML files in the <code>config</code> directory.</p> <p>The deployable components will be built and placed in the <code>dist/&lt;ENV&gt;</code> directory.</p>"},{"location":"11-lava-job-framework.html#installing-deployable-components","title":"Installing Deployable Components","text":"<p>The lava components can be installed using:</p> <pre><code># cd to the project root directory then ...\n\n# Activate the virtualenv\nsource venv/bin/activate\n\n# Deploy the lava artefacts\nmake install env=&lt;ENV&gt;\n</code></pre> <p>This will do the following:</p> <ol> <li> <p>Build any out of date artefacts.</p> </li> <li> <p>Perform some basic pre-installation checks (e.g. verify permission to write     to the payloads area in S3).</p> </li> <li> <p>Backup any existing payloads in the realm S3 bucket under the <code>__bak__</code>     prefix.</p> </li> <li> <p>Deploy the DynamoDB table entries and payload components.</p> </li> </ol> <p>Warning</p> <p>No backup is made of existing DynamoDB entries prior to uploading new ones.</p> <p>To perform an installation without the pre-installation checks use:</p> <pre><code># Deploy the lava artefacts without pre-install checks.\nmake _install env=&lt;ENV&gt;\n</code></pre>"},{"location":"11-lava-job-framework.html#uninstalling-deployable-components","title":"Uninstalling Deployable Components","text":"<p>The lava components can be uninstalled using:</p> <pre><code># cd to the project root directory then ...\n\n# Activate the virtualenv\nsource venv/bin/activate\n\n# Remove the lava artefacts\nmake uninstall env=&lt;ENV&gt;\n</code></pre> <p>To clean up the local <code>dist</code> area:</p> <pre><code># cd to the project root directory then ...\n\nmake clean\n</code></pre>"},{"location":"11-lava-job-framework.html#health-checking-deployable-components","title":"Health Checking Deployable Components","text":"<p>See also Maintaining DynamoDB Table Entries.</p>"},{"location":"11-lava-job-framework.html#code-hygiene","title":"Code Hygiene","text":"<p>The lava job framework incorporates some basic code health checks. The checks can be run using:</p> <pre><code>make check\n\n# or ...\n\netc/git-hooks/pre-commit\n</code></pre> <p>The checks are also run prior to any installation process. Installation is blocked if the checks fail.</p> <p>If the framework was used to automatically initialise Git for the project then the checking process is also configured as a pre-commit hook.</p> Check Type Tool Description Python quality flake8 Performs a range of PEP8 compliance and other code health checks, including compliance with black formatting. The configuration file for flake8 is contained in <code>.flake8</code> and for black in <code>pyproject.toml</code>. YAML correctness yamllint Performs correctness and style checks on the project YAML files. The configuration file is in <code>.yamllint.yaml</code>. Config alignment Builtin Compares the key structures in the configuration files in the <code>config</code> directory and highlights any differences. Generally, configuration files for a project correlate to different target realms (e.g. test vs prod). While the configuration values will vary by environment, the key hierarchies should be identical. The only configuration option is the choice between <code>warning</code> and <code>strict</code> modes which is specified in <code>etc/git-hooks/pre-commit</code>. <p>The following command will apply black formatting to project Python files:</p> <pre><code>make black\n\n# or ...\n\nblack lava-payloads misc\n\n# or even ...\n\nblack\n</code></pre>"},{"location":"11-lava-job-framework.html#configuration-drift-detection","title":"Configuration Drift Detection","text":"<p>Changes to a lava job framework based project should always be done via a <code>make install</code> from an appropriately managed Git repo to ensure that the deployed components are fully aligned with the committed contents of the repo.</p> <p>Deviation from this practice can result in misalignment between deployed components and the repo contents; aka drift.</p> <p>The lava job framework supports drift detection for the DynamoDB table entries. To detect differences between the repo contents and the deployed table entries, run the following command:</p> <pre><code>make diff env=...\n</code></pre> <p>Note that fields starting with <code>x-</code> / <code>X-</code> are excluded from drift comparisons.</p>"},{"location":"11-lava-job-framework.html#updating-the-framework-in-an-existing-project","title":"Updating the Framework in an Existing Project","text":"<p>The lava framework can be updated for an existing project by obtaining the new framework package <code>lav-job-framework-&lt;NEW-VERSION&gt;.zip</code> and applying it over the top of the project.</p> <p>This process is automated by the framework itself. A backup is made first as part of the process in case of problems. However it is strongly recommended to do a <code>git commit</code> and <code>git push</code> before starting the process.</p> <p>The update process is relatively straightforward when updating from a framework version of 5.1.0 (Tungurahua) or above. Updating earlier versions is possible with a little bit of fiddling.</p>"},{"location":"11-lava-job-framework.html#updating-from-lava-version-510-tungurahua-or-above","title":"Updating from Lava Version 5.1.0 (Tungurahua) or Above","text":"<p>The process is:</p> <pre><code># Go to the project root directory. Then ...\n# Commit and push your code just in case. Then ...\n# Deposit the new package at the root of the project directory. Then ...\n\n# Activate the virtual environment\nsource venv/bin/activate\n\n# Run the update process\nmake update pkg=lav-job-framework-&lt;NEW-VERSION&gt;.zip\n</code></pre> <p>This will do a backup of the project into a zip file, rerun the cookiecutter using the new package and apply the new framework components over the existing project.</p>"},{"location":"11-lava-job-framework.html#updating-from-lava-versions-prior-to-510-tungurahua","title":"Updating from Lava Versions Prior to 5.1.0 (Tungurahua)","text":"<p>The process is:</p> <pre><code># Go to the project root directory. Then ...\n# Commit and push your code just in case. Then ...\n# Deposit the new package at the root of the project directory. Then ...\n\n# Extract the `bin` directory from the new framework package\n# The quotes are important here.\nunzip -j -d bin lav-job-framework-&lt;NEW-VERSION&gt;.zip '*bin/*'\nchmod u+x bin/*\n\n# Activate the virtual environment\nsource venv/bin/activate\n\n# Run the update process\nPATH=$(pwd)/bin:$PATH make update pkg=lav-job-framework-&lt;NEW-VERSION&gt;.zip\n</code></pre> <p>Note that later versions of the framework move the framework's <code>requirements.txt</code> file into the <code>etc</code> directory. After the update the <code>requirements.txt</code> in the base directory can be deleted if there are no locally added packages. If there are, only those packages need to be retained in that file.</p>"},{"location":"12-developing-lava-jobs.html","title":"Developing Lava Jobs","text":""},{"location":"12-developing-lava-jobs.html#executable-jobs","title":"Executable Jobs","text":"<p>Executable jobs are handled by cmd, exe, pkg and docker job types.</p> <p>Executable scripts (bash, Python, Perl etc), as well as worker compatible binaries, are fine for use in lava. The information in this section is applicable to all of these.</p> <p>For Python based jobs, additional capabilities are provided by direct access to the lava packages.</p> <p>The run-time environment for executable jobs in lava is a conventional Linux environment based on the worker on which the job runs.</p> <p>For docker jobs, some details may depend on the nature of the container being run. Refer to the chapter on lava and docker for more information.</p> <p>The main peculiarities associated with lava based executable jobs are outlined below.</p>"},{"location":"12-developing-lava-jobs.html#executable-scripts","title":"Executable Scripts","text":"<p>Lava relies on the hashbang line at the beginning of the script to determine the appropriate interpreter in exactly the same way that a UNIX shell does.</p> <p>Beware DOS</p> <p>If the script has been edited on a DOS system, it is very likely that it will have DOS style CRLF line endings instead of UNIX style LF endings. This will prevent the hashbang line from being recognised and the job will fail.</p>"},{"location":"12-developing-lava-jobs.html#handling-of-temporary-files","title":"Handling of Temporary Files","text":"<p>Lava jobs are run in a temporary directory created by lava and deleted by lava when the job exits. </p> <p>The <code>TMPDIR</code> environment variable is set for cmd, exe and pkg jobs to point within the private run area for the job rather than inheriting the default system setting. This variable can be referenced explicitly in a job. Alternatively, the mktemp(1) command line utility or the Python tempfile module can be used as these will use <code>TMPDIR</code> if used correctly.</p> <p>Info</p> <p>The following applies to Linux. Note that macOS mktemp(1) behaves very differently in a number of ways, including using of <code>TMPDIR</code>.</p> <p>Typical usage in a shell is:</p> <pre><code>#!/bin/bash\n\n# Create a temp file in our private job area\nMY_TMP_FILE=$(mktemp)\n\n# Create a temp directory in our private job area\nMY_TMP_DIR=$(mktemp -d)\n\n# Create a temp directory using a name template. The -t is critical here.\nMY_TMP_DIR2=$(mktemp -d -t tmp-XXXXXX)\n</code></pre> <p>Typical usage in Python is:</p> <pre><code>#!/usr/bin/env python3\n\nimport tempfile\n\n# Create a tempfile. The file is open.\ntmp_file_descriptor, tmp_file_name = tempfile.mkstemp()\n\n# Create a temp directory.\ntmp_dir_name = tempfile.mkdtemp()\n</code></pre> <p>While lava will clean these up when the job exits, it is still good practice for jobs to clean up after themselves. Jobs should generally avoid creating temporary objects in <code>/tmp</code> because lava will not clean these up and there are no guarantees about availability of storage space in <code>/tmp</code>.</p>"},{"location":"12-developing-lava-jobs.html#testing-if-running-in-lava","title":"Testing if Running in Lava","text":"<p>Sometimes it's necessary for an executable to test whether or not it is running in lava. The easiest way to do this is to look for the presence of one of the lava environment variables <code>LAVA_REALM</code> or <code>LAVA_JOB_ID</code>.</p> <p>In a bash script, this would look like:</p> <pre><code>#!/bin/bash\n\nif [ \"$LAVA_REALM\" != \"\" ]\nthen\n    # We are in lava\n    echo \"I lava you\"\nelse\n    # We are not in lava\n    echo \"I don't lava you anymore\"\nfi\n</code></pre>"},{"location":"12-developing-lava-jobs.html#handling-of-stdin-stdout-and-stderr","title":"Handling of stdin, stdout and stderr","text":"<p>For executable jobs, stdin is redirected from <code>/dev/null</code> while stdout and stderr are captured and uploaded to the realm temporary area in S3 unless the worker is running with the <code>--dev</code> option. In that case, stdout and stderr are emitted locally on the worker.</p>"},{"location":"12-developing-lava-jobs.html#exit-status","title":"Exit Status","text":"<p>Lava assumes that a zero exit status indicates that the job has succeeded. This will trigger any <code>on_success</code> job actions.</p> <p>A non-zero exit status indicates to lava that the job has failed. This will trigger any <code>on_fail</code> actions.</p>"},{"location":"12-developing-lava-jobs.html#status-and-error-messages","title":"Status and Error messages","text":"<p>Executable jobs running under lava should print useful status and error messages to stdout and stderr, just as they should when running in any other environment.</p> <p>In normal operation, lava will collect this and upload it to S3 and place a pointer to it in the job event record.</p> <p>When the worker is running with the <code>--dev</code> option, stdout and stderr from the job are emitted locally rather than being sent to S3. This can help with development and debugging.</p>"},{"location":"12-developing-lava-jobs.html#connection-handling-for-executable-jobs","title":"Connection Handling for Executable Jobs","text":"<p>Handling of connections to external resources is facilitated via small executables created by lava to effect the connection. The path to the connector executable is passed to the lava job executable as an environment variable.</p> <p>The following example shows how this would be used in a shell script to access a database connection, but the mechanism is generic and available to any executable that can read environment variables and invoke an external program.</p> <pre><code>#!/bin/bash\n\n# The following environment variables are set in the Lava exe job specification.\n# The can be as many connections to different resources as is required.\n#\n# LAVA_CONN_AURORA01\n#    Lava connector script for the \"aurora01\" database. The lava job spec must\n#    have an \"aurora01\" connections entry.\n\n# SQL to do something\nSQL=\"....\"\n\n# Run a command line SQL client that is preconfigured for auto login.\n$LAVA_CONN_AURORA01 --database=dbname -e \"$SQL\" &gt; local-temp-file\n\n# Now do something clever with the results.\n</code></pre> <p>Python based executable jobs have additional options for handling connections by virtue of programmatic access to the underlying lava connection manager.</p>"},{"location":"12-developing-lava-jobs.html#python-executable-jobs","title":"Python Executable Jobs","text":"<p>In addition to the facilities available to all executable jobs, additional functionality is available for Python jobs as lava itself is Python based.</p> <p>Python based executable jobs can interface directly with the lava Python base package. This provides access to the lava connection manager as well as other modules included with lava.</p> <p>See Installing Lava Locally.</p>"},{"location":"12-developing-lava-jobs.html#connection-handling-for-python-based-jobs","title":"Connection Handling for Python Based Jobs","text":"<p>Python based lava programs can invoke the lava connection manager directly.  See lava.connections in the API documentation for more information.</p> <p>This is a simple example showing how to create a connection to an SQL-based database:</p> <pre><code>import os\nfrom lava.connection import get_pysql_connection\n\n# As we are using the lava connection manager, we just need the connection ID\n# this time \u2013 not the connector script. So use LAVA_CONNID_DB not LAVA_CONN_DB.\n\nconn_id = os.environ['LAVA_CONNID_DB']\nrealm = os.environ['LAVA_REALM']\n\n# Get a standard DBAPI 2.0 connection\nconn = get_pysql_connection(conn_id, realm)\n\n# Knock yourself out with SQL wizardry\u2026\ncursor = conn.cursor()\n...\nconn.close()\n</code></pre>"},{"location":"12-developing-lava-jobs.html#connection-handling-for-sqlalchemy","title":"Connection Handling for SQLAlchemy","text":"<p>The SQL database connectors provide native support for SQLAlchemy. An SQLAlchemy engine can be created using a lava connector to manage the underlying connection process.</p> <p>This is useful, not just for using SQLAlchemy natively, but also for packages such as pandas that rely on SQLAlchemy for database interaction.</p> <p>The following example shows how this would be used.</p> <pre><code>import os\nimport pandas as pd\nfrom lava.connection import get_sqlalchemy_engine\n\n# As we are using the lava connection manager, we just need the connection ID\n# this time \u2013 not the connector script. So use LAVA_CONNID_DB not LAVA_CONN_DB.\n\nconn_id = os.environ['LAVA_CONNID_DB']\nrealm = os.environ['LAVA_REALM']\n\nengine = get_sqlalchemy_engine(conn_id, realm)\n# engine is a standard SQLAlchemy engine.\n\nwith engine.connect() as conn:\n    for row in conn.execute('... an SQL query ...'):\n        print(row)\n\n# Or use with pandas\ntable_df = pd.read_sql_table('my_table', con=engine)\n</code></pre> <p>Note</p> <p>There is a known issue for SQLAlchemy and pg8000. See the workaround.</p>"},{"location":"12-developing-lava-jobs.html#database-connections-the-good-the-bad-and-the-ugly-of-dbapi-20","title":"Database Connections - The Good, the Bad and the Ugly of DBAPI 2.0","text":"<p>Aaaah\u2013aaaah\u2013aaah\u2013aaaah\u2026 Wah\u2013wah\u2013wahhhh\u2026</p> <p>(Don't tell me you don't know)</p> <p>Lava uses DBAPI 2.0 based database drivers, the interface for which is specified in PEP 249.</p>"},{"location":"12-developing-lava-jobs.html#the-good","title":"The Good","text":"<p>DBAPI 2.0 provides some level of interface consistency across database types. In simple cases, you only need to invoke the lava <code>get_pysql_connection()</code> function as described above to obtain a database connection which can be used to execute queries in a more or less consistent way across database types. But ...</p>"},{"location":"12-developing-lava-jobs.html#the-ugly","title":"The Ugly","text":"<p>While DBAPI 2.0 provides <code>Connection.commit()</code> and <code>Connection.rollback()</code> functions, it does not provide a <code>Connection.begin()</code> function to start a transaction and driver implementations can differ in how they handle this. (Most, but not all, handle this by setting <code>Connection.autocommit</code>). Different databases also use different SQL syntax to begin a transaction. Oracle is notable in that it does not support <code>BEGIN TRANSACTION</code> in the way that Postgres and MySQL do.</p> <p>To avoid this problem, lava provides a helper function lava.lib.db.begin_transaction(). <pre><code>from lava.connection import get_pysql_connection\nfrom lava.lib.db import begin_transaction\n\nconn = get_pysql_connection(...)\ncursor = conn.cursor()\n\ntry:\n    begin_transaction(conn, cursor)\n    # Do some SQL stuff\nexcept Exception:\n    conn.rollback()\nelse:\n    conn.commit()\nfinally:\n    conn.close()\n</code></pre></p>"},{"location":"12-developing-lava-jobs.html#the-bad","title":"The Bad","text":"<p>PEP 249 defines 5 different possible mechanisms for passing query parameters when a query is executed.</p> <p>This is because it is absolutely critical for a standard to have 5 incompatible ways of doing exactly the same thing.</p> <p>Unfortunately there is no consistency across different drivers as to which subset of these is implemented or the default setting. The <code>paramstyle</code> module constant will specify the default mechanism.</p> <p>Some drivers, such as pg8000, allow the <code>paramstyle</code> constant to be set to different values to support different parameter passing styles. Some don't.</p> <p>It's a bit of a mess unfortunately and makes writing driver independent code inordinately difficult. You either need to test the module's <code>paramstyle</code> setting and adapt the parameter passing mechanism at run-time or just make do with the specific driver settings.</p>"},{"location":"12-developing-lava-jobs.html#dbapi-20-usage-in-lava","title":"DBAPI 2.0 Usage in Lava","text":"<p>Lava uses the following drivers by default. Check the documentation for the driver for more details.</p> Database Family Driver MSSQL pyodbc MySQL PyMySQL Oracle cx_Oracle Postgres pg8000 Redshift pg8000 SQLite3 sqlite3 <p>Python code either must be sophisticated enough to adapt to the DBAPI 2.0 variations at run-time or must have specific knowledge of which driver is being used. Using SQLAlchemy instead of the native interface may be of assistance in the former option.</p> <p>Lava also provides limited support to select an alternate driver for some database types. This is done using the <code>subtype</code> field in the database connection specification. Refer to individual connectors for details.</p>"},{"location":"12-developing-lava-jobs.html#sql-jobs","title":"SQL Jobs","text":"<p>The sql, sqlc, sqli and sqlv jobs will run SQL commands against a target RDBMS.</p> <p>There is nothing special that needs to be done with the SQL to prepare it to run with lava but it is important to keep the following in mind:</p> <ul> <li> <p>Lava will manage all of the connectivity to the database.</p> </li> <li> <p>The SQL must match the syntax requirements of the target database.</p> </li> <li> <p>sqlc jobs use the command line client     specific to the target database. Typically these will support some client     specific meta commands to control behaviour of the client. These can be used     in the job payload script.</p> </li> <li> <p>sqlc and     sqlv jobs have a timeout that can be     configured in the job specification. sql     and sqli jobs do not have a timeout. Like     all jobs, the visibility timeout on the worker queue needs to be kept in     mind.</p> </li> <li> <p>If the queries return data, this will be placed into the temporary area in     S3. Some other process may need to do something with this data.</p> </li> </ul>"},{"location":"13-jinja-rendering-in-lava.html","title":"Jinja Rendering in Lava","text":"<p>Lava uses Jinja extensively to provide flexibility in the preparation of job payloads, parameters and post-job actions.</p> <p>The full range of Jinja syntax is available, including attribute value substitutions using <code>{{ ... }}</code> delimiters and control structures (loops, conditionals etc.) using <code>{% ... %}</code> syntax.</p> <p>Info</p> <p>Note that the lava job framework deliberately uses <code>&lt;{ ...}&gt;</code>and <code>&lt;% ... %&gt;</code> delimiters to differentiate between build-time rendering done in the framework and run-time rendering performed by the lava worker.</p> <p>The attributes made available to the renderer vary by job and action type. Typically they include elements such as:</p> <ul> <li> <p>realm specification details</p> </li> <li> <p>augmented job specification     details</p> </li> <li> <p>job run details</p> </li> <li> <p>job globals, which includes user defined globals and     globals owned and provided by lava</p> </li> <li> <p>state item values.</p> </li> </ul> <p>Attributes provided to the renderer are typically one of the following types:</p> <ul> <li> <p>Scalar values (strings, integers etc)</p> </li> <li> <p>Structured objects</p> </li> <li> <p>DateTime values</p> </li> <li> <p>Utility functions provided for     convenience.</p> </li> </ul> <p>Refer to specific job and action types for details.</p>"},{"location":"13-jinja-rendering-in-lava.html#the-augmented-job-specification","title":"The Augmented Job Specification","text":"<p>The job specification passed to the Jinja renderer for jobs and actions has the same contents as the relevant item from the jobs table with the addition of the following elements:</p> Name Type Description realm str The realm name. run_id str The run ID. state dict[str,*] The state map from the job specification, updated to replace the default values from the map with any current values obtainable from the state table. ts_dispatch datetime The timezone aware datetime when the job was dispatched. ts_start datetime The timezone aware local datetime when the job started. ts_ustart datetime The timezone aware UTC datetime when the job started."},{"location":"13-jinja-rendering-in-lava.html#working-with-structured-attributes","title":"Working with Structured Attributes","text":"<p>Some of the attributes passed to the Jinja renderer are structured objects.</p> <p>For example, the <code>realm</code> attribute is the DynamoDB map object from the realms table for the realm. This is converted to a Python dictionary and passed in that form to the Jinja renderer. Elements of this object can be referenced using standard Jinja object references.  For example, the realm name can be injected as either <code>{{ realm.realm }}</code> or <code>{{ realm[\"realm\"] }}</code>.</p>"},{"location":"13-jinja-rendering-in-lava.html#working-with-datetime-attributes","title":"Working with DateTime Attributes","text":"<p>Some of the attributes passed to the Jinja renderer are DateTime attributes. These include the <code>start</code> and <code>ustart</code> attributes that capture the job start time in local and UTC time respectively.</p> <p>Within Jinja, these become standard Python datetime.datetime objects. Jinja thus provides access to all the methods associated with a Python datetime. The strptime() method is particularly useful. For example:</p> <pre><code>{# Get the job start date in local time #}\n{{ start.strptime('%Y-%m-%d') }}\n\n{# Get the job start time in UTC as an ISOO 8601 format timestamp #}\n{{ ustart.isoformat() }}\n</code></pre> <p>It is even possible to do some elementary date calculations, although this can be accomplished more easily since lava version 4.3.0 (Volc\u00e1n Wolf) using the provided utility functions.</p> <pre><code>{# Calculate yesterday's date #}\n{{ (start.fromtimestamp(start.timestamp()-86400)).date() }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#jinja-utility-functions","title":"Jinja Utility Functions","text":"<p>Lava also provides a number of utilities to the Jinja renderer as Python runnable objects in the <code>utils</code> attribute.</p>"},{"location":"13-jinja-rendering-in-lava.html#date","title":"date","text":"<p>This is the standard Python datetime.date class.</p> <pre><code>{# The epoch #}\n{{ utils.date(year=1970, month=1, day=1) }}\n\n{# What day of the week is next year's ANZAC day? #}\n{{ utils.date(year=ustart.year + 1, month=4, day=25).strftime('%A') }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#datetime","title":"datetime","text":"<p>This is the standard Python datetime.datetime class.</p> <pre><code>{# Next new year's day #}\n{{ utils.datetime(year=ustart.year + 1, month=1, day=1) }}\n\n{# What day of the week is that? #}\n{{ utils.datetime(year=ustart.year + 1, month=1, day=1).strftime('%A') }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#dateutil","title":"dateutil","text":"<p>This is the dateutil Python module.</p> <pre><code>{# Get the date 6 months from now #}\n{{ ustart + utils.dateutil.relativedelta.relativedelta(months=6) }}\n</code></pre> <p>If the long module path gets too painful ...</p> <pre><code>{% set relativedelta = utils.dateutil.relativedelta.relativedelta %}\n{{ ustart + relativedelta(months=6) }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#parsedate","title":"parsedate","text":"<p>This is the dateutil.parser module. It is useful for parsing strings into dates.</p> <pre><code>{# Convert a string back to a datetime #}\n{{ utils.parsedate.isoparse('2020-02-11T09:30:00+11:00') }}\n</code></pre> <p>Note</p> <p>This is a legacy feature but is still supported. The alternative is <code>utils.dateutil.parser</code>.</p>"},{"location":"13-jinja-rendering-in-lava.html#path","title":"path","text":"<p>This is the standard Python os.path module.</p> <pre><code>{# \"s3://bucket.xyzzy.com/an/s3/key\" --&gt; \"key\" #}\n{{ utils.path.basename('s3://bucket.xyzzy.com/an/s3/key') }}\n\n{# Get the last component of the lava temp bucket prefix #}\n{{ utils.path.basename(realm.s3_temp) }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#re","title":"re","text":"<p>This is the standard Python re (regex) module. It is useful for extracting selected components of other render variables.</p> <pre><code>{# \"s3://bucket.xyzzy.com/an/s3/key\" --&gt; \"bucket\" #}\n{{ utils.re.search('s3://([^.]*)', 's3://bucket.xyzzy.com/an/s3/key)'.group(1) }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#s3bucket","title":"s3bucket","text":"<p>Extract the S3 bucket name component from a string.</p> <pre><code>{# \"s3://bucket.xyzzy.com/an/s3/key\" --&gt; \"bucket.xyzzy.com\" #}\n{{ utils.s3bucket('s3://bucket.xyzzy.com/an/s3/key') }}\n\n{# Get the lava temp bucket name #}\n{{ utils.s3bucket(realm.s3_temp) }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#s3key","title":"s3key","text":"<p>Extract the S3 key name component from a string.</p> <pre><code>{# \"s3://bucket.xyzzy.com/an/s3/key\" --&gt; \"an/s3/key\" #}\n{{ utils.s3key('s3://bucket.xyzzy.com/an/s3/key') }}\n\n{# Get the lava temp bucket prefix #}\n{{ utils.s3key(realm.s3_temp) }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#time","title":"time","text":"<p>This is the standard Python datetime.time class.</p>"},{"location":"13-jinja-rendering-in-lava.html#timedelta","title":"timedelta","text":"<p>This is the standard Python datetime.timedelta class.</p> <pre><code>{# Calculate yesterday's date #}\n{{ (start - utils.timedelta(days=1)).date() }}\n</code></pre>"},{"location":"13-jinja-rendering-in-lava.html#uuid","title":"uuid","text":"<p>Generate a random UUID.</p> <pre><code>{# Generate a random S3 object name #}\ns3://bucket/some/prefix/{{ utils.uuid() }}.csv\n</code></pre>"},{"location":"14-enhancing-lava.html","title":"Enhancing Lava","text":"<p>Lava can be enhanced in a number of ways:</p> <ul> <li> <p>External dispatchers</p> </li> <li> <p>Additional job types</p> </li> <li> <p>Additional post-job actions</p> </li> <li> <p>Additional connectors.</p> </li> </ul>"},{"location":"15-lava-and-docker.html","title":"Lava and Docker","text":"<p>Lava can use docker in distinct, but related ways:</p> <ol> <li> <p>Lava can run docker container based jobs using the     docker job type.</p> </li> <li> <p>The lava programs (worker, dispatcher and other tools) can run in a docker     container.</p> </li> <li> <p>The lava code bundle can be built for various target Linux versions inside     docker containers.</p> </li> </ol>"},{"location":"15-lava-and-docker.html#running-docker-in-lava","title":"Running Docker in Lava","text":"<p>The docker job type runs a docker container in more or less the same way that it runs exe and pkg jobs. In the former case, the payload is obtained as a docker image and in the latter cases as an executable or code bundle loaded from S3.</p>"},{"location":"15-lava-and-docker.html#handling-of-temporary-files-in-docker-jobs","title":"Handling of Temporary Files in Docker Jobs","text":"<p>The lava worker maps the temporary area for the job into the container and makes an additional environment variable, <code>LAVA_TMP</code>, available to point to it. Anything written to this area in the container will appear on the host. This can be useful with chain jobs as all jobs in the chain share the same temporary area on the lava worker.</p>"},{"location":"15-lava-and-docker.html#handling-of-container-logs","title":"Handling of Container Logs","text":"<p>The container uses the default container logging configuration. Logs are collected by the lava worker and uploaded to the realm temporary area in S3 unless the worker is running with the <code>--dev</code> option. In that case the logs are emitted locally on the worker.</p>"},{"location":"15-lava-and-docker.html#exit-status","title":"Exit Status","text":"<p>Lava assumes that a zero exit status from the container indicates that the job has succeeded. This will trigger any <code>on_success</code> job actions.</p> <p>A non-zero exit status indicates to lava that the job has failed. This will trigger any <code>on_fail</code> actions.</p>"},{"location":"15-lava-and-docker.html#connection-handling-for-container-based-jobs","title":"Connection Handling for Container Based Jobs","text":"<p>Connections to external resources are implemented as small executables created by lava which are mapped into the container and invoked within the container in the same way that they would be for an exe or pkg job.</p> <p>Refer to the relevant section in the chapter on developing lava jobs for more information.</p> <p>This mechanism should work with any Linux based container provided that the required CLI components are installed in the container. So for example, if a Postgres connector is required, the container must have the psql CLI installed.</p> <p>An easy way to ensure the required components are installed is to use one of the full docker images for lava. These images contain most, if not all, of the CLI components required for lava supported connection types.</p> <p>Python executables running in a docker container based on one of the docker images for lava can use the connection manager directly as described in the relevant section in the chapter on developing lava jobs. It is important to ensure that <code>/usr/local/lib/lava</code> is on the <code>PYTHONPATH</code> for the executable in the container.</p>"},{"location":"15-lava-and-docker.html#running-lava-in-docker","title":"Running Lava in Docker","text":"<p>The lava worker, dispatch and events viewer executables can themselves run in a docker container. The docker images for lava have lava pre-installed.</p>"},{"location":"15-lava-and-docker.html#the-docker-registry","title":"The Docker Registry","text":"<p>Lava obtains container images from a docker registry. The following registry options are supported:</p> <ul> <li>AWS ECR (recommended)</li> <li>Private docker registries</li> <li>Public registries.</li> </ul> <p>The lava connection manager handles the process of connecting to docker servers and docker registries. The docker connection ID can be specified at the job level or the realm level. A value specified at the job level will take precedence.</p> <p>By default, an AWS EC2 based lava worker has permissions to pull images from the following ECR repositories:</p> <ul> <li><code>dist/lava/*</code> (legacy reasons)</li> <li><code>lava/&lt;REALM&gt;/*</code></li> </ul>"},{"location":"15-lava-and-docker.html#docker-images-for-lava","title":"Docker Images for Lava","text":"<p>Note</p> <p>As of version 8.1.0 (K\u012blauea), lava supports multi-platform images suitable for use on both ARM (<code>linux/arm64</code>) and x86 (<code>linux/amd64</code>).</p> <p>Lava comes with some standard docker images that have the lava code and other essential components pre-installed. Currently available image types are:</p> Image name Description <code>ghcr.io/jin-gizmo/lava/&lt;OS&gt;/base</code> A base installation of lava but without the external components required to use CLI based connectors. <code>ghcr.io/jin-gizmo/lava/&lt;OS&gt;/full</code> A full installation of lava that includes the external components required to use CLI based connectors. <p>Images are tagged <code>latest</code>. A tag matching the version of the lava code is also applied.</p> <p>Currently available O/S types for the lava images are:</p> O/S Description amzn2023 An Amazon Linux 2023 based image. The full version of the image includes all CLI based connectors, including Oracle sql*plus. ubuntu24 A standard Ubuntu Linux 24.04 (LTS) based image. The full version of the image includes all CLI based connectors, including Oracle sql*plus. <p>Info</p> <p>They all have the following as of v8.1 (K\u012blauea)</p> <ul> <li>Python 3.12</li> <li>OpenSSL 3 or later</li> <li>AWS CLI version 2</li> </ul> <p>Instructions on building the images can be found in the section on lava installation.</p>"},{"location":"16-commands-utilities.html","title":"Lava Commands and Utilities","text":"<p>The lava utilities are included in the jinlava Python package together with the lava APIs.</p> <p>Lava includes a number of CLI commands and utilities. Some of these are used by the lava worker itself while others are support tools. Most of them can be used stand-alone or invoked by lava exe, pkg and docker jobs.</p> <p>All of the utilities support a <code>-h</code> , <code>--help</code> option.</p> Utility Description jinja Render a Jinja template with specified parameters. lava-ami Manage the AMIs used in lava worker CloudFormation stacks. lava-backup Backup the DynamoDB entries for a specified lava realm. lava-check Perform some basic health checks on DynamoDB table entries. lava-checksum Set and validate checksums on lava DynamoDB table entries. lava-conn-usage Generate an activity map of specified jobs over a defined time period. lava-dag-gen Generate a DAG specification for lava dag jobs from a dependency matrix. lava-dispatcher The lava job dispatcher. lava-dump Extract lava configurations from DynamoDB and dump them to files. lava-email Send email using lava email connections. lava-events Query the lava events table. lava-job-activity Query the lava realm events table to map job activity over a specified time window. lava-new Create a new lava job framework project. lava-ps Show lava worker process information on the current host. lava-schema Perform deep schema validation for lava DynamoDB specification objects. lava-sharepoint Operate on SharePoint sites using lava sharepoint connections. lava-slack Send Slack messages using lava slack connections. lava-smb Operate on SMB file shares using lava smb connections. lava-sql Run SQL using lava database connections. lava-state Manipulate lava state items. lava-stop Perform a controlled shutdown of the lava worker daemons. This also support AWS auto scaling lifecycle hooks. lava-version Provides version information on the installed lava version. lava-worker The main lava worker. lava-ws Show lava worker status based on the worker SQS queues (queue depths, worker backlog etc.). This does not need to run on the worker host. s3lambda Trigger an AWS Lambda function by generating synthetic S3 event notifications."},{"location":"16-commands-utilities.html#lava-icons","title":"Lava Icons","text":"<p>The following are the official lava icons. These used to change based on a whim, same as AWS architecture icons, but are now stable.</p> PNG 512x512256x256 <p> </p> <p> </p> SVG 512x512256x256120x120 <p> </p> <p> </p> <p> </p>"},{"location":"16-commands-utilities.html#lava-ami-utility","title":"Lava AMI Utility","text":"<p>The lava-ami utility displays the available, lava compatible, AMIs and the AMI specified in each lava worker CloudFormation stack.</p> Usage <pre><code>usage: lava-ami [-h] [-n N] [--sak] [--profile PROFILE] [-U] [-v] [-W]\n                [STACK-NAME ...]\n\nManage the AMIs used in lava worker CloudFormation stacks.\n\npositional arguments:\n  STACK-NAME         CloudFormation stack name for a lava worker. Glob style\n                     patterns can be used. If not specified, or any of the\n                     patterns is *, the -U / --update option is not permitted.\n\noptional arguments:\n  -h, --help         show this help message and exit\n  -n N               Only include specified number of most recent images of\n                     each type in the selection list. Default 5.\n  --profile PROFILE  As for AWS CLI.\n  -U, --update       Initiate an interactive update process to allow a new AMI\n                     to be applied for selected stacks. If specified, one or\n                     more stack patterns must be specified (no single *) to\n                     make it harder to maniacally update a whole bunch of\n                     stacks in one go. You can thank me later.\n  -v, --version      show program's version number and exit\n  -W, --no-wait      Don't wait for CloudFormation stack updates to complete.\n</code></pre> <p>Lava-ami also provides an update mode that allows a (more or less) interactive process to select and apply a different AMI to one or more lava worker stacks. This process is a lot simpler and less error prone than trying to manage the AMI used on multiple workers in the AWS CloudFormation console.</p> <p>Lava-ami will silently ignore worker stacks that appear to be parasitic workers hosted on another worker instance. These are detected by the absence of a machine type or AMI ID parameter in the CloudFormation stack. If these need to be updated, use the AWS CloudFormation console.</p> <p>Lava-ami is conservative in its definition of lava compatibility for an AMI. The lava worker itself can run on any Linux machine with the right prerequisite components installed but these two images support the deployment and bootstrapping processes preferred in lava operational environments. Lava-ami will highlight the most recent lava AMI in its output.</p>"},{"location":"16-commands-utilities.html#lava-backup-utility","title":"Lava Backup Utility","text":"<p>Lava-backup performs a complete extract of all of the configuration tables for a given realm and stores the result in a zip file, either locally or in AWS S3.</p> Usage <pre><code>usage: lava-backup [options] realm zip-file\n\nBackup the DynamoDB entries for a specified lava realm. The output is a zip file.\n\npositional arguments:\n  realm               Realm name.\n  zip-file            Name of the output zip file. Can be on the local machine\n                      or in S3 (s3://....). \n\noptional arguments:\n  -h | --help         Print help and exit.\n  -y | --yaml         Output the entries in YAML format. The default is JSON.\n</code></pre> <p>Lava-backup uses the lava-dump utility under the covers. It can be run as a lava cmd job if required. A lava job specification suitable for backing up the current realm is:</p> <pre><code>{\n  \"description\": \"Backup the DynamoDB entries for the realm\",\n  \"dispatcher\": \"Sydney\",\n  \"enabled\": true,\n  \"job_id\": \"lava/dynamo-backup\",\n  \"owner\": \"lava\",\n  \"parameters\": {\n    \"args\": [\n      \"{{realm.realm}}\",\n      \"{{realm.s3_temp}}/lava/dynamo-backup/{{ustart.strftime('%Y-%m-%d')}}.zip\"\n    ]\n  },\n  \"payload\": \"lava-backup\",\n  \"schedule\": \"0 19 * * *\",\n  \"type\": \"cmd\",\n  \"worker\": \"core\"\n}\n</code></pre> <p>Note</p> <p>Dispatcher, worker and schedule will need to be adjusted in the example.</p>"},{"location":"16-commands-utilities.html#lava-check-utility","title":"Lava Check Utility","text":"<p>The lava-check utility performs some basic health checks on DynamoDB table entries.</p> Usage <pre><code>usage: lava-check [-h] [-c GLOB] [--profile PROFILE] [-r REALM] [-S] [-v]\n                  [--no-colour] [-l LEVEL] [--log LOG] [--tag TAG]\n\nCheck lava specifications for problems.\n\noptions:\n  -h, --help            show this help message and exit\n  -c GLOB, --check GLOB\n                        Run the health checks with names matching the given\n                        glob patterns. Can be used multiple times. If not\n                        specified, print a list of available checks.\n  --profile PROFILE     As for AWS CLI.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the environment\n                        variable LAVA_REALM must be set.\n  -S, --no-suppress     Disable suppression of checks for specific DynamoDB\n                        entries via the x-lava-nocheck field. By default\n                        suppression of specific checks is permitted for some\n                        check types.\n  -v, --version         show program's version number and exit\n\nlogging arguments:\n  --no-colour, --no-color\n                        Don't use colour in information messages.\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The Default\n                        is info.\n  --log LOG             Log to the specified target. This can be either a file\n                        name or a syslog facility with an @ prefix (e.g.\n                        @local0).\n  --tag TAG             Tag log entries with the specified value. The default\n                        is lava-check.\n</code></pre> <p>See also lava-schema.</p> <p>Lava-check supports the following checks:</p> Check Type Description conmeta Connection specs with missing metadata (e.g. description, owner). jobjinja * Job specs with Jinja rendering issues. This includes jobs that use globals for which there is no placeholder entry in the job specification, referred to as undeclared globals. While lava tolerates undeclared globals, it is good practice to declare them with a placeholder value. jobmeta Job specs with missing metadata (e.g. description, owner). joborphan * Jobs with no recorded run events. jobrepo * Job specs that don't appear to have an associated repo (no <code>x-lava-git-repo</code> field). jobrsu * redshift_unload jobs with <code>insecure</code> set to <code>true</code>. trigmeta S3trigger specs with missing metadata (e.g. description, owner). <p>The checks marked with a * can be suppressed on an entry specific basis.</p> <p>Note</p> <p>The checks need to perform a full table scan on the relevant table. This is not usually a problem but something to remember. Performing multiple checks on a given table in a single invocation will only do a single table scan though.</p> <p>Output is in markdown formatted tables on the assumption that these issues may end up in a backlog somewhere for correction.</p>"},{"location":"16-commands-utilities.html#suppressing-checks-for-specific-entries","title":"Suppressing Checks for Specific Entries","text":"<p>Some check types can be suppressed for specific DynamoDB table entries by including an <code>x-lava-nocheck</code> field in the table entry. The value is a string identifying a single check type to suppress, or a list of such strings.</p> <p>For example, the following would suppress the <code>joborphan</code> check for a given job specification:</p> <pre><code>{\n  \"job_id\": \"rarely-run-job\",\n  \"x-lava-nocheck\": \"joborphan\",\n  ...\n}\n</code></pre> <p>This would suppress the <code>joborphan</code> and <code>jobrsu</code> checks:</p> <pre><code>{\n  \"job_id\": \"yet-another-job\",\n  \"x-lava-nocheck\": [\n    \"joborphan\",\n    \"jobrsu\"\n  ],\n  ...\n}\n</code></pre>"},{"location":"16-commands-utilities.html#lava-checksum-utility","title":"Lava Checksum Utility","text":"<p>The lava-checksum utility verifies, adds and updates checksums on entries in the following lava DynamoDB tables.</p> <ul> <li> <p>the connections table</p> </li> <li> <p>the jobs table</p> </li> <li> <p>the s3triggers table.</p> </li> <li> <p>the realms table.</p> </li> </ul> <p>Note</p> <p>The checksums are intended for drift detection only. They are not a code signing mechanism and they are not cryptographically sealed.</p> Usage <pre><code>usage: lava-checksum [-h] [-f {txt,tty,html,md}]\n                     [--hash-algorithm ALGORITHM]\n                     [-i] [--profile PROFILE] [-r REALM] [-t TABLE] [-v]\n                     [--version]\n                     {check,add,update} ...\n\nSet and validate checksums on lava DynamdoDB entries.\n\npositional arguments:\n  {check,add,update}\n    check               Validate checksums.\n    add                 Add missing checksums.\n    update              Update existing checksums.\n\noptions:\n  -h, --help            show this help message and exit\n  -f {txt,tty,html,md}, --format {txt,tty,html,md}\n                        Output format. Default is \"tty\" if stdout is a\n                        terminal and \"txt\" otherwise.\n  --hash-algorithm ALGORITHM\n                        Algorithm to use for checksums. Default is sha256.\n  -i, --ignore-case     Matching of glob patterns is case insensitive.\n  --profile PROFILE     As for AWS CLI.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the environment\n                        variable LAVA_REALM must be set.\n  -t TABLE, --table TABLE\n                        Extract from the specified table. This can be one of\n                        jobs, connections, s3triggers (or triggers) or realms.\n                        Any unique initial sequence is accepted. The default\n                        is \"jobs\".\n  -v, --verbose         Increase verbosity. By default, only checksum errors,\n                        updates etc are reported. Can be specified multiple\n                        times.\n  --version             show program's version number and exit\n</code></pre> <p>To get help on a sub-command, use <code>-h</code> / <code>--help</code> on the sub-command. e.g.</p> <pre><code>lava-checksum check --help\n</code></pre> <p>Key points to note:</p> <ul> <li> <p>The checksums are stored in the entry in the field <code>x-lava-chk</code>.</p> </li> <li> <p>Checksum calculation ignores any field starting with <code>x-</code> or <code>X-</code>.</p> </li> <li> <p>The lava-job-framework generates compatible checksums     when deploying entries to the tables.</p> </li> <li> <p>The checksum structure and format are internal to lava and subject to change     at the capricious whim of the developer. The lava-checksum utility will     manage backward compatibility.</p> </li> </ul> <p>Arguments for the lava-checksum utility shown above must be placed before the sub-command. Arguments specific to sub-command must be placed after the sub-command.</p> <p>Note:</p> <ul> <li> <p>The <code>add</code> sub-command will only add missing checksums and <code>update</code> will only     update existing checksums.</p> </li> <li> <p>If any table entries are modified, a ZIP file will be left in the current     directory containing the entries before they were updated. Delete this     manually if not required.</p> </li> </ul>"},{"location":"16-commands-utilities.html#examples","title":"Examples","text":"<pre><code># Check all of the jobs in realm \"prod\"\nlava-checksum --realm prod --table jobs -vv check\n\n# Add missing checksums to connections matching app/* in realm \"prod\"\nlava-checksum --realm prod --table conn -vv update 'app/*'\n</code></pre>"},{"location":"16-commands-utilities.html#lava-conn-usage-utility","title":"Lava-conn-usage Utility","text":"<p>The lava-conn-usage utility will find the job IDs of jobs that reference specified connectors.</p> Usage <pre><code>usage: lava-conn-usage [-h] [--profile PROFILE] [-i] [-r REALM]\n                       connector-glob [connector-glob ...]\n\nFind lava jobs that reference specified connectors.\n\npositional arguments:\n  connector-glob        Report jobs that use connectors that match any of the\n                        specified glob style patterns.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --profile PROFILE     As for AWS CLI.\n  -i, --ignore-case     Matching is case insensitive.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the value of the\n                        LAVA_REALM environment variable is used. A value must\n                        be specified by one of these mechanisms.\n</code></pre> <p>Tip</p> <p>See also Lava-job-activity Utility.</p> <p>For example, the following will find job IDs that reference connectors with IDs containing the string <code>redshift</code> using a glob-style pattern match:</p> <pre><code>lava-conn-usage -r my-realm '*redshift*'\n</code></pre> <p>This can then be used with the lava-job-activity utility to estimate the load lava is placing on particular resources (e.g. a database). See Estimating Lava Load on a Connection.</p> <p>Info</p> <p>Only connections referenced in parameters known by lava to hold connection IDs will be found.</p>"},{"location":"16-commands-utilities.html#lava-dag-generator","title":"Lava DAG Generator","text":"<p>The lava-dag-gen utility generates a DAG specification for lava dag jobs from a dependency matrix. It is provided as part of the standard lava worker installation and is also included in the <code>bin</code> directory with the lava job framework. The lava job framework also provides support for using the utility to automatically generate DAGs at build time.</p> Usage <pre><code>usage: lava-dag-gen [-h] [-c] [-g GROUP] [-o] [-p PREFIX] [-r REALM] [-w KEY]\n                    [--table TABLE] [-y]\n                    source\n\nGenerate a DAG specification for lava dag jobs from a dependency matrix.\n\npositional arguments:\n  source                Source data for the DAG dependency matrix. CSV, Excel\n                        XLSX files and sqlite3 files are supported. The\n                        filename suffix is used to determine file type. If the\n                        value is not a recognised file type, it is assumed to\n                        be a lava database connection ID. In this case the\n                        lava realm must be specified via -r, --realm or the\n                        LAVA_REALM environment variable. For CSV and Excel,\n                        the first column contains successor job names and the\n                        first row contains predecessor job names. Any non-\n                        empty value in the intersection of row and column\n                        indicates a dependency. For database sources, a table\n                        with three columns (job_group, job, depends_on) is\n                        required. The \"job\" and \"depends_on\" columns each\n                        contain a single job name. The \"depends_on\" column may\n                        contain a NULL indicating the \"job\" must be included\n                        but has no dependency. There can be multiple rows\n                        containing the same \"job\".\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c, --compact         Use a more compact form for singleton and empty\n                        dependencies.\n  -g GROUP, --group GROUP\n                        Select only the specified group of source entries. For\n                        CSV files, this is ignored. For Excel files, this\n                        specifies the worksheet name and defaults to the first\n                        worksheet. For sqlite3 files, this is used as a filter\n                        value on the \"job_group\" column of the source table\n                        and defaults to selecting all entries.\n  -o, --order           If specified, just print one possible ordering of the\n                        jobs instead of the DAG specification.\n  -p PREFIX, --prefix PREFIX\n                        Prepend the specified prefix to all job IDs.\n  -r REALM, --realm REALM\n                        Lava realm. Required if the DAG source is specified as\n                        a lava connection ID. Defaults to the value of the\n                        LAVA_REALM environment variable.\n  -w KEY, --wrap KEY    Wrap the DAG specification in the specified map key.\n  --table [SCHEMA.]TABLE \n                        Table name for database sources. Default is dag.\n  -y, --yaml            Generate YAML output instead of JSON.\n</code></pre> <p>Lava-dag-gen can read the dependency information from any of the following:</p> <ul> <li> <p>An SQLite3 database or a database accessed via a lava     database connector in     columnar format.</p> </li> <li> <p>A CSV (<code>*.csv</code>) or Excel (<code>*.xlsx</code>) file in matrix format.</p> </li> </ul>"},{"location":"16-commands-utilities.html#columnar-format","title":"Columnar Format","text":"<p>Dependency information in columnar format must contain the following three columns (only):</p> Column Description job_group An arbitrary grouping label for sets of jobs. job The job ID of the successor job. If all the jobs in a DAG have a common prefix in the job ID, this can be omitted here and inserted at run-time in the dag job specification. depends_on The ID of a predecessor job on which the subject job depends. This may be empty/NULL if the job has no dependencies. Once again, a common prefix can be omitted. <p>Each row contains a single predecessor/successor pair. If a job has multiple predecessors, there will be multiple rows for that job.</p> <p>Sample DDL for a database:</p> <pre><code>CREATE TABLE dag\n(\n    job_group  VARCHAR(50),\n    job        VARCHAR(50) NOT NULL,\n    depends_on VARCHAR(50)\n);\n</code></pre>"},{"location":"16-commands-utilities.html#matrix-format","title":"Matrix Format","text":"<p>In matrix format, the first column contains successor job names and the first row contains predecessor job names. Any non-empty value in the intersection of row and column indicates a dependency. Like so:</p> Jobs J1 J2 J3 J5 J1 x x J2 J4 x J4 x J5 <p>This would result in the following dag payload:</p> <pre><code>{\n    \"J1\": [\n        \"J3\",\n        \"J5\"\n    ],\n    \"J2\": null,\n    \"J4\": [\n        \"J1\",\n        \"J3\"\n    ]\n}\n</code></pre> <p>Note that <code>J5</code> doesn't require its own entry as it is present as a predecessor of <code>J1</code> and has no predecessors of its own.</p>"},{"location":"16-commands-utilities.html#lava-dispatcher-utility","title":"Lava Dispatcher Utility","text":"<p>The lava dispatcher utility is typically run by cron(8) to dispatch jobs on a schedule. It can also be run as a stand-alone utility to dispatch jobs on demand.</p> Usage <pre><code>usage: lava-dispatcher [-h] [--profile PROFILE] [-v] [--check-dispatch]\n                       [-d DELAY] [-q QUEUE] [-r REALM] [-w WORKER]\n                       [-g name=VALUE] [-p name=VALUE] [-c] [-l LEVEL]\n                       [--log-json] [--log LOG] [--tag TAG]\n                       job-id [job-id ...]\n\nLava job dispatcher.\n\noptions:\n  -h, --help            show this help message and exit\n  --profile PROFILE     As for AWS CLI.\n  -v, --version         show program's version number and exit\n  --check-dispatch      If specified, check for the the existence of a\n                        dispatch suppression file \"/tmp/lava/__nodispatch__\".\n                        If the file is present, all dispatches are suppressed.\n                        This is typically only used for scheduled dispatches\n                        when a dispatcher node is in the process of shutting\n                        down.\n\ndispatch control options:\n  -d DELAY, --delay DELAY\n                        Delay dispatch by the specified duration. Default is\n                        0. Maximum is 15 minutes.\n  -q QUEUE, --queue QUEUE\n                        AWS SQS queue name. If not specified, the queue name\n                        is derived from the realm and worker name.\n  -r REALM, --realm REALM\n                        Lava realm name. Defaults to the value of the LAVA\n                        REALM environment variable. A value must be specified\n                        by one of these mechnisms.\n  -w WORKER, --worker WORKER\n                        Lava worker name. The worker must be a member of the\n                        specified realm. If specified, the worker name must\n                        match the value in the job specification. If not\n                        specified, the correct value will be looked up in the\n                        jobs table.\n\njob options:\n  -g name=VALUE, --global name=VALUE\n                        Additional global attribute to include in the job\n                        dispatch event. This option can be used multiple\n                        times. If global names contain dots, they will be\n                        converted into a hierachy using the dots as level\n                        separators.\n  -p name=VALUE, --param name=VALUE\n                        Additional parameter to include in the job dispatch\n                        event. This option can be used multiple times. If\n                        parameter names contain dots, they will be converted\n                        into a hierarchy using the dots as level separators.\n  job-id                One or more job IDs for the specified realm.\n\nlogging arguments:\n  -c, --no-colour, --no-color\n                        Don't use colour in information messages.\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The Default\n                        is info.\n  --log-json            Log messages in JSON format. This is particularly\n                        useful when log messages end up in CloudWatch logs as\n                        it simplifies searching.\n  --log LOG             Log to the specified target. This can be either a file\n                        name or a syslog facility with an @ prefix (e.g.\n                        @local0).\n  --tag TAG             Tag log entries with the specified value. The default\n                        is lava-dispatcher.\n</code></pre> <p>See also The Lava Dispatch Process.</p> <p>Info</p> <p>To enable JSON format logging when performing scheduled dispatches, add <code>--log-json</code> to the <code>args</code> parameter in the lavasched jobs.</p>"},{"location":"16-commands-utilities.html#lava-dump-utility","title":"Lava Dump Utility","text":"<p>Lava-dump performs a bulk extract of data from a single table to a local directory. It can extract all entries with keys that match any of a list of GLOB style patterns. By default, all entries are extracted.</p> Usage <pre><code>usage: lava-dump [-h] [-d DIR] [--profile PROFILE] [-i] [-n] [-r REALM] [-q]\n                 [-t TABLE] [-y]\n                 [glob-pattern [glob-pattern ...]]\n\nExtract lava configurations from DynamoDB and dump them to files.\n\npositional arguments:\n  glob-pattern          Only extract items with keys that match any of the\n                        specified glob style patterns. This test is inverted by\n                        the -n / --not-match option.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d DIR, --dir DIR     Store files in the specified directory, which will be\n                        created if it does npt exist. Defaults to the current\n                        directory.\n  --profile PROFILE     As for AWS CLI.\n  -i, --ignore-case     Matching is case insensitive.\n  -n, --not-match       Only extract items with keys thay do not match any of\n                        the specified glob patterns.\n  -r REALM, --realm REALM\n                        Lava realm name. This is required for all tables\n                        except the realms table.\n  -q, --quiet           Quiet mode.\n  -t TABLE, --table TABLE\n                        Extract from the specified table. This can be one of\n                        jobs, connections, s3triggers (or triggers) or realms.\n                        Any unique initial sequence is accepted.\n  -y, --yaml            Dump items in YAML format. The default is JSON.\n</code></pre> <p>As well as being useful for backup, it is also useful for importing existing items into the lava job framework.</p> <p>See also lava-backup.</p>"},{"location":"16-commands-utilities.html#lava-email-utility","title":"Lava Email Utility","text":"<p>The lava-email utility uses the lava email connector to send emails.</p> Usage <pre><code>usage: lava-email [-h] [--profile PROFILE] [-v] -c CONN_ID [-r REALM]\n                  [-a FILE] [--bcc EMAIL] [--cc EMAIL] [--from EMAIL]\n                  [--reply-to EMAIL] [--to EMAIL] -s SUBJECT [--html FILENAME]\n                  [--text FILENAME] [--no-colour] [-l LEVEL] [--log LOG]\n                  [--tag TAG]\n                  [FILENAME]\n\nSend email using lava email connections.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --profile PROFILE     As for AWS CLI.\n  -v, --version         show program's version number and exit\n\nlava arguments:\n  -c CONN_ID, --conn-id CONN_ID\n                        Lava connection ID. Required.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the environment\n                        variable LAVA_REALM must be set.\n\nemail arguments:\n  -a FILE, --attach FILE\n                        Add the specified file as an attachment. Can be a\n                        local file or an object in S3 in the form\n                        s3://bucket/key. Can be used multiple times.\n  --bcc EMAIL           Recipients to place on the Bcc: line of the message.\n                        Can be used multiple times.\n  --cc EMAIL            Recipients to place on the Cc: line of the message.\n                        Can be used multiple times.\n  --from EMAIL          Message sender. If not specified, a value must be\n                        available in either the connection specification or\n                        the realm specification.\n  --reply-to EMAIL      Reply-to address of the message. Can be used multiple\n                        times.\n  --to EMAIL            Recipients to place on the To: line of the message.\n                        Can be used multiple times.\n  -s SUBJECT, --subject SUBJECT\n                        Message subject. Required.\n\nmessage source arguments:\n  At most one of the following arguments is permitted.\n\n  --html FILENAME       This is a legacy argument for backward compatibility.\n  --text FILENAME       This is a legacy argument for backward compatibility.\n  FILENAME              Name of file containing the message body. If not\n                        specified or \"-\", the body will be read from stdin. An\n                        attempt is made to determine if the message is HTML\n                        and send it accordingly. Only the first 2MB is read.\n\nlogging arguments:\n  --no-colour, --no-color\n                        Don't use colour in information messages.\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The Default\n                        is info.\n  --log LOG             Log to the specified target. This can be either a file\n                        name or a syslog facility with an @ prefix (e.g.\n                        @local0).\n  --tag TAG             Tag log entries with the specified value. The default\n                        is lava-email.\n</code></pre>"},{"location":"16-commands-utilities.html#lava-job-activity-utility","title":"Lava Job Activity Utility","text":"<p>The lava-job-activity utility queries the realm events table to generate an activity map of specified jobs over a defined time period. This is useful to see when specified jobs are running, particularly for event triggered jobs.</p> Usage <pre><code>usage: lava-job-activity [-h] [--profile PROFILE] [-q] -r REALM [-s START_DTZ]\n                         [-e END_DTZ] [--status STATUS] [--dump FILE]\n                         [--load FILE] [-i MINUTES]\n                         [job-id ...]\n\nLava event log query utility\n\npositional arguments:\n  job-id                Retrieve records for the specified job-id. Required\n                        unless --load is used. If used with --load, this acts\n                        as a further filter on event records loaded from the\n                        dump file.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --profile PROFILE     As for AWS CLI.\n  -q, --quiet           Don't print progress messages on stderr.\n  -r REALM, --realm REALM\n                        Lava realm name.\n\nquery arguments:\n  -s START_DTZ, --start START_DTZ\n                        Start datetime. Preferred format is ISO 8601. If a\n                        timezone is not specified, UTC is assumed. When using\n                        --load, the default is the value from the source file.\n                        Otherwise, the default is the most recent midnight\n                        (UTC).\n  -e END_DTZ, --end END_DTZ\n                        End datetime. Preferred format is ISO 8601. If a\n                        timezone is not specified, UTC is assumed. When using\n                        --load, the default is the value from the source file.\n                        Otherwise the default is 24 hours after the start\n                        time.\n  --status STATUS       Only include events with the given status.\n\ndump / load arguments:\n  --dump FILE           Dump the raw data into the specified file in JSON\n                        format. The format is suitable for loading using the\n                        --load option. If both --load and --store are used,\n                        they must be different files.\n  --load FILE           Load the raw data from the specified file instead of\n                        reading it from DynamoDB. The file will have been\n                        produced by a previous run using the --dump option.\n                        This allows a set of data to be reprocessed without\n                        re-extracting the same data.\n\noutput arguments:\n  -i MINUTES, --interval MINUTES\n                        Aggregate job activity into intervals of the specified\n                        duration (minutes). Stick to divisors or multiples of\n                        60. Default is 10.\n</code></pre> <p>Tip</p> <p>See also Lava-conn-usage Utility.</p> <p>The process of extracting data from the events table can be expensive in usage of DynamoDB table read capacity. Hence the extraction process has two optimisations:</p> <ol> <li> <p>Specific job IDs must be requested. This enables full table scans to be     avoided on the, often large, events table.</p> </li> <li> <p>The extracted event data can be stored in a JSON formatted dump file. This     file can be read back in subsequent runs of the utility to alter other     parameters, such as the aggregation granularity. See the --dump and     --load arguments.</p> </li> </ol> <p>The output to stdout is a CSV file containing two tables:</p> <ol> <li> <p>Run-seconds per time-slice by job ID</p> </li> <li> <p>Run-seconds per time-slice by lava worker.</p> </li> </ol> <p>For example, the following command will extract the data for a given day and job ID. The output CSV will have activity sliced into 10 minute blocks. The raw data is retained for further analysis:</p> <pre><code>lava-job-activity -r my-realm --start 2024-02-15T00:00:00+11:00 \\\n    --dump evdata.json job-id &gt; job-10.csv\n</code></pre> <p>Each time-slice column in the output CSV will indicate for how many seconds that job run within that time-slice (i.e. run seconds). This can be greater than the number of seconds in the time-slice if the job ran more than once in that slice.</p> <p>The extracted data can be reprocessed into 4 hour time-slices by reusing the dump file thus:</p> <pre><code>lava-job-activity -r my-realm --start 2024-02-15T00:00:00+11:00 \\\n    --dump evdata.json --interval 240 job-id &gt; job-240.csv\n</code></pre> <p>The CSV file will contain information something like this:</p> Job ID 15/2/2024 0:00 15/2/2024 4:00 15/2/2024 8:00 15/2/2024 12:00 15/2/2024 16:00 15/2/2024 20:00 job-id 288 348 376 335 287 344 Worker 15/2/2024 0:00 15/2/2024 4:00 15/2/2024 8:00 15/2/2024 12:00 15/2/2024 16:00 15/2/2024 20:00 core 288 348 376 335 287 344 <p>Info</p> <p>It is important to keep and re-use dump files where possible, particularly when extracting data for a large number of job IDs. However, the dump file will only contain raw event data requested for the initial set of job IDs and time window. It is not possible to reuse data that wasn't extracted in the first place.</p>"},{"location":"16-commands-utilities.html#estimating-lava-load-on-a-connection","title":"Estimating Lava Load on a Connection","text":"<p>Sometimes it's useful to estimate how much load lava is placing on a particular resource (such as a database) over a defined time window. This can be done by combining the lava-job-activity and lava-conn-usage utilities.</p> <p>The following example will produce the time-slice view described above for all jobs that reference connectors with IDs starting with <code>redshift</code>:</p> <pre><code>lava-job-activity -r my-realm --start 2024-02-15T00:00:00+11:00 \\\n    --dump evdata.json \\\n    $(lava-conn-usage -r my-realm 'redshift*') &gt; conn-activity.csv\n</code></pre> <p>Note that this does not mean the connection was in active use at all times in the activity window. Lava has no way of knowing that. It does give a proxy view of load intensity.</p>"},{"location":"16-commands-utilities.html#lava-new-utility","title":"Lava-new Utility","text":"<p>Note</p> <p>New in v8.2 (K\u012blauea).</p> <p>The lava-new utility is used to create a new lava job framework project.</p> Usage <pre><code>usage: lava-new [-h] [-v] [--no-input] [-p KEY=VALUE] directory\n\nCreate a new lava job framework project.\n\npositional arguments:\n  directory             Create the template source in the specified directory\n                        (which must not already exist).\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n  --no-input            Do not prompt for user input. The -p / --param option\n                        should be used to specify parameter values.\n  -p KEY=VALUE, --param KEY=VALUE\n                        Specify default parameters for the underlying\n                        cookiecutter used to create the new lava project. Can\n                        be used multiple times. Available parameters are\n                        config_checks, description, docker_platform,\n                        docker_prefix, environment, git_setup,\n                        include_lava_libs, job_prefix, jupyter_support, owner,\n                        payload_prefix, pip_index_url, project_dir,\n                        project_name, realm, rule_prefix, s3trigger_prefix.\n</code></pre>"},{"location":"16-commands-utilities.html#lava-schema-utility","title":"Lava Schema Utility","text":"<p>The lava-schema utility performs deep schema validation for lava DynamoDB specification objects.</p> Usage <pre><code>usage: lava-schema.py [-h] [-d] [-r REALM] [-s DIRNAME]\n                      [-t {job,s3trigger,connection}] [-v]\n                      [SPEC ...]\n\nDeep schema validation for lava DynamoDB specification objects.\n\npositional arguments:\n  SPEC                  If specified, specifications are read directly from\n                        DynamoDB and any SPEC arguments are treated as GLOB style\n                        patterns that the ID of the specifications must match. If\n                        the -d / --dynamodb option is not specified, JSON formatted\n                        lava object specifications are read from the named files.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d, --dynamodb        Read lava specifications from DynamoDB instead of the local\n                        file system. The lava realm must be specified, either via\n                        the -r / --realm option or the LAVA_REALM environment\n                        variable.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the environment variable\n                        LAVA_REALM will be used. If --d / --dynamodb is specified,\n                        a value must be specified by one of these mechanisms.\n  -s DIRNAME, --schema-dir DIRNAME\n                        Directory containing lava schema specifications. Default is\n                        /usr/local/lib/lava/lava/lib/schema.\n  -t {job,s3trigger,connection}, --type {job,s3trigger,connection}\n                        Use the schema appropriate to the specified lava object\n                        type. Options are job, s3trigger, connection. The default\n                        is job.\n  -v, --verbose         Print results for all specifications. By default, only\n                        validation failures are printed.\n</code></pre> <p>Lava-schema can read specifications directly from DynamoDB or from the local file system. The latter is useful to check the install components produced in the <code>dist</code> directory by a lava job framework project.</p> <p>Whereas lava-check is focused on basic configuration management hygiene, lava-schema is focused on strict compliance with lava DynamoDB table specifications using detailed JSON Schema specifications. (They may merge at some point.)</p>"},{"location":"16-commands-utilities.html#lava-schema-vs-lava-worker-validation","title":"Lava-schema vs Lava Worker Validation","text":"<p>As of v7.1.0 (Pichincha), deep schema validation only manifests in the lava-schema utility. The lava worker doesn't use this. Instead, it uses its traditional process of checking just enough to validate that it can try to run the job.</p> <p>This will change in a future release and the worker will also perform deep schema validation of the fully resolved augmented job specification and the other DynamoDB object types at run-time. Malformed jobs that could run under the current validation process will be rejected outright.</p> <p>I'm from the Government and I'm here to help you.</p> <p>Commonly observed configuration errors that the lava worker will tolerate but lava-schema will not include:</p> <ul> <li> <p>Malformed action specifications      These don't prevent the job from running but the malformed actions will     fail, potentially causing operational issues or unreported errors.</p> </li> <li> <p>Optional fields in the wrong place      For example, the <code>timeout</code> parameter of exe jobs     occasionally pops up at the top level of the job specification instead of     within the job <code>parameters</code>. The lava worker will silently ignore the     incorrectly placed <code>timeout</code> and do its best to run the job -- with the     default timeout.</p> </li> <li> <p>Imaginary fields      For example, the sqlc and sqlv job types     support a <code>timeout</code> parameter but the sql and     sqli jobs do not. If a job is migrated between these     types, it's easy to miss the need to add / remove the <code>timeout</code> parameter.     The lava worker will happily run the job -- with the default timeout.</p> </li> <li> <p>Incorrect parameter types      For example, the <code>args</code> parameter of the exe job type     expects a list of strings (although numbers are OK also). Booleans     are not. In a YAML job specification file in a     lava job framework project, it is easy to mix up     an argument value of <code>true</code> (boolean) with <code>\"true\"</code> (string). The first one     will get presented to the job payload at run-time as <code>True</code> and the second     one will get presented as <code>true</code>. This could be a problem.</p> </li> <li> <p>It will be alright on the night      When the lava worker runs a job, it grabs the job specification from     DynamoDB and then merges in any parameters received as part of the dispatch     message to produce the fully resolved     augmented job specification. It's     tempting to leave out of the job specification parameters that are going to     be replaced at run-time anyway. Problem is, this can make it tricky to work     out what the job will do just by looking at the job specification.     Lava-schema will complain about this because when it does its static     analysis, the job is malformed. The lava worker is happy because it gets     everything required at run-time. DevOps folk are less than pleased with a     partially specified job in DynamoDB that frustrates attempts to diagnose     problems.    </p> <p>It's good practice to include a placeholder in the job specification for every parameter, global and state item the job requires, even if it's replaced at run-time.</p> </li> </ul>"},{"location":"16-commands-utilities.html#caveats","title":"Caveats","text":"<p>Because of the way JSON Schema works, some of the error messages that occur when non-compliance is detected can be rather obscure. It's not uncommon for a single specification error to generate multiple error messages. Generally, the error messages give a pretty good indication of what's wrong.</p> <p>There are, possibly, (rare) occasions when its legitimate for a job specification in DynamoDB to not comply with lava schema requirements but the fully resolved augmented job specification at run-time does comply.</p> <p>Did I mention that this is rare?</p> <p>Trying to think of a situation where this is good design practice ...</p> <p>Still thinking ...</p>"},{"location":"16-commands-utilities.html#lava-sql-utility","title":"Lava SQL Utility","text":"<p>The lava-sql utility provides a simple, uniform CLI across different database types with connectivity managed by the lava connection subsystem.</p> Usage <pre><code>usage: lava-sql [-h] [--profile PROFILE] [-v] [-a NAME] [-b BATCH_SIZE]\n                [--format {csv,jsonl,html,parquet}] [--header] [-o FILENAME]\n                [--raw] [--transaction] -c CONN_ID [-r REALM] [--no-colour]\n                [-l LEVEL] [--log LOG] [--tag TAG] [--delimiter CHAR]\n                [--dialect {excel,excel-tab,unix}] [--doublequote]\n                [--escapechar CHAR] [--quotechar CHAR] [--quoting QUOTING]\n                [--sort-keys]\n                [SQL-FILE ...]\n\nRun SQL using lava database connections.\n\npositional arguments:\n  SQL-FILE              SQL files. These can be local or in S3 (s3://...). If\n                        not specified or \"-\", stdin is used.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --profile PROFILE     As for AWS CLI.\n  -v, --version         show program's version number and exit\n  -a NAME, --app-name NAME, --application-name NAME\n                        Use the specified application name when connecting to\n                        the database. Ignored for database types that don't\n                        support this concept.\n  -b BATCH_SIZE, --batch-size BATCH_SIZE\n                        Number of records per batch when processing SELECT\n                        querues. Default is 1024.\n  --format {csv,jsonl,html,parquet}\n                        Output format. Default is csv.\n  --header              Print a header for SELECT queries (output format\n                        dependent).\n  -o FILENAME, --output FILENAME\n                        Write output to the specified file which may be local\n                        or in S3 (s3://...). If not specified, output is\n                        written to stdout.\n  --raw                 Don't split SQL source files into individual\n                        statements. By default, an attempt will be made to\n                        split each source file into individual SQL statements.\n  --transaction         Disable auto-commit and run all SQLs in a transaction.\n\nlava arguments:\n  -c CONN_ID, --conn-id CONN_ID\n                        Lava database connection ID. Required.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the environment\n                        variable LAVA_REALM must be set.\n\nlogging arguments:\n  --no-colour, --no-color\n                        Don't use colour in information messages.\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The Default\n                        is info.\n  --log LOG             Log to the specified target. This can be either a file\n                        name or a syslog facility with an @ prefix (e.g.\n                        @local0).\n  --tag TAG             Tag log entries with the specified value. The default\n                        is lava-sql.\n\nCSV format arguments:\n  --delimiter CHAR      Single character field delimiter. Default |.\n  --dialect {excel,excel-tab,unix}\n                        CSV dialect (as per the Python csv module). Default is\n                        excel.\n  --doublequote         See Python csv.writer.\n  --escapechar CHAR     See Python csv.writer. Escaping is disabled by\n                        default.\n  --quotechar CHAR      See Python csv.writer. Default is \".\n  --quoting QUOTING     As for csv.writer QUOTE_* parameters (without the\n                        QUOTE_ prefix). Default is minimal (i.e.\n                        QUOTE_MINIMAL).\n\nJSONL format arguments:\n  --sort-keys           Sort keys in JSON objects.\n</code></pre> <p>Lava-sql can run one or more queries in a transaction and also capture output from <code>SELECT</code> queries in various formats, either to a local file or to AWS S3.</p> <p>Info</p> <p>Do not have more than one <code>SELECT</code> query in the batch unless you are deliberately trying to create a mess.</p>"},{"location":"16-commands-utilities.html#csv","title":"CSV","text":"<p>Note that the default delimiter for <code>csv</code> format is the pipe symbol <code>|</code>, not a comma. The original rationale for this was for consistency with the Redshift <code>COPY</code> and <code>UNLOAD</code> commands. All I can say is that it seemed to make sense at the time.</p>"},{"location":"16-commands-utilities.html#html-format","title":"HTML Format","text":"<p>The output data is encoded as an HTML table with a class of <code>lava-sql</code>. Only the table HTML is produced to allow the output to be incorporated into a larger HTML document. (i.e. no <code>HTML</code>, <code>BODY</code> tags etc.).</p> <p>Values will be escaped as needed to ensure HTML correctness.</p>"},{"location":"16-commands-utilities.html#jsonl-format","title":"JSONL Format","text":"<p>Each row of output data is encoded as a single line JSON formatted object.</p>"},{"location":"16-commands-utilities.html#parquet-format","title":"Parquet Format","text":"<p>Parquet compression will generally benefit from a larger batch size. The default of 1024 is reasonable for many purposes but increasing it will often give a smaller output file. Don't get carried away though. Each batch has to be held entirely in memory.</p> <p>A word of caution about the Parquet schema ... It's quite difficult to handle schema inference in a predictable or consistent way, particularly with data sourced via a DBAPI 2 connector as the standard does not provide any consistency in how, or if, implementations signal type information in query responses. The approach used by lava-sql is to let PyArrow form an educated guess based on the first record batch. This should be fine for most purposes.</p>"},{"location":"16-commands-utilities.html#lava-state-utility","title":"Lava State Utility","text":"<p>The lava state utility provides a CLI to the lava state manager.</p> Usage <pre><code>usage: lava-state [-h] [--profile PROFILE] [-r REALM] [-v] [--no-colour]\n                  [-l LEVEL] [--log LOG] [--tag TAG]\n                  {put,get} ...\n\nManipulate lava state entries.\n\npositional arguments:\n  {put,get}\n    put                 Add / replace a state entry.\n    get                 Get a state entry.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --profile PROFILE     As for AWS CLI.\n  -r REALM, --realm REALM\n                        Lava realm name. If not specified, the environment\n                        variable LAVA_REALM must be set.\n  -v, --version         show program's version number and exit\n\nlogging arguments:\n  --no-colour, --no-color\n                        Don't use colour in information messages.\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The Default\n                        is info.\n  --log LOG             Log to the specified target. This can be either a file\n                        name or a syslog facility with an @ prefix (e.g.\n                        @local0).\n  --tag TAG             Tag log entries with the specified value. The default\n                        is lava-state.\n</code></pre>"},{"location":"16-commands-utilities.html#creating-a-lava-state-item","title":"Creating a Lava State Item","text":"<p>State items are created with the <code>put</code> sub-command.</p> <p>Info</p> <p>Do not create state items with a <code>state_id</code> starting with <code>lava</code>. This prefix is reserved.</p> Usage: lava-state put <pre><code>usage: lava-state put [-h] [-p KEY=VALUE | -v VALUE] [--kms-key KMS_KEY]\n                      [--publisher PUBLISHER] [--ttl DURATION]\n                      [--type STATE_TYPE]\n                      state_id\n\npositional arguments:\n  state_id              State ID.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -p KEY=VALUE, --param KEY=VALUE\n                        Add the specified key/value pair to the state item.\n                        Can be repeated to set multiple key/value pairs.\n  -v VALUE, --value VALUE\n                        Set the value to the specified string.\n  --kms-key KMS_KEY     The \"secure\" state item type supports KMS encryption\n                        of the value. This argument specifies the KMS key to\n                        use, either as a KMS key ARN or a key alias in the\n                        form \"alias/key-id\". Defaults to the \"sys\" key for the\n                        lava realm. Ignored for other state item types.\n  --publisher PUBLISHER\n                        Set the state item publisher to the specified value.\n                        Default is the contents of the LAVA_JOB_ID environment\n                        variable, if set, or else \"lava-state CLI\".\n  --ttl DURATION        Time to live as a duration (e.g. 10m, 2h, 1d).\n  --type STATE_TYPE     State item type. Options are json, raw, secure.\n                        Default is json.\n</code></pre>"},{"location":"16-commands-utilities.html#retrieving-a-lava-state-item","title":"Retrieving a Lava State Item","text":"<p>State items are retrieved with the <code>get</code> sub-command.</p> Usage: lava-state get <pre><code>usage: lava-state get [-h] [-i] state_id [template]\n\npositional arguments:\n  state_id              State ID.\n  template              An optional Jinja2 template that will be rendered with\n                        the retrieved value as the \"state\" and \"s\" parameters.\n                        e.g if set to \"{{ state }}\" (the default) the value is\n                        printed as is.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i, --ignore-missing  Ignore errors for missing state items and return an\n                        empty string. By default, attempting to get a non-\n                        existent state item is an error.\n</code></pre>"},{"location":"16-commands-utilities.html#lava-stop-utility","title":"Lava Stop Utility","text":"<p>The lava-stop utility initiates a controlled shutdown of the lava worker daemons.</p> Usage <pre><code>usage: lava-stop [-h] [-D] [--profile PROFILE] [--signal SIGNAL] [-v]\n                 [-w DURATION] [--auto-scaling-group-name NAME]\n                 [--instance-id ID] [--lifecycle-action-token UUID]\n                 [--lifecycle-hook-name NAME] [--lifecycle-heartbeat DURATION]\n                 [-c] [-l LEVEL] [--log LOG] [--tag TAG]\n\nStop lava worker processes.\n\noptions:\n  -h, --help            show this help message and exit\n  -D, --no-dispatch     Inhibit further scheduled dispatches by creating\n                        /tmp/lava/__nodispatch__. This requires the lava-\n                        dispatcher utility to check for this file by\n                        specifying the --check-dispatch argument.\n  --profile PROFILE     As for AWS CLI.\n  --signal SIGNAL, --sig SIGNAL\n                        Send the specified signal to the lava worker\n                        processes. Can be specified as a signal name (e.g.\n                        SIGHUP or HUP) or a signal number. The default is 0\n                        which only tests if the process exists. SIGHUP is\n                        interpreted as a controlled shutdown instruction\n                        allowing running jobs to complete. SIGTERM is\n                        interpreted as a controlled, but immediate,\n                        termination that allows final cleanup tasks but takes\n                        no account of running jobs. See --w, --wait.\n  -v, --version         show program's version number and exit\n  -w DURATION, --wait DURATION\n                        Wait for up to the specified duration for the lava\n                        workers to finish voluntarily before killing them.\n                        This requires the signal to be set to SIGHUP / HUP as\n                        this is interpreted by the lava worker daemons as a\n                        controlled shutdown request. The duration must be in\n                        the form nn[X] where nn is a number and X is one of s\n                        (seconds), m (minutes) or h (hours). If X is not\n                        specified, seconds are assumed.\n\nAWS auto scaling lifecycle options:\n  These arguments are designed to complete an AWS auto scaling \"EC2\n  Instance-terminate Lifecycle Action\". See the AWS CLI or AWS auto scaling\n  documentation for meaning and usage. Note that the lifecycle action result\n  is always set to CONTINUE which means the auto scaling group _will_\n  terminate the instance.\n\n  --auto-scaling-group-name NAME\n                        Send a complete-lifecycle-action signal for the\n                        specified AWS auto scaing group. If specified,\n                        --lifecycle-hook-name is also required.\n  --instance-id ID      The ID of the EC2 instance (optional). If specified,\n                        --auto-scaling-group-name / --lifecycle-hook-name are\n                        required.\n  --lifecycle-action-token UUID\n                        lifecycle action identifier (optional). If specified,\n                        --auto-scaling-group-name / --lifecycle-hook-name are\n                        required.\n  --lifecycle-hook-name NAME\n                        The name of the AWS auto scaling lifecycle hook. If\n                        specified, --auto-scaling-group-name is also required.\n  --lifecycle-heartbeat DURATION\n                        Record a heartbeat for the lifecycle action at\n                        specified intervals (optional). If specified, --auto-\n                        scaling-group-name / --lifecycle-hook-name are\n                        required. THe duration must be in the form nn[X] where\n                        nn is a number and X is one of s (seconds), m\n                        (minutes) or h (hours). If X is not specified, seconds\n                        are assumed. The minimum permitted value is 60\n                        seconds.\n\nlogging arguments:\n  -c, --no-colour, --no-color\n                        Don't use colour in information messages.\n  -l LEVEL, --level LEVEL\n                        Print messages of a given severity level or above. The\n                        standard logging level names are available but debug,\n                        info, warning and error are most useful. The Default\n                        is info.\n  --log LOG             Log to the specified target. This can be either a file\n                        name or a syslog facility with an @ prefix (e.g.\n                        @local0).\n  --tag TAG             Tag log entries with the specified value. The default\n                        is lava-stop.\n</code></pre> <p>The process for stopping a lava worker is:</p> <ol> <li> <p>Send it a <code>SIGHUP</code> signal. This tells the worker to complete any in-flight     or queued jobs but not to accept any more jobs.</p> </li> <li> <p>Wait a while.</p> </li> <li> <p>Send it another <code>SIGHUP</code> signal. The second <code>SIGHUP</code> is a more aggressive     shutdown command and will interrupt in-flight jobs but still allow the     worker an opportunity to cleanup.</p> </li> <li> <p>Give it another 10-20 seconds.</p> </li> <li> <p>If the worker is still running, kill it with <code>SIGKILL</code>.</p> </li> </ol> <p>Lava-stop will do a process listing to find worker processes. It can be used interactively and is also designed for use within an AWS auto scaling lifecycle hook for terminating worker nodes. This is all built in to a standard lava deployment using the provided CloudFormation templates.</p>"},{"location":"16-commands-utilities.html#lava-version-utility","title":"Lava Version Utility","text":"<p>The lava-version utility provides version information on the installed lava version.</p> Usage <pre><code>usage: lava-version [-h] [-n | -a | --ge VERSION | --eq VERSION]\n\nPrint lava version information.\n\noptional arguments:\n  -h, --help    show this help message and exit\n  -n, --name    Print version name only.\n  -a, --all     Print all version inforamtion.\n  --ge VERSION  Exit with zero status if the lava version is greater than or\n                equal to the specified version.\n  --eq VERSION  Exit with zero status if the lava version is equal to the\n                specified version.\n\nIf no arguments are specified the lava version number is printed.\n</code></pre>"},{"location":"16-commands-utilities.html#lava-worker-status-utility","title":"Lava Worker Status Utility","text":"<p>The lava-ws utility displays worker status information based on the worker SQS queues (queue depths, worker backlog etc.).</p> Usage <pre><code>usage: lava-ws [-h] [-f FORMAT] [-l] -r REALM [-w WORKER] [-v]\n\nGet status info about lava workers.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f FORMAT, --format FORMAT\n                        Output table format (see below). The formats supported\n                        by tabulate (https://pypi.org/project/tabulate/) can\n                        be used. The default is fancy_grid.\n  -l                    Show more information. Repeat up to 2 times to get\n                        more details.\n  -r REALM, --realm REALM\n                        Lava realm name.\n  -w WORKER, --worker WORKER\n                        Lava worker name prefix. If not specified, report on\n                        all workers in the realm (assumes lava standard queue\n                        naming conventions).\n  -v, --version         show program's version number and exit\n\noutput columns:\n  BCKAVG   Average worker backlog in the last 15 minutes\n  BCKMAX   Maximum worker backlog in the last 15 minutes\n  BCKNOW   Current backlog\n  DELAVG   Average run delay in the last 15 minutes\n  DELMAX   Maximum run delay in the last 15 minutes\n  EC2      Number of running EC2 instances\n  EC2TYPE  EC2 instance type\n  MSGS     Messages visible\n  NVIS     Messages not visible\n  QUEUE    SQS queue name\n  RET      Message retention period\n  VIS      Visibility timeout\n\noutput formats:\n  fancy_grid  fancy_outline   github           grid       html        jira\n  latex       latex_booktabs  latex_longtable  latex_raw  mediawiki   moinmoin\n  orgtbl      pipe            plain            presto     pretty      psql\n  rst         simple          textile          tsv        unsafehtml  youtrack\n</code></pre> <p>Unlike lava-ps, which displays worker process information, lava-ws does not need to run on the worker host.</p>"},{"location":"17-job-framework-samples.html","title":"Lava Job Framework Samples","text":"<p>This section contains samples for various DynamoDB table entries for lava.  The samples are intended for use with the lava job framework, hence are provided in YAML format with Jinja references to the most common elements of job framework configuration files.</p> <ul> <li>Job samples</li> <li>Connection samples</li> <li>Rule samples</li> <li>S3trigger samples</li> </ul>"},{"location":"17-job-framework-samples.html#job-samples","title":"Job Samples","text":"chain <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: chain\n\n## #############################################################################\n## The payload is either a comma separated list of job_ids or an actual list of\n## job_ids.\n##\npayload:\n  - \"job1\"\n  - \"job2\"\n  - \"...\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Prepend the specified value to each job_id in the payload.\n  # job_prefix: \"app/myjobs/\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The job_id of the starting point in the chain. Defaults to first in list.\n  # start: \"start_job_id\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Child jobs matching this glob pattern (or any in a list of patterns) are\n  ## allowed to fail without causing the chain to fail.\n  # can_fail: \"glob-pattern\"\n</code></pre> cmd <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: cmd\n\n## #############################################################################\n## The payload is the command string. This will be parsed using standard Linux\n## shell lexical analysis to determine the executable and arguments. Additional\n## arguments can also be specified with the args parameter\n##\npayload: \"???\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A list of additional arguments for the executable(s) specified in payload.\n  ##\n  # args:\n  #   - \"arg1\"\n  #   - \"arg2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of additional environment variables for the exe.\n  ##\n  # env:\n  #   env_var1: \"value1\"\n  #   env_var2: \"value2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Job timeout. Default is 10 minutes. Values are in the form nnX where nn is\n  ## a number and X is s (seconds), m (minutes) or h (hours). If payload is a\n  ## list, this is applied to each individual exe.\n  ##\n  # timeout: \"10m\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: value1\n  #   var2: value2\n</code></pre> dag <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: dag\n\n## #############################################################################\n## The payload is either a map of job dependencies. Keys are successor jobs and\n## values are lists of predecessor jobs or null / [] when no predecessors.\n## job_ids.\n##\npayload:\n  \"job1\":\n    - \"predecessor_1\"\n    - \"predecessor_2\"\n  \"job2\": \"just_one_predecessor\"\n  \"job3\": null\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Prepend the specified value to each job_id in the payload.\n  # job_prefix: \"app/myjobs/\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Thread pool size for running child jobs. Don't get carried away.\n  # workers: 4\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Child jobs matching this glob pattern (or any in a list of patterns) are\n  ## allowed to fail without causing the chain to fail.\n  # can_fail: \"glob-pattern\"\n</code></pre> db_from_s3 <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: db_from_s3\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  db_conn_id: \"???\"\n  bucket: \"???\"\n  key: \"???\"\n  schema: \"???\"\n  table: \"???\"\n  mode: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## Skip missing files without throwing an error.\n  # skip_missing: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## S3 connectivity -- either s3_conn_id or s3_iam_role\n  ##\n  ## The connection ID for AWS S3.\n  # s3_conn_id: \"???\"\n  ## The IAM role name used to allow access to the source data in S3.\n  # s3_iam_role: \"???\"\n\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A list of additional arguments dependent on target DB type\n  # args:\n  #   - \"arg1\"\n  #   - \"arg2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## List of SQL column specifications.\n  # columns:\n  #  - \"col1 VARCHAR(20)\"\n  #  - \"col2 TIMESTAMP\"\n</code></pre> dispatch <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: dispatch\n\n## #############################################################################\n## The payload is either a comma separated list of job_ids or an actual list of\n## job_ids.\n##\npayload:\n  - \"job1\"\n  - \"job2\"\n  - \"...\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Dispatch message sending delay in the form nnX where nn is a number and X\n  ## is s (seconds) or m (minutes). The maximum allowed value is 15 minutes.\n  ##\n  # delay: \"5m\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If false, disable Jinja rendering of the payload. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Prepend the specified value to each job_id in the payload.\n  # job_prefix: \"app/myjobs/\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of parameters that will be passed to the jobs being dispatched. This\n  ## is Jinja rendered.\n  ##\n  # parameters:\n  #   p1: \"v1\"\n  #   p2: \"v2\"\n</code></pre> docker <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: docker\n\n## #############################################################################\n## The payload is the container repository and, optionally, tag in the form\n## repository[:tag]. If the tag is not specified, a tag of latest is used.\n##\npayload: \"&lt;{ lava.aws.ecr_uri }&gt;/&lt;{ prefix.docker_repo }&gt;/???\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The command to run in the container. If not specified, the default entry\n  ## point for the container is used.\n  # command: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The conn_id for connecting to docker. If not specified, a value must be\n  ## specified for the entire realm in the realms table.\n  # docker: conn_id_for_docker_registry\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A list of additional arguments for the executable(s) specified in payload.\n  # args:\n  #   - \"arg1\"\n  #   - \"arg2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Keys are connection labels and the values are conn_id.\n  # connections:\n  #     conn1: \"conn1_id\"\n  #     conn2: \"conn2_id\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of additional environment variables for the exe.\n  ##\n  # env:\n  #   env_var1: \"value1\"\n  #   env_var2: \"value2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Job timeout. Default is 10 minutes. Values are in the form nnX where nn is\n  ## a number and X is s (seconds), m (minutes) or h (hours). If payload is a\n  ## list, this is applied to each individual exe.\n  ##\n  # timeout: \"10m\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  # A map of container host configuration parameters.\n  # host_config:\n  #   blkio_weight: ??\n  #   blkio_weight_device: ??\n  #   cap_add: ??\n  #   cap_drop: ??\n  #   cpu_count: ??\n  #   cpu_percent: ??\n  #   cpu_period: ??\n  #   cpu_quota: ??\n  #   cpu_shares: ??\n  #   cpuset_cpus: ??\n  #   cpuset_mems: ??\n  #   device_read_bps: ??\n  #   device_read_iops: ??\n  #   device_write_bps: ??\n  #   device_write_iops: ??\n  #   dns: ??\n  #   dns_opt: ??\n  #   dns_search: ??\n  #   domainname: ??\n  #   extra_hosts: ??\n  #   group_add: ??\n  #   mem_limit: ??\n  #   mem_swappiness: ??\n  #   memswap_limit: ??\n  #   nano_cpus: ??\n  #   network_disabled: ??\n  #   network_mode: ??\n  #   ports: ??\n  #   publish_all_ports: ??\n  #   shm_size: ??\n  #   user: ??\n  #   working_dir: ??\n</code></pre> exe <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: exe\n\n## #############################################################################\n## Payload is the path to an executable (or list of paths) relative to the\n## payloads area in S3.\n##\npayload: \"&lt;{ prefix.payload }&gt;/???\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A list of additional arguments for the executable(s) specified in payload.\n  ##\n  # args:\n  #   - \"arg1\"\n  #   - \"arg2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Keys are connection labels and the values are conn_id.\n  ##\n  # connections:\n  #     conn1: \"conn1_id\"\n  #     conn2: \"conn2_id\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of additional environment variables for the exe.\n  ##\n  # env:\n  #   env_var1: \"value1\"\n  #   env_var2: \"value2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Job timeout. Default is 10 minutes. Values are in the form nnX where nn is\n  ## a number and X is s (seconds), m (minutes) or h (hours). If payload is a\n  ## list, this is applied to each individual exe.\n  ##\n  # timeout: \"10m\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> foreach <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: foreach\n\n## #############################################################################\n## The payload is the job_id for the child job to be run iteratively.\n##\npayload: \"child_job_id\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  foreach:\n    type: \"iterator type\"  # e.g. inline, csv etc.\n    param1: ...  # These are type dependent\n    param2: ...\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Child job iterations are allowed to fail without causing the main job to fail.\n  # can_fail: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Abort without running any jobs if the loop is longer than this.\n  # limit: 5\n</code></pre> log <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: log\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n</code></pre> pkg <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: pkg\n\n## #############################################################################\n## Payload is the path to an executable (or list of paths) relative to the\n## payloads area in S3.\n##\npayload: \"&lt;{ prefix.payload }&gt;/???\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## The name of the entry point executable in the bundle, relative to the root\n  ## of the bundle.  This will be parsed using standard Linux shell lexical\n  ## analysis to determine the executable and arguments. Additional arguments\n  ## can also be specified with the args parameter.\n  ##\n  command: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A list of additional arguments for the executable(s) specified in payload.\n  ##\n  # args:\n  #   - \"arg1\"\n  #   - \"arg2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Keys are connection labels and the values are conn_id.\n  ##\n  # connections:\n  #     conn1: \"conn1_id\"\n  #     conn2: \"conn2_id\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of additional environment variables for the exe.\n  ##\n  # env:\n  #   env_var1: \"value1\"\n  #   env_var2: \"value2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Job timeout. Default is 10 minutes. Values are in the form nnX where nn is\n  ## a number and X is s (seconds), m (minutes) or h (hours). If payload is a\n  ## list, this is applied to each individual exe.\n  ##\n  # timeout: \"10m\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> redshift_unload <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: redshift_unload\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  conn_id: \"???\"\n  bucket: \"???\"\n  prefix: \"???\"\n  schema: \"???\"\n\n  # Source relation can be a string or a list\n  relation: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A list of additional arguments dependent on target DB type\n  args:\n    - \"arg1\"\n    - \"arg2\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## S3 connectivity -- either s3_conn_id or s3_iam_role\n  ##\n  ## The connection ID for AWS S3.\n  # s3_conn_id: \"???\"\n  ## The IAM role name used to allow access to the source data in S3.\n  # s3_iam_role: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Disable bucket security checks. Default is false.\n  ##\n  # insecure: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Name of the relation to start with when unloading a list of relations. If\n  ## not specified, start at the beginning of the list.\n  ##\n  # start: \"relation\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ##  If true, stop when any unload fails otherwise keep moving through the\n  ## unload list. Default is true.\n  ##\n  # stop_on_fail: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the S3 target prefix is Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## An optional WHERE condition for the UNLOAD queries. Do not include the\n  ## WHERE keyword.\n  ##\n  # where: \"a where clause common to all source relations\"\n</code></pre> sharepoint_get_doc <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sharepoint_get_doc\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for a SharePoint site.\n  ##\n  conn_id: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Source SharePoint library name.\n  ##\n  library: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Source document path in SharePoint. Use POSIX, not DOS, style path names\n  ## (i.e. forward slash path separators). Must be an absolute path starting\n  ## with /. If a local file and not absolute, it will be relative to the\n  ## basedir parameter.\n  ##\n  path: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The destination file name. If it starts with s3:// it is assumed to be an\n  ## object in S3, otherwise a local file.\n  ##\n  file: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If the target file is specified as a relative filename, it will be treated\n  ## as relative to the specified directory. Defaults to the lava temporary\n  ## directory for the job.\n  ##\n  # basedir: \"dir\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## AWS KMS encryption key to use when uploading to AWS S3.\n  ##\n  # kms_key_id: \"alias/whatever\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> sharepoint_get_list <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sharepoint_get_list\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for a SharePoint site.\n  ##\n  conn_id: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The destination file name. If it starts with s3:// it is assumed to be an\n  ## object in S3, otherwise a local file.\n  ##\n  file: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Name of the SharePoint list. It must already exist in SharePoint.\n  ##\n  list: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If the target file is specified as a relative filename, it will be treated\n  ## as relative to the specified directory. Defaults to the lava temporary\n  ## directory for the job.\n  ##\n  # basedir: \"dir\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A comma separated list of column names. If specified, then only columns\n  ## listed are extracted (in addition to any specified system_columns).\n  ##\n  # data_columns: \"col1,col2,col3\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Column delimiter. Default is pipe (|)\n  ##\n  # delimiter: \"|\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer.\n  ##\n  # escapechar: null\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If true, include a header line containing column names\n  ##\n  # header: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## AWS KMS encryption key to use when uploading to AWS S3.\n  ##\n  # kms_key_id: \"alias/whatever\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default \"\n  ##\n  # quotechar: '\"'\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer QUOTE_* parameters (without the QUOTE_ prefix). Default\n  ## minimal (i.e. QUOTE_MINIMAL).\n  ##\n  # quoting: \"minimal\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A comma separated list of system columns to retrieve in addition to the\n  ## data columns. Unless specified, only data columns are retrieved.\n  ##\n  # system_columns: \"Author,Created\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the parameters are Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> sharepoint_get_multi_doc <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sharepoint_get_multi_doc\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for a SharePoint site.\n  ##\n  conn_id: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Source SharePoint library name.\n  ##\n  library: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Source document path in SharePoint. Use POSIX, not DOS, style path names\n  ## (i.e. forward slash path separators). Must be an absolute path starting\n  ## with /. If a local file and not absolute, it will be relative to the\n  ## basedir parameter.\n  ##\n  path: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The destination path. If it starts with s3:// it is assumed to be an\n  ## object in S3 with base prefix to store files, otherwise a local directory.\n  ##\n  outpath: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The glob to filter on in source SharePoint path. Only downloads files\n  ## matching this glob direct in the path, and doesn't resurse down folders.\n  ##\n  # glob: \"*.csv\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If the target file is specified as a relative filename, it will be treated\n  ## as relative to the specified directory. Defaults to the lava temporary\n  ## directory for the job.\n  ##\n  # basedir: \"dir\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## AWS KMS encryption key to use when uploading to AWS S3.\n  ##\n  # kms_key_id: \"alias/whatever\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> sharepoint_put_doc <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sharepoint_put_doc\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for a SharePoint site.\n  ##\n  conn_id: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Source SharePoint library name.\n  ##\n  library: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Source document path in SharePoint. Use POSIX, not DOS, style path names\n  ## (i.e. forward slash path separators). Must be an absolute path starting\n  ## with /. If a local file and not absolute, it will be relative to the\n  ## basedir parameter.\n  ##\n  path: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The destination file name. If it starts with s3:// it is assumed to be an\n  ## object in S3, otherwise a local file.\n  ##\n  file: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ##  A title for the document.\n  ##\n  # title: \"Document title\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If the target file is specified as a relative filename, it will be treated\n  ## as relative to the specified directory. Defaults to the lava temporary\n  ## directory for the job.\n  ##\n  # basedir: \"dir\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## AWS KMS encryption key to use when uploading to AWS S3.\n  ##\n  # kms_key_id: \"alias/whatever\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> sharepoint_put_list <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sharepoint_get_list\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for a SharePoint site.\n  ##\n  conn_id: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The source file name. If it starts with s3:// it is assumed to be an\n  ## object in S3, otherwise a local file.\n  ##\n  file: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Name of the SharePoint list. It must already exist in SharePoint.\n  ##\n  list: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If the source file is specified as a relative filename, it will be treated\n  ## as relative to the specified directory. Defaults to the lava temporary\n  ## directory for the job.\n  ##\n  # basedir: \"dir\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A comma separated list of column names. If specified, then only columns\n  ## listed are modified.\n  ##\n  # data_columns: \"col1,col2,col3\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Column delimiter. Default is pipe (|)\n  ##\n  # delimiter: \"|\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.reader. Default false.\n  ##\n  # doubleqoute: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If true and there are columns in the source file that are not in the\n  ## SharePoint list, raise an error. If false, the extra columns are silently\n  ## ignored. Default false.\n  ##\n  # error_missing: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.reader.\n  ##\n  # escapechar: null\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Put mode. Must be append, delete, replace or update. Default is append.\n  ##\n  # mode: \"append\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.reader. Default \"\n  ##\n  # quotechar: '\"'\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.reader QUOTE_* parameters (without the QUOTE_ prefix). Default\n  ## minimal (i.e. QUOTE_MINIMAL).\n  ##\n  # quoting: \"minimal\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the parameters are Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> smb_get <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: smb_get\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for a SMB share.\n  ##\n  conn_id: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The destination file name. If it starts with s3:// it is assumed to be an\n  ## object in S3, otherwise a local file.\n  ##\n  file: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The source file path on the SMB file share. Use POSIX, not DOS, style path\n  ## names (i.e. forward slash path separators). This value is Jinja rendered.\n  ##\n  path: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The name of the file share.\n  ##\n  share: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If the target file is specified as a relative filename, it will be treated\n  ## as relative to the specified directory. Defaults to the lava temporary\n  ## directory for the job.\n  ##\n  # basedir: \"dir\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## AWS KMS encryption key to use when uploading to AWS S3.\n  ##\n  # kms_key_id: \"alias/whatever\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> smb_put <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: smb_put\n\n## #############################################################################\n## The payload is ignored.\n##\npayload: \"--\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for a SMB file share.\n  ##\n  conn_id: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The source file name. If it starts with s3:// it is assumed to be an\n  ## object in S3, otherwise a local file.\n  ##\n  file: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The destination file path on the SMB file share. Use POSIX, not DOS, style\n  ## path names (i.e. forward slash path separators). This value is Jinja\n  ## rendered.\n  ##\n  path: \"???\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The name of the file share.\n  ##\n  share: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If the source file is specified as a relative filename, it will be treated\n  ## as relative to the specified directory. Defaults to the lava temporary\n  ## directory for the job.\n  ##\n  # basedir: \"dir\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If true, the target directory, including parent directories, will be\n  ## created if it doesn\u2019t exist. Default is false\n  ##\n  # create_dirs: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the command arguments and environment are\n  ## Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> sql <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sql\n\n## #############################################################################\n## The payload is a location in S3 relative to the s3_payloads area. It can be\n## either an object key, in which case a single file is downloaded, or a prefix\n## ending in /, in which case all files under that prefix will be downloaded and\n## run in lexicographic order.\n##\npayload: \"???\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for the database.\n  ##\n  conn_id: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Fetch this many rows at a time.\n  ##\n  # batch_size: 1000\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Column delimiter. Default is pipe (|)\n  ##\n  # delimiter: \"|\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default is excel.\n  ##\n  # dialect: \"excel\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default false.\n  ##\n  # doubleqoute: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer.\n  ##\n  # escapechar: null\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Add a header for SELECT outputs if true. Default is false.\n  ##\n  # header: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default \"\n  ##\n  # quotechar: '\"'\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer QUOTE_* parameters (without the QUOTE_ prefix). Default\n  ## minimal (i.e. QUOTE_MINIMAL).\n  ##\n  # quoting: \"minimal\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Suppress splitting of payload files into individual SQL statements.\n  ## Default: false (i.e. allow splitting).\n  ##\n  # raw: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If true, auto-commit is disabled and the sequence of SQLs is run within a\n  ## transaction. If false, auto-commit is enabled. Default false.\n  ##\n  # transaction: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the SQL is Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> sqlc <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sqlc\n\n## #############################################################################\n## The payload is a location in S3 relative to the s3_payloads area. It can be\n## either an object key, in which case a single file is downloaded, or a prefix\n## ending in /, in which case all files under that prefix will be downloaded and\n## run in lexicographic order.\n##\npayload: \"???\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for the database.\n  ##\n  conn_id: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A list of zero or more additional arguments provided to the database client.\n  ## These are necessarily specific to the database type and underlying database\n  ## client.\n  ##\n  # args:\n  #   - \"arg1\"\n  #   - \"arg2\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Timeout for each job component script. Default is 10 minutes. Values are in\n  ## the form nnX where nn is a number and X is s (seconds), m (minutes) or\n  ## h (hours).\n  ##\n  # timeout: \"10m\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the SQL is Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> sqli <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sqli\n\n## #############################################################################\n## The payload is an SQL statement or a list of statements. The YAML multi-line\n## syntax can help here. https://yaml-multiline.info\n##\npayload: |\n  SELECT *\n  FROM my_schema.my_table;\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for the database.\n  ##\n  conn_id: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Fetch this many rows at a time.\n  ##\n  # batch_size: 1000\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Column delimiter. Default is pipe (|)\n  ##\n  # delimiter: \"|\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default is excel.\n  ##\n  # dialect: \"excel\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default false.\n  ##\n  # doubleqoute: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer.\n  ##\n  # escapechar: null\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Add a header for SELECT outputs if true. Default is false.\n  ##\n  # header: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default \"\n  ##\n  # quotechar: '\"'\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer QUOTE_* parameters (without the QUOTE_ prefix). Default\n  ## minimal (i.e. QUOTE_MINIMAL).\n  ##\n  # quoting: \"minimal\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Suppress splitting of payload files into individual SQL statements.\n  ## Default: false (i.e. allow splitting).\n  ##\n  # raw: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If true, auto-commit is disabled and the sequence of SQLs is run within a\n  ## transaction. If false, auto-commit is enabled. Default false.\n  ##\n  # transaction: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the SQL is Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre> sqlv <pre><code>## #############################################################################\n## Mandatory fields\n\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/demo\"\nworker: \"&lt;{ worker.main }&gt;\"\n\ndescription: \"What does this job do?\"\nowner: \"&lt;{ owner }&gt;\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatcher name\n##\n## For scheduled jobs it will be something like...\n# dispatcher: \"&lt;{ dispatcher.main }&gt;\"\n##\n## For non-scheduled jobs it will be omitted\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## enable/disable generation of CloudWatch custom metrics for this job.\n## If not set, use realm level setting.\n##\n# cw_metrics: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Log specified information in the events table before the first iteration\n## commences. Can be a string or an arbitrary object. DON'T LOG SECRETS.\n##\n# event_log: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Limit the number of times a single SQS dispatch message can be processed.\n## Default is 0 which means the limit is determined by SQS queue and worker config.\n##\n# max_tries: 0\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Lava internal retry control\n##\n# iteration_limit: 1\n# iteration_delay: 0s\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Simplest form is cron(1) style schedule.\n##\n# schedule: \"&lt;{ schedule.main }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Global values.\n##\n# globals:\n#   global1: \"value1\"\n#   global2: \"value2\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Post job actions. If not specified here, the realm level values are used.\n## Can have list of multiple actions.\n##\n# on_success:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_retry:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n#\n# on_fail:\n#   - action: action-type\n#     param1: ...\n#     param2: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sqlv\n\n## #############################################################################\n## The payload is a location in S3 relative to the s3_payloads area. It can be\n## either an object key, in which case a single file is downloaded, or a prefix\n## ending in /, in which case all files under that prefix will be downloaded and\n## run in lexicographic order.\n##\npayload: \"???\"\n\n## #############################################################################\n## Job parameters\n\nparameters:\n  ## ---------------------------------------------------------------------------\n  ## Mandatory params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## The connection ID for the database.\n  ##\n  conn_id: \"???\"\n\n  ## ---------------------------------------------------------------------------\n  ## Optional params\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Fetch this many rows at a time.\n  ##\n  # batch_size: 1024\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Column delimiter. Default is pipe (|)\n  ##\n  # delimiter: \"|\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default is excel.\n  ##\n  # dialect: \"excel\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default false.\n  ##\n  # doubleqoute: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer.\n  ##\n  # escapechar: null\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Output format for SELECTs\n  ##\n  # format: csv\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Add a header for SELECT outputs if true. Default is false.\n  ##\n  # header: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n  ## rendering of the args. Default true.\n  ##\n  # jinja: true\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer. Default \"\n  ##\n  # quotechar: '\"'\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## As for csv.writer QUOTE_* parameters (without the QUOTE_ prefix). Default\n  ## minimal (i.e. QUOTE_MINIMAL).\n  ##\n  # quoting: \"minimal\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Suppress splitting of payload files into individual SQL statements.\n  ## Default: false (i.e. allow splitting).\n  ##\n  # raw: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## Timeout for each job component script. Default is 10 minutes. Values are in\n  ## the form nnX where nn is a number and X is s (seconds), m (minutes) or\n  ## h (hours).\n  ##\n  # timeout: \"10m\"\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## If true, auto-commit is disabled and the sequence of SQLs is run within a\n  ## transaction. If false, auto-commit is enabled. Default false.\n  ##\n  # transaction: false\n\n  ## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n  ## A map of variables injected when the SQL is Jinja rendered.\n  ##\n  # vars:\n  #   var1: \"value1\"\n  #   var2: \"value2\"\n</code></pre>"},{"location":"17-job-framework-samples.html#connection-samples","title":"Connection Samples","text":"aws <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: aws\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n# One of `access_keys` or `role_arn` must be specified.\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the access keys.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\naccess_keys: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The ARN of a role to assume. The example shows how to construct an ARN in the\n## same account. Otherwise, a full IAM role ARN is required. Cross account roles\n## can be used.\nrole_arn: \"&lt;{ lava.aws.arn('iam-role', 'some-role') }&gt;\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of an SSM parameter containing a unique identifier that may be required\n## when assuming a (typically cross-account) role.\nexternal_id: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The duration of the role session.\n# duration: 1h\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## IAM managed policies to use as managed session policies.\n# policy_arns:\n#   - policy_arn1\n#   - policy_arn2\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## An IAM policy to use as an inline session policy. This should be expressed as\n## a full policy object. Lava will manage conversion to JSON\n# policy:\n#  Version: '2012-10-17'\n#  Statement:\n#    - Sid: Stmt1\n#      Effect: Allow\n#      Action: 's3:ListAllMyBuckets'\n#      Resource: '*'\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## A dictionary of tags to apply to the assumed role session.\n# tags:\n#  a: tagA\n#  b: tagB\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The AWS region name. If not specified, the current region is assumed.\n##\n# region: \"ap-southeast-2\"\n</code></pre> docker <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: docker\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Email address for registry login\n##\n# email: \"John.Bigboot\u00e9@eigth.dimension.com\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name for authenticating to the registry. Required for private docker\n## repositories. Ignored for ECR registries.\n# user: \"...\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of the SSM parameter containing the password for authenticating to the\n## registry. Required for private docker repositories. Ignored for ECR\n## registries. For a given realm, the SSM parameter name must be of the form\n## /lava/&lt;REALM&gt;/... and the value must be a secure string encrypted using the\n## lava-&lt;REALM&gt;-sys KMS key\n##\n# password: \"/lava/&lt;{ realm }&gt;/...\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Either the URL for a standard registry or ecr[:account-id]. In the latter\n## case, lava will connect to the AWS ECR registry in the specified AWS account\n## or the current account if no account-id is specified. If no registry is\n## specified, the default public docker registry is used\n##\n# registry: \"aws\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## URL for the docker server. If not specified, then the normal docker\n## environment variables are used. Generally, this means using the local docker\n## daemon accessed via the UNIX socket.\n##\n# server: \"...\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Timeout on docker API calls in seconds\n##\n# timeout: 10\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Use TLS when connecting to the docker server. Default True.\n##\n# tls: true\n</code></pre> email <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: ses\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The email handler subtype. Default is ses.\n# subtype: ses\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The email address that is sending the email. If not specified, a value must\n## specified at the realm level.\n##\n# from: \"y2@colossal.cave\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Reply-to address(es). Can be string or list of strings.\n##\n# reply_to: \"bedquilt@colossal.cave\"\n\n## -----------------------------------------------------------------------------\n## ses subtype specific fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The AWS region name for SES. If not specified, us-east-1 is used.\n##\n# region: \"us-east-1\"\n</code></pre> generic <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: generic\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\nattributes:\n  attr1: Sample value\n  attr2:\n    type: ssm\n    parameter: ssm-parameter-name\n\n## -----------------------------------------------------------------------------\n## Optional fields\n</code></pre> git <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: git\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the SSH private key.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\nssh_key: \"/lava/&lt;{ realm }&gt;/???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n</code></pre> mariadb-rds <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: mysql\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pymysql\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of a file containing the CA certificate for the database server.\n## Ignored unless ssl is true.\n##\n# ca_cert: \"/usr/local/lib/rds/rds-ca-2019-root.pem\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> mariadb <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: mysql\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pymysql\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of a file containing the CA certificate for the database server.\n## Ignored unless ssl is true.\n##\n# ca_cert: \"/usr/local/lib/rds/rds-ca-2019-root.pem\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> mysql-aurora <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: mysql\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pymysql\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of a file containing the CA certificate for the database server.\n## Ignored unless ssl is true.\n##\n# ca_cert: \"/usr/local/lib/rds/rds-ca-2019-root.pem\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> mysql-rds <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: mysql\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pymysql\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of a file containing the CA certificate for the database server.\n## Ignored unless ssl is true.\n##\n# ca_cert: \"/usr/local/lib/rds/rds-ca-2019-root.pem\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> mysql <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: mysql\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pymysql\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of a file containing the CA certificate for the database server.\n## Ignored unless ssl is true.\n##\n# ca_cert: \"/usr/local/lib/rds/rds-ca-2019-root.pem\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> oracle-rds <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: oracle\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Oracle version for compatibility in the form x.y[.z].\n##\n# edition: \"x.y.z\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: cx_oracle\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The Oracle database service name. Generally exactly one of service_name or\n## sid must be specified.\n##\n# service_name: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The Oracle System Identifier of the database. Generally exactly one of\n## service_name or sid must be specified.\n##\n# sid: \"???\"\n</code></pre> oracle <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: oracle\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Oracle version for compatibility in the form x.y[.z].\n##\n# edition: \"x.y.z\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: cx_oracle\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The Oracle database service name. Generally exactly one of service_name or\n## sid must be specified.\n##\n# service_name: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The Oracle System Identifier of the database. Generally exactly one of\n## service_name or sid must be specified.\n##\n# sid: \"???\"\n</code></pre> postgres-aurora <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: postgres\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pg8000\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> postgres-rds <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: postgres\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pg8000\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> postgres <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: postgres\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pg8000\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> psql <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: postgres\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database (schema) within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pg8000\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n</code></pre> redshift-serverless <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: redshift-serverless\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database within the Redshift Serverless namespace.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The Redshift serverless workgroup endpoint address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n## If not provided, IAM authentication to Redshift Serverless is attempted.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name. This is required except when using secrets managaer or IAM based\n## authentication.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of an SSM parameter containing an external ID to use when assuming the\n## IAM role specified by role_arn.\n##\n# external_id: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The password duration when generating temporary IAM user credientials.\n##\n# password_duration: \"15m\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## don't fold database object names to lower case when quoting for db_from_s3 jobs\n##\n# preserve_case: False\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## ARN of IAM role to be assumed when generating temporary IAM user credentials.\n##\n# role_arn: ...\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false.\n##\n# ssl: false\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pg8000\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of the workgroup associated with the database. Used when generating\n## temporary IAM user credentials. Defaults to the first component of the host.\n##\n# workgroup: ...\n</code></pre> redshift <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: redshift\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Conditional fields - Required but may come from Secrets Manager\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of the database within the database server.\n##\ndatabase: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the password.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n## If not provided, IAM authentication to Redshift is attempted.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Port number.\n##\nport: ???\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The Redshift cluster identifier. Default is first part of host name.\n##\n# cluster_id: my_cluster\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Name of a secret in AWS Secrets Manager\n# secret_id: /lava/&lt;{ realm }&gt;/secret-name\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The password duration when generating temporary IAM user credientials.\n##\n# password_duration: \"15m\"\n\n### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n### don't fold database object names to lower case when quoting for db_from_s3 jobs\n###\n## preserve_case: False\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Set to true to enable SSL. Default is false\n##\n# ssl: false\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The subtype specifies the driver to use.\n# subtype: pg8000\n</code></pre> scp <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: scp\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the SSH private key.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\nssh_key: \"/lava/&lt;{ realm }&gt;/???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n</code></pre> ses <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\n## -----------------------------------------------------------------------------\n## This is a legacy connector. Use email instead.\n## -----------------------------------------------------------------------------\n\ntype: ses\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The email address that is sending the email. If not specified, a value must\n## specified at the realm level.\n##\n# from: \"y2@colossal.cave\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Reply-to address(es). Can be string or list of strings.\n##\n# reply_to: \"bedquilt@colossal.cave\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The email address that bounces and complaints will be forwarded to when\n## feedback forwarding is enabled\n##\n# return_path: \"plugh@colossal.cave\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The AWS region name for SES. If not specified, us-east-1 is used.\n##\n# region: \"us-east-1\"\n</code></pre> sftp <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sftp\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the SSH private key.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\nssh_key: \"/lava/&lt;{ realm }&gt;/???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n</code></pre> sharepoint <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: sharepoint\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The Application ID that the SharePoint registration portal assigned your app.\n## This resembles a UUID.\n##\nclient_name: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## SSM parameter containing the client secret.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\nclient_secret: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The organisation\u2019s SharePoint base URL. e.g. acme.sharepoint.com.\n##\norg_base_url: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## SSM parameter containing password for authenticating to SharePoint.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## SharePoint site name.\n##\nsite_name: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Azure AD registered domain ID.\ntenant: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name for authenticating to SharePoint.\n##\n# user: \"???\"\n</code></pre> smb <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: smb\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The database host DNS name or IP address.\n##\nhost: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## NetBIOS machine name of the remote server.\n##\nremote_name: \"???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## SSM parameter containing password for authenticating to the SMB server.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\npassword: \"/lava/&lt;{ realm }&gt;/???\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## User name for authenticating to the SMB server.\n##\nuser: \"???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The connection subtype. Choose between 'pysmb' and 'smbprotocol'. smbprotocol\n## supports encryption and access via DFS. Defaults to 'pysmb'.\n##\n# subtype: \"smbprotocol\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The network domain. Defaults to an empty string.\n##\n## In the 'smbprotocol' job subtype, domain can refer to a DFS domain.\n## Connecting via DFS is not supported in the 'pysmb' job subtype.\n##\n# domain: \"\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## A custom port to use. Defaults to 445 if is_direct_tcp is True else 139.\n##\n# port: 445\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## If false, request and transfer files without encryption. If true use encryption.\n##\n## Default is True when using the 'smbprotocol' job subtype.\n## The 'pysmb' job subtype doesn't support encryption.\n##\n# encrypt: false\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## If false, use NetBIOS over TCP/IP. If true use SMB over TCP/IP. Default false.\n##\n# is_direct_tcp: false\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Local NetBIOS machine name that will identify the origin of connections. If\n## not specified, defaults to the first 15 characters of lava-&lt;REALM&gt;\n##\n# my_name: \"&lt;{ ('lava-' + realm)[:15] }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Indicates whether pysmb should be NTLMv1 or NTLMv2 authentication algorithm\n## for authentication. Default is true.\n##\n# use_ntlm_v2: true\n</code></pre> ssh <pre><code>## #############################################################################\n## Mandatory fields\n##\nconn_id: \"???\"\nenabled: true\n\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n\ntype: ssh\n\n## #############################################################################\n## Connector type specific fields\n\n## -----------------------------------------------------------------------------\n## Mandatory fields\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The name of an encrypted SSM parameter containing the SSH private key.\n## This must be a secure string encrypted using the lava-&lt;REALM&gt;-sys KMS key.\n##\nssh_key: \"/lava/&lt;{ realm }&gt;/???\"\n\n## -----------------------------------------------------------------------------\n## Optional fields\n</code></pre>"},{"location":"17-job-framework-samples.html#rule-samples","title":"Rule Samples","text":"common <pre><code>## #############################################################################\n## Mandatory fields\n##\nrule_id: \"&lt;{ prefix.rule }&gt;.demo\"\nenabled: true\nowner: \"&lt;{ owner }&gt;\"\ndescription: \"Rule description\"\n\n## #############################################################################\n## Optional fields\n##\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Event bus name. This should almost always be left out to use the default bus.\n##\n# event_bus_name: default\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Event pattern. This will almost always be required. The example shown is an\n## S3 object creation event.\n##\n# event_pattern:\n#   detail:\n#     bucket:\n#       name:\n#         - my-bucket\n#     object:\n#       key:\n#         - prefix: a/prefix/in/the/bucket/\n#   detail-type:\n#     - Object Created\n#   source:\n#     - aws.s3\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Schedule expression (cron or rate based). Usually only one of event_pattern\n## and schedule_expression are required.\n##\n# schedule_expression: \"rate(1 day)\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## The ARN of the IAM role associated with the rule. Not needed for basic use\n## cases such as s3trigger and logging events to CloudWatch.\n##\n# role_arn: \"&lt;{ lava.aws.arn('iam-role', 'my-role-name') }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Event targets. This is required if you want the rule to do anything. The\n## example shown is for using s3trigger to dispatch a job on an S3 event.\n##\n# targets:\n#   # Construct the ARN for the realm s3trigger lambda\n#   - &lt;{ lava.aws.arn('lambda-function', 'lava-' + realm + '-s3trigger') }&gt;\n#   # Let's log messages in CloudWatch logs\n#   - &lt;{ lava.aws.arn('log-group', '/aws/events/lava') }&gt;\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Tags for the rule.\n##\n# tags:\n#   key1: val1\n#   key2: val2\n</code></pre>"},{"location":"17-job-framework-samples.html#s3trigger-samples","title":"S3trigger Samples","text":"common <pre><code>## #############################################################################\n## Mandatory fields\n##\ntrigger_id: \"&lt;{ prefix.s3trigger }&gt;/???\"\nenabled: true\njob_id: \"&lt;{ prefix.job }&gt;/???\"\nbucket: \"???\"\nprefix: \"???\"\n\n## #############################################################################\n## Optional fields\n##\ndescription: \"...\"\nowner: \"&lt;{ owner }&gt;\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Dispatch message sending delay in the form nnX where nn is a number and X\n## is s (seconds) or m (minutes). The maximum allowed value is 15 minutes.\n##\n# delay: \"5m\"\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Enable / disable jinja rendering. Default is true. If false, disable Jinja\n## rendering of the parameters and globals.\n##\n# jinja: true\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Globals for the dispatched job\n##\n# globals:\n#   global1: val1\n#   global2: val2\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## Parameters for the dispatched job\n##\n# parameters:\n#   param1: val1\n#   param2: val2\n\n\n## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n## X-* / x-* vars are ignored by lava but may be useful for other purposes.\n##\n# X-whatever: \"Some value\"\n</code></pre>"},{"location":"18-api.html","title":"Lava API Reference","text":"<p>Lava has a relatively narrow client API that focuses on the connector subsystem and a few generic utilities.</p> <p>The API, which includes the main lava worker and all of the lava utilities is available on PyPI as the <code>jinlava</code> package.</p> <pre><code>pip install jinlava\n\n# Include support for PyGreSQL, the AWS Redshift driver, Pyodbc etc.\npip install 'jinlava[extras]'\n</code></pre> <p>The lava internals are (deliberately) not catalogued here to avoid distracting those who are just interested in developing lava exe,  pkg and docker payloads.</p>"},{"location":"18-api.html#lava","title":"lava","text":"<p>Lava specific modules.</p>"},{"location":"18-api.html#lava.LavaError","title":"lava.LavaError","text":"<pre><code>LavaError(*args, data: Any = None, **kwargs)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Lava specific exception.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Any JSON serialisable object.</p> <code>None</code> <p>Create a LavaError.</p>"},{"location":"18-api.html#lava.dispatch","title":"lava.dispatch","text":"<pre><code>dispatch(\n    realm: str,\n    job_id: str,\n    worker: str = None,\n    params: dict[str, Any] = None,\n    delay: int = 0,\n    queue_name: str = None,\n    aws_session: Session = None,\n    globals_: dict[str, Any] = None,\n) -&gt; str\n</code></pre> <p>Send a dispatch message for the specified realm / job.</p> <p>Parameters:</p> Name Type Description Default <code>realm</code> <code>str</code> <p>The realm name.</p> required <code>job_id</code> <code>str</code> <p>The ID of the job to dispatch.</p> required <code>worker</code> <code>str</code> <p>The target worker name. If not specified, look up the worker name in the job table.</p> <code>None</code> <code>params</code> <code>dict[str, Any]</code> <p>An optional dictionary of parameters to include in the dispatch.</p> <code>None</code> <code>globals_</code> <code>dict[str, Any]</code> <p>An optional dictionary of global attributes to include in the dispatch.</p> <code>None</code> <code>delay</code> <code>int</code> <p>Delay in seconds for the dispatch SQS message. This delay is handled by SQS itself and is thus limited to values acceptable to SQS. Default is 0.</p> <code>0</code> <code>queue_name</code> <code>str</code> <p>Name of the dispatch SQS queue. If not specified, the value is derived from the realm and worker names. It should almost never be specified. Default None.</p> <code>None</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session object. If not specified, a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The run ID.</p>"},{"location":"18-api.html#lava.get_job_spec","title":"lava.get_job_spec","text":"<pre><code>get_job_spec(job_id: str, jobs_table) -&gt; dict[str, Any]\n</code></pre> <p>Get the job spec from the DynamoDB table.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job ID.</p> required <code>jobs_table</code> <p>DynamoDB jobs table resource.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The job spec.</p>"},{"location":"18-api.html#lava.get_realm_info","title":"lava.get_realm_info","text":"<pre><code>get_realm_info(realm: str, realm_table) -&gt; dict[str, Any]\n</code></pre> <p>Get the realm record from DynamoDB for the given realm.</p> <p>Parameters:</p> Name Type Description Default <code>realm</code> <code>str</code> <p>The realm.</p> required <code>realm_table</code> <p>DynamoDB realm table resource.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The realm specification.</p>"},{"location":"18-api.html#lava.scan_jobs","title":"lava.scan_jobs","text":"<pre><code>scan_jobs(\n    realm: str,\n    attributes: Iterable[str] = None,\n    aws_session: Session = None,\n) -&gt; dict[str, dict[str, str]]\n</code></pre> <p>Scan a realm for jobs.</p> <p>Parameters:</p> Name Type Description Default <code>realm</code> <code>str</code> <p>Realm name</p> required <code>attributes</code> <code>Iterable[str]</code> <p>An iterable of attribute names to return. If None then just the job name will be returned. The job name is always included no matter what is requested.</p> <code>None</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session(). If not specified, a default session will be created.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, str]]</code> <p>A dictionary mapping job name to selected job attributes.</p>"},{"location":"18-api.html#lava.scan_realms","title":"lava.scan_realms","text":"<pre><code>scan_realms(\n    attributes: Iterable[str] = None,\n    aws_session: Session = None,\n) -&gt; dict[str, dict[str, str]]\n</code></pre> <p>Scan the available realms.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>Iterable[str]</code> <p>An iterable of attribute names to return. If None then just the realm name will be returned. The realm name is always included no what is requested.</p> <code>None</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session(). If not specified, a default session will be created.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, str]]</code> <p>A dictionary mapping realm name to selected realm attributes.</p>"},{"location":"18-api.html#lava.connection","title":"lava.connection","text":"<p>Lava connection subssystem API.</p>"},{"location":"18-api.html#lava.connection.cli_connect_generic","title":"lava.connection.cli_connect_generic","text":"<pre><code>cli_connect_generic(\n    conn_spec: dict[str, Any],\n    workdir: str,\n    aws_session: Session = None,\n) -&gt; str\n</code></pre> <p>Generate a CLI command to deliver values from a generic connector.</p> <p>The CLI command accepts a single positional argument that is the name of the element in the generic connection. See Using the Generic Connector for more information.</p> <p>Parameters:</p> Name Type Description Default <code>conn_spec</code> <code>dict[str, Any]</code> <p>Connection specification</p> required <code>workdir</code> <code>str</code> <p>Working directory name.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session(). This is used to get credentials.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Name of an executable that implements the connection.</p>"},{"location":"18-api.html#lava.connection.get_aws_connection","title":"lava.connection.get_aws_connection","text":"<pre><code>get_aws_connection(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; dict[str, str]\n</code></pre> <p>Extract a set of AWS access credentials from the connections table.</p> <p>It is up to the caller to do something useful with those (e.g. create a boto3 Session()).</p> <p>This supports conventional access keys as well as the option of assuming an IAM role, including cross-account roles.</p> <p>Note</p> <p>The <code>get_aws_connection</code> naming is a bit of poetic licence, as it doesn't actually connect to anything. In most cases get_aws_session is a better option.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID.</p> required <code>realm</code> <code>str</code> <p>Realm</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session(). If not specified, a default session is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>A dict containing <code>aws_access_key_id</code>, <code>aws_secret_access_key</code> <code>aws_session_token</code> (conditional) and <code>region</code>.</p>"},{"location":"18-api.html#lava.connection.get_aws_session","title":"lava.connection.get_aws_session","text":"<pre><code>get_aws_session(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; boto3.Session\n</code></pre> <p>Get a boto3 session based on the details specified in a lava AWS connection.</p> <p>This supports conventional access keys as well as the option of assuming an IAM role, including cross-account roles.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID.</p> required <code>realm</code> <code>str</code> <p>Realm</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session() used to access the local AWS environment. If not specified, a default session is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>Session</code> <p>A boto3 Session().</p>"},{"location":"18-api.html#lava.connection.get_cli_connection","title":"lava.connection.get_cli_connection","text":"<pre><code>get_cli_connection(\n    conn_id: str,\n    realm: str,\n    workdir: str,\n    aws_session: Session = None,\n) -&gt; str\n</code></pre> <p>Get a CLI connection to a resource.</p> <p>This will produce an command line executable that will establish a connection to the target resource, managing authentication as required. The usage of the executable is dependent on the nature of the resource. The caller needs to know how to call it.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID</p> required <code>realm</code> <code>str</code> <p>Realm</p> required <code>workdir</code> <code>str</code> <p>Working directory. This is required in case the connection handler needs to store stuff like credential files.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Name of an executable that implements the connection.</p>"},{"location":"18-api.html#lava.connection.get_connection_spec","title":"lava.connection.get_connection_spec","text":"<pre><code>get_connection_spec(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; dict[str, Any]\n</code></pre> <p>Read a connection specification from DynamoDB and do some basic checking.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID.</p> required <code>realm</code> <code>str</code> <p>Realm</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The connection spec.</p>"},{"location":"18-api.html#lava.connection.get_docker_connection","title":"lava.connection.get_docker_connection","text":"<pre><code>get_docker_connection(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; docker.DockerClient\n</code></pre> <p>Get a docker client connection with a login to a registry.</p> <p>The caller is expected to close it when done. Connection type must be <code>docker</code>.</p> <p>Allowed connection params are:</p> <ul> <li> <p><code>registry</code>:     Either hostname:port or <code>ecr:[account]</code>. If not specified, no registry     login is done.</p> </li> <li> <p><code>user</code>:     Username for the registry. If registry is <code>ecr</code> type then this is ignored     and the AWS ECR API is used to get credentials.</p> </li> <li> <p><code>password</code>:     SSM key containing the password for the registry. If registry is <code>ecr</code>     type then this is ignored and the AWS ECR API is used to get     credentials.</p> </li> <li> <p><code>email</code>:     Optional email address for registry login.</p> </li> <li> <p><code>server</code>:     URL for the docker server. If not specified then the environment is     used.</p> </li> <li> <p><code>tls</code>:     Enable TLS. Boolean. Default is True.</p> </li> <li> <p><code>timeout</code>:     Timeout on API calls in seconds.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID. If None, then just get a default connection using local environment.</p> required <code>realm</code> <code>str</code> <p>Realm</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session(). If not specified a default session is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>DockerClient</code> <p>A docker client.</p>"},{"location":"18-api.html#lava.connection.get_email_connection","title":"lava.connection.get_email_connection","text":"<pre><code>get_email_connection(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; Emailer\n</code></pre> <p>Get a connection to an email sender.</p> <p>This returns a lava.lib.email.Emailer instance which can be used as a context manager. Otherwise it is up to the caller to call the handler's <code>close()</code> method.</p> <p>This provides a common interface to an email sending subsystem, independent of the underlying sending mechanism. Different sending mechanisms can be selected using the <code>subtype</code> field in the connection spec. The default handler is AWS SES.</p> <p>Typical usage would be:</p> <pre><code>with get_email_connection('my_conn_id', 'realm-name') as emailer:\n    emailer.send(to='x@y.com', subject='Hello', message='world')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID</p> required <code>realm</code> <code>str</code> <p>Realm.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <p>Returns:</p> Type Description <code>Emailer</code> <p>An Emailer instance.</p>"},{"location":"18-api.html#lava.connection.get_generic_connection","title":"lava.connection.get_generic_connection","text":"<pre><code>get_generic_connection(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; dict[str, Any]\n</code></pre> <p>Get a generic connection.</p> <p>Connection params area:</p> <ul> <li><code>attributes</code>:     Essentially a map of key:value pairs. See the     generic connector for more     information.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID.</p> required <code>realm</code> <code>str</code> <p>Realm.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of resolved attributes.</p>"},{"location":"18-api.html#lava.connection.get_pysql_connection","title":"lava.connection.get_pysql_connection","text":"<pre><code>get_pysql_connection(\n    conn_id: str,\n    realm: str,\n    autocommit: bool = False,\n    aws_session: Session = None,\n    application_name: str = None,\n)\n</code></pre> <p>Get a Python connection to the specified SQL database.</p> <p>The specifics depend on the underlying database type as specified on the connection info.</p> <p>We are assuming a DBAPI 2.0 interface.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID</p> required <code>realm</code> <code>str</code> <p>Realm</p> required <code>autocommit</code> <code>bool</code> <p>If True, attempt to enable autocommit. This is database and driver dependent as not all DBs support it (e.g. sqlite3) If False, autocommit is not enabled (the default state for DBAPI 2.0).</p> <code>False</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <code>application_name</code> <code>str</code> <p>If possible, set the the application name when connecting to the database. This is supported for Postgres-like DBs, and in various ways, for some others but not all database types. If not specified, we try to construct a fallback value from some lava specific environment variables.</p> <code>None</code> <p>Returns:</p> Type Description <p>A live DB connection</p>"},{"location":"18-api.html#lava.connection.get_sharepoint_connection","title":"lava.connection.get_sharepoint_connection","text":"<pre><code>get_sharepoint_connection(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; Sharepoint\n</code></pre> <p>Get a sharepoint connection to the specified sharepoint site.</p> <p>Return a Sharepoint instance which uses a Microsoft Graph API interface.</p> <p>Connection params are (* means optional):</p> <ul> <li> <p><code>org_base_url</code>:     The organisation's SharePoint base URL.</p> </li> <li> <p><code>site_name</code>:     The SharePoint site name.</p> </li> <li> <p><code>tenant</code>:     Azure AD registered domain ID.</p> </li> <li> <p><code>client_id</code>:     The Application ID that the SharePoint registration portal assigned your app.</p> </li> <li> <p><code>client_secret</code>:     SSM key containing the client secret.</p> </li> <li> <p><code>user</code>:     Name of the user for login to SharePoint.</p> </li> <li> <p><code>password</code>:     SSM key containing the user's password.</p> </li> <li> <p><code>https_proxy</code>:     HTTPS proxy to use for accessing the SharePoint API endpoints. If not     specified the HTTPS_PROXY environment variable is used if set.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID.</p> required <code>realm</code> <code>str</code> <p>Realm.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <p>Returns:</p> Type Description <code>Sharepoint</code> <p>A Sharepoint instance.</p>"},{"location":"18-api.html#lava.connection.get_slack_connection","title":"lava.connection.get_slack_connection","text":"<pre><code>get_slack_connection(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; Slack\n</code></pre> <p>Get a connection to a Slack message sender.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID</p> required <code>realm</code> <code>str</code> <p>Realm.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <p>Returns:</p> Type Description <code>Slack</code> <p>A Slack instance.</p>"},{"location":"18-api.html#lava.connection.get_smb_connection","title":"lava.connection.get_smb_connection","text":"<pre><code>get_smb_connection(\n    conn_id: str, realm: str, aws_session: Session = None\n) -&gt; LavaSMBConnection\n</code></pre> <p>Get a Python connection to the specified SMB fileshare.</p> <p>The subtype specifies the python package used (pysmb/smbprotocol).</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID</p> required <code>realm</code> <code>str</code> <p>Realm</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <p>Returns:</p> Type Description <code>LavaSMBConnection</code> <p>An active SMB connection.</p>"},{"location":"18-api.html#lava.connection.get_sqlalchemy_engine","title":"lava.connection.get_sqlalchemy_engine","text":"<pre><code>get_sqlalchemy_engine(\n    conn_id: str,\n    realm: str,\n    aws_session: Session = None,\n    application_name: str = None,\n    **kwargs\n) -&gt; sqlalchemy.engine.Engine\n</code></pre> <p>Get an SQLalchemy Engine() instance for the specified SQL database.</p> <p>The specifics depend on the underlying database type as specified on the connection info.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Connection ID</p> required <code>realm</code> <code>str</code> <p>Realm</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code> <code>application_name</code> <code>str</code> <p>Application name when connecting.</p> <code>None</code> <code>kwargs</code> <p>All other args are passed to sqlalchemy.create_engine. Be careful.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Engine</code> <p>An SQLalchemy Engine instance for the specified SQL database.</p>"},{"location":"18-api.html#lava.connection.install_ssh_key","title":"lava.connection.install_ssh_key","text":"<pre><code>install_ssh_key(\n    ssm_param_name: str,\n    key_file_name: str,\n    aws_session: Session = None,\n) -&gt; None\n</code></pre> <p>Extract an SSH key from SSM and put it in a file.</p> <p>Parameters:</p> Name Type Description Default <code>ssm_param_name</code> <code>str</code> <p>SSM parameter name containing the key. This may have been joined into a single line with commas replacing line breaks and will need to be unpacked before use.</p> required <code>key_file_name</code> <code>str</code> <p>Name of file in which to place the key.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> <code>None</code>"},{"location":"18-api.html#lava.lib","title":"lava.lib","text":"<p>General library modules.</p>"},{"location":"18-api.html#lava.lib.argparse","title":"lava.lib.argparse","text":"<p>Argparse utilities.</p>"},{"location":"18-api.html#lava.lib.argparse.ArgparserExitError","title":"ArgparserExitError","text":"<p>               Bases: <code>Exception</code></p> <p>When a premature exit from argparse is suppressed.</p>"},{"location":"18-api.html#lava.lib.argparse.ArgparserNoExit","title":"ArgparserNoExit","text":"<p>               Bases: <code>ArgumentParser</code></p> <p>Argparse that throws exception on bad arg instead of exiting.</p>"},{"location":"18-api.html#lava.lib.argparse.ArgparserNoExit.exit","title":"exit","text":"<pre><code>exit(status=0, message=None)\n</code></pre> <p>Stop argparse from exiting on bad options.</p>"},{"location":"18-api.html#lava.lib.argparse.StoreNameValuePair","title":"StoreNameValuePair","text":"<p>               Bases: <code>Action</code></p> <p>Used with argparse to store values from options of the form <code>--option name=value</code>.</p> <p>The destination (self.dest) will be created as a dict {name: value}. This allows multiple name-value pairs to be set for the same option.</p> <p>Usage is:</p> <pre><code>argparser.add_argument('-x', metavar='key=value', action=StoreNameValuePair)\n</code></pre> <p>... or ...</p> <pre><code>argparser.add_argument(\n    '-x', metavar='key=value ...', action=StoreNameValuePair, nargs='+'\n)\n</code></pre>"},{"location":"18-api.html#lava.lib.aws","title":"lava.lib.aws","text":"<p>AWS utilities.</p>"},{"location":"18-api.html#lava.lib.aws.cw_put_metric","title":"cw_put_metric","text":"<pre><code>cw_put_metric(\n    metric: str,\n    namespace: str,\n    dimensions: OrderedDict | list[dict[str, Any]],\n    value: float | int,\n    unit: str = \"None\",\n    resolution: str = \"low\",\n    cw_client=None,\n) -&gt; None\n</code></pre> <p>Send metric data to CloudWatch.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>Metric name.</p> required <code>namespace</code> <code>str</code> <p>CloudWatch namespace. If None then this is a no-op.</p> required <code>dimensions</code> <code>OrderedDict | list[dict[str, Any]]</code> <p>Either an ordered dict or a a list of dictionaries with a single key/value</p> required <code>value</code> <code>float | int</code> <p>The metric value.</p> required <code>unit</code> <code>str</code> <p>The metric unit. If not specified the default of None translates to CloudWatch 'None'.</p> <code>'None'</code> <code>resolution</code> <code>str</code> <p>Either 'hi'/'high' (1 second resolution or 'lo'/'low' (1 minute resolution). Default is 'low'.</p> <code>'low'</code> <code>cw_client</code> <p>CloudWatch client. If None then this is a no-op.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.aws.dynamo_scan_table","title":"dynamo_scan_table","text":"<pre><code>dynamo_scan_table(\n    table: str, aws_session: Session\n) -&gt; Iterator[dict[str, Any]]\n</code></pre> <p>Scan the specified DynamoDB table and return the items one at a time.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The table name.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session().</p> required <p>Returns:</p> Type Description <code>Iterator[dict[str, Any]]</code> <p>An iterator yielding items from the table.</p>"},{"location":"18-api.html#lava.lib.aws.dynamo_unmarshall_item","title":"dynamo_unmarshall_item","text":"<pre><code>dynamo_unmarshall_item(\n    item: dict[str, Any],\n) -&gt; dict[str, Any]\n</code></pre> <p>Convert a DynamoDB structured table item to a normal Python structure.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>dict[str, Any]</code> <p>The DynamoDB item (as a Python object)</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Standard Python object.</p>"},{"location":"18-api.html#lava.lib.aws.dynamo_unmarshall_value","title":"dynamo_unmarshall_value","text":"<pre><code>dynamo_unmarshall_value(v: dict[str, Any]) -&gt; Any\n</code></pre> <p>Convert a DynamoDB structured value to a normal Python structure.</p> <p>Handles simple and compound types.</p> <p>For example, the following are typical conversions:</p> <pre><code>{ 'S': 'abc' } --&gt; 'abc'\n{ 'BOOL': True } --&gt; True\n{ 'NULL': True } --&gt; None\n{ 'N': '99.9' } --&gt; 99.9\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>dict[str, Any]</code> <p>The DynamoDB value.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Python object</p>"},{"location":"18-api.html#lava.lib.aws.ec2_instance_id","title":"ec2_instance_id  <code>cached</code>","text":"<pre><code>ec2_instance_id() -&gt; str\n</code></pre> <p>Get the current machine's EC2 instance ID.</p> <p>Assumes IMDSv2.</p> <p>Returns:</p> Type Description <code>str</code> <p>The EC2 instance ID.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the instance does not appear to be EC2.</p>"},{"location":"18-api.html#lava.lib.aws.s3_bucket_is_encrypted","title":"s3_bucket_is_encrypted","text":"<pre><code>s3_bucket_is_encrypted(\n    bucket_name: str, aws_session: Session = None\n) -&gt; bool\n</code></pre> <p>Check if a bucket has default encryption enabled.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The bucket name</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the bucket has default encryption enabled.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the bucket doesn't exist or encryption status cannot be determined.</p>"},{"location":"18-api.html#lava.lib.aws.s3_bucket_is_mine","title":"s3_bucket_is_mine","text":"<pre><code>s3_bucket_is_mine(\n    bucket_name: str, aws_session: Session = None\n) -&gt; bool\n</code></pre> <p>Determine if a bucket is owned by the current account.</p> <p>Requires IAM list buckets permission and access to the bucket ACL. Results are cached as buckets are not likely to change owning accounts.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The bucket name</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the bucket is owned by the current account.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the bucket doesn't exist or the ownership cannot be determined.</p>"},{"location":"18-api.html#lava.lib.aws.s3_bucket_is_public","title":"s3_bucket_is_public","text":"<pre><code>s3_bucket_is_public(\n    bucket_name: str, aws_session: Session = None\n) -&gt; bool\n</code></pre> <p>Determine if a bucket has any public visibility.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The bucket name</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if there is any public access to the bucket.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the bucket doesn't exist or the ACL cannot be read.</p>"},{"location":"18-api.html#lava.lib.aws.s3_bucket_is_server_logging_enabled","title":"s3_bucket_is_server_logging_enabled","text":"<pre><code>s3_bucket_is_server_logging_enabled(\n    bucket_name: str, aws_session: Session = None\n) -&gt; bool\n</code></pre> <p>Check if a bucket has server logging enabled.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The bucket name</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the bucket has logging enabled.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the bucket doesn't exist or logging status cannot be determined.</p>"},{"location":"18-api.html#lava.lib.aws.s3_check_bucket_security","title":"s3_check_bucket_security","text":"<pre><code>s3_check_bucket_security(\n    bucket_name: str,\n    require_no_public_access: bool = True,\n    require_encryption: bool = True,\n    require_server_logging: bool = True,\n    require_bucket_is_mine: bool = True,\n    aws_session: Session = None,\n) -&gt; None\n</code></pre> <p>Perform a bunch of security checks on am S3 bucket.</p> <p>These are described by the various require_* variables.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The bucket name</p> required <code>require_no_public_access</code> <code>bool</code> <p>If True the bucket must not have any public access. Default True.</p> <code>True</code> <code>require_encryption</code> <code>bool</code> <p>If True the bucket must have default encryption enabled. Default True.</p> <code>True</code> <code>require_server_logging</code> <code>bool</code> <p>If True the bucket must have default server logging enabled. Default True.</p> <code>True</code> <code>require_bucket_is_mine</code> <code>bool</code> <p>If True the bucket must belong to the current AWS account. Default True.</p> <code>True</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Nothing.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the bucket fails any of the security checks or the status of any of the checks cannot be verified.</p>"},{"location":"18-api.html#lava.lib.aws.s3_download","title":"s3_download","text":"<pre><code>s3_download(\n    bucket: str, key: str, filename: str, s3_client\n) -&gt; None\n</code></pre> <p>Download object from S3 bucket to a local file.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>S3 bucket name of source.</p> required <code>key</code> <code>str</code> <p>Key of source object in source bucket.</p> required <code>filename</code> <code>str</code> <p>Name of local file in which the object is stored.</p> required <code>s3_client</code> <p>boto3 s3 client.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>On failure</p>"},{"location":"18-api.html#lava.lib.aws.s3_list","title":"s3_list","text":"<pre><code>s3_list(\n    bucket: str,\n    prefix: str = None,\n    match: str = None,\n    not_match: str = None,\n    s3_client=None,\n) -&gt; Iterator[str]\n</code></pre> <p>List an area of an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Bucket name.</p> required <code>prefix</code> <code>str</code> <p>An optional prefix.</p> <code>None</code> <code>match</code> <code>str</code> <p>An optional glob style pattern to select files.</p> <code>None</code> <code>not_match</code> <code>str</code> <p>An optional glob style pattern to skip files.</p> <code>None</code> <code>s3_client</code> <p>A boto3 s3 client. One is created if not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>A generator of object names (without the bucket).</p>"},{"location":"18-api.html#lava.lib.aws.s3_load_json","title":"s3_load_json","text":"<pre><code>s3_load_json(\n    bucket: str, key: str, aws_session: Session = None\n) -&gt; Any\n</code></pre> <p>Load a JSON file from S3.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the S3 bucket.</p> required <code>key</code> <code>str</code> <p>Object key (file name) for the JSON file</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The object decoded from the JSON file.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If file doesn't exist or can't be retrieved.</p> <code>ValueError</code> <p>If the file was retrieved but is not valid JSON.</p>"},{"location":"18-api.html#lava.lib.aws.s3_object_exists","title":"s3_object_exists","text":"<pre><code>s3_object_exists(\n    bucket: str, key: str, aws_session: Session = None\n) -&gt; bool\n</code></pre> <p>Check if an S3 object exists within an existing, accessible bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Bucket name.</p> required <code>key</code> <code>str</code> <p>Object key.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None, a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the object exists. False if the bucket exists but the object does not. An exception is raised if the bucket does not exist.</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>If the bucket does not exist or permissions prevent access.</p>"},{"location":"18-api.html#lava.lib.aws.s3_set_object_encoding","title":"s3_set_object_encoding","text":"<pre><code>s3_set_object_encoding(\n    bucket: str, key: str, encoding: str, s3_client\n) -&gt; None\n</code></pre> <p>Set the Content-Encoding meta data for the given object to the given value.</p> <p>This requires copying the object onto itself. No copy is done if the content encoding is already correctly set.</p> <p>Warning</p> <p>This is actually doing an S3 object copy. Most of the obvious metadata and encryption settings are preserved but beware.</p> <p>Warning</p> <p>Because of the nature of S3, there is a potential race condition if you try to use the new object too quickly as S3 may not have finished the copy operation.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>key</code> <code>str</code> <p>Prefix of the object in the remote bucket.</p> required <code>encoding</code> <code>str</code> <p>New content encoding for t</p> required <code>s3_client</code> <p>boto3 s3 client.</p> required"},{"location":"18-api.html#lava.lib.aws.s3_split","title":"s3_split","text":"<pre><code>s3_split(s: str) -&gt; tuple[str, str]\n</code></pre> <p>Split an S3 object name into bucket and prefix components.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The object name. Typically <code>bucket/prefix</code> but the following are also accepted:  <code>text s3:bucket/prefix s3://bucket/prefix /bucket/prefix</code></p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>A tuple (bucket, prefix)</p>"},{"location":"18-api.html#lava.lib.aws.s3_upload","title":"s3_upload","text":"<pre><code>s3_upload(\n    bucket: str,\n    key: str,\n    filename: str,\n    s3_client,\n    kms_key: str = None,\n) -&gt; None\n</code></pre> <p>Upload local file to S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>S3 bucket name of remote bucket</p> required <code>key</code> <code>str</code> <p>Key of the object in the remote bucket.</p> required <code>filename</code> <code>str</code> <p>Name of local file.</p> required <code>s3_client</code> <p>boto3 s3 client.</p> required <code>kms_key</code> <code>str</code> <p>Identifier for a KMS key. If not specified then AES256 is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>On failure</p>"},{"location":"18-api.html#lava.lib.aws.secman_get_json_secret","title":"secman_get_json_secret","text":"<pre><code>secman_get_json_secret(\n    name: str, aws_session: Session = None\n) -&gt; dict[str, Any]\n</code></pre> <p>Read a string secret from AWS Secrets Manager containing JSON and decode it.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Secret name.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the parameter doesn't exist or cannot be accessed.</p>"},{"location":"18-api.html#lava.lib.aws.secman_get_secret","title":"secman_get_secret","text":"<pre><code>secman_get_secret(\n    name: str,\n    allow_binary=True,\n    decode_binary=True,\n    aws_session: Session = None,\n) -&gt; str | bytes\n</code></pre> <p>Read a secret from AWS Secrets Manager.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Secret name.</p> required <code>allow_binary</code> <p>If True allow binary parameters. Default True.</p> <code>True</code> <code>decode_binary</code> <p>If True, b64 decode binary parameters.</p> <code>True</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | bytes</code> <p>The secret value.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the parameter doesn't exist or cannot be accessed.</p>"},{"location":"18-api.html#lava.lib.aws.ses_send","title":"ses_send","text":"<pre><code>ses_send(\n    sender: str,\n    to: str | Iterable[str] = None,\n    cc: str | Iterable[str] = None,\n    reply_to: str | Iterable[str] = None,\n    return_path: str = None,\n    subject: str = None,\n    message: str = None,\n    html: str = None,\n    region: str = \"us-east-1\",\n    charset: str = \"UTF-8\",\n    config_set: str = None,\n    aws_session: Session = None,\n    bcc: str | Iterable[str] = None,\n) -&gt; None\n</code></pre> <p>Send an email via SES.</p> <p>Note that SES is not available in all regions so we default to us-east-1.</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <code>str</code> <p>Sending email address. Must be verified or in a verified domain.</p> required <code>to</code> <code>str | Iterable[str]</code> <p>Destination email address(es).</p> <code>None</code> <code>cc</code> <code>str | Iterable[str]</code> <p>Cc address(es).</p> <code>None</code> <code>bcc</code> <code>str | Iterable[str]</code> <p>Bcc address(es).</p> <code>None</code> <code>reply_to</code> <code>str | Iterable[str]</code> <p>Reply To address(es).</p> <code>None</code> <code>return_path</code> <code>str</code> <p>Return path for bounces.</p> <code>None</code> <code>subject</code> <code>str</code> <p>Message subject.</p> <code>None</code> <code>message</code> <code>str</code> <p>Message text body. At least one of message and html arguments needs to be supplied. Default None.</p> <code>None</code> <code>html</code> <code>str</code> <p>Message html body. At least one of message and html arguments needs to be supplied. Default None.</p> <code>None</code> <code>region</code> <code>str</code> <p>Region for SES service. Defaults to us-east-1.</p> <code>'us-east-1'</code> <code>charset</code> <code>str</code> <p>The character set of the content. Default is 'UTF-8'.</p> <code>'UTF-8'</code> <code>config_set</code> <code>str</code> <p>Configuration set name.</p> <code>None</code> <code>aws_session</code> <code>Session</code> <p>A boto3 session object. If None a default session will be created. Default None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither message nor html are specified.</p>"},{"location":"18-api.html#lava.lib.aws.sqs_send_msg","title":"sqs_send_msg","text":"<pre><code>sqs_send_msg(\n    msg: str,\n    queue_name,\n    delay: int = 0,\n    aws_session: Session = None,\n) -&gt; None\n</code></pre> <p>Send a message to an SQS queue.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message.</p> required <code>queue_name</code> <p>The boto3 queue resource</p> required <code>delay</code> <code>int</code> <p>Send delay (controlled by SQS).</p> <code>0</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.aws.ssm_get_param","title":"ssm_get_param","text":"<pre><code>ssm_get_param(\n    name: str,\n    decrypt: bool = True,\n    aws_session: Session = None,\n) -&gt; str\n</code></pre> <p>Read a secure parameter from AWS SSM service.</p> <p>Warning</p> <p>Does not handle list of string SSM parameters sensibly. This is not a problem for lava but be warned before using it more generally.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Valid SSM parameter name</p> required <code>decrypt</code> <code>bool</code> <p>If True, decrypt parameter values. Default True.</p> <code>True</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session. If None a default is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The parameter value.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the parameter doesn't exist or cannot be accessed.</p>"},{"location":"18-api.html#lava.lib.daemon","title":"lava.lib.daemon","text":"<p>Make the current process run as a daemon.</p>"},{"location":"18-api.html#lava.lib.daemon.daemonify","title":"daemonify","text":"<pre><code>daemonify(\n    chroot_dir: str = None,\n    working_dir: str = None,\n    umask: str | int = None,\n    uid: str | int = None,\n    gid: str | int = None,\n    close_fds: bool = True,\n    pidfile: str = None,\n    stdout: str = None,\n    stderr: str = None,\n    signals: dict[str, Any] = None,\n    preserve_fds: list[str | int] = None,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p>Convert the current process into a daemon.</p> <p>All params are optional. Any unrecognised kwargs are silently ignored.</p> <p>Parameters:</p> Name Type Description Default <code>chroot_dir</code> <code>str</code> <p>chroot to this directory. Optional. Not tested.</p> <code>None</code> <code>working_dir</code> <code>str</code> <p>cd to this directory. Optional</p> <code>None</code> <code>umask</code> <code>str | int</code> <p>Set umask. Must be an int or an octal formatted numeric string. Optional.</p> <code>None</code> <code>uid</code> <code>str | int</code> <p>Set uid to this. Can be a user name or a numeric id. Optional. If not specified use real uid.</p> <code>None</code> <code>gid</code> <code>str | int</code> <p>Set gid to this. Can be a group name or a numeric id. Optional. If not specified use real uid.</p> <code>None</code> <code>close_fds</code> <code>bool</code> <p>If True close all open file descriptors in the child and reconnect stdin/stdout/stderr to /dev/null. See also the stdout/stderr params which allow these to be sent to a file instead. Default True.</p> <code>True</code> <code>pidfile</code> <code>str</code> <p>Name of file in which to write the PID. This is also a basic locking mechanism to prevent multiple daemons. Optional.</p> <code>None</code> <code>stdout</code> <code>str</code> <p>Redirect stdout to the specified file. {pid} will be replaced with the pid. If not specified use /dev/null. Will replace any previous file with same name.</p> <code>None</code> <code>stderr</code> <code>str</code> <p>Redirect stdout to the specified file. {pid} will be replaced with the pid. If not specified use /dev/null. Will replace any previous file with same name.</p> <code>None</code> <code>signals</code> <code>dict[str, Any]</code> <p>A dictionary. Keys are signal names (e.g. 'SIGHUP') and values are either None (meaning ignore the signal) or a signal handler function - which must take the two arguments required of handlers by signal.signal() ie. the signal number and the current stack frame.</p> <code>None</code> <code>preserve_fds</code> <code>list[str | int]</code> <p>A list of any file descriptors that should not be closed. The entries in the list can either be numeric (i.e. file descriptors) or filenames. If any of these are already open they will be left open. Any entries that don't correspond to an open file will be silently ignored. There is a bug in Python 3.4.0 (supposedly fixed in 3.4.1) which causes random.urandom() to fail if the file descriptor to /dev/urandom is closed. So if None is specified for preserve_fds, the fds for /dev/urandom and /dev/random will be preserved. If you really don't want that behaviour, provide an empty list as the argument but beware of \"bad file descriptor\" exceptions in unusual places.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Any left over named args are ignored.</p> <code>{}</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If something goes wrong.</p>"},{"location":"18-api.html#lava.lib.daemon.lock_file","title":"lock_file","text":"<pre><code>lock_file(fname: str) -&gt; bool\n</code></pre> <p>Create a lock file with the given name and write the current process PID in it.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>Name of lockfile</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if lockfile created, False otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the lock file cannot be created.</p>"},{"location":"18-api.html#lava.lib.daemon.set_signal","title":"set_signal","text":"<pre><code>set_signal(sig: str | int, handler=None) -&gt; None\n</code></pre> <p>Set the named signal (SIGHUP) to the specified handler.</p> <p>If the named signal does not exist on the system then an exception is raised. This can only be called from the main thread or a ValueError exception occurs.</p> <p>Parameters:</p> Name Type Description Default <code>sig</code> <code>str | int</code> <p>The number or name of the signal (as per signal(2)). e.g. SIGHUP or HUP</p> required <code>handler</code> <p>A signal handler. It may be either either None (meaning ignore the signal), signal.SIG_IGN to ignore the signal, signal.SIG_DFL to restore the default or a signal handler function - which must take the two arguments required by handlers by signal.signal() ie. the signal number and the current stack frame. Default None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the signal name is not known on this system or the handler is not None, SIG_IGN, SIG_DFL or a callable.</p> <code>Exception</code> <p>If something else goes wrong.</p>"},{"location":"18-api.html#lava.lib.daemon.stop_core_dumps","title":"stop_core_dumps","text":"<pre><code>stop_core_dumps() -&gt; None\n</code></pre> <p>Prevent core dumps.</p>"},{"location":"18-api.html#lava.lib.dag","title":"lava.lib.dag","text":"<p>Utilities for mucking about with lava DAG components.</p>"},{"location":"18-api.html#lava.lib.dag.load_dag_from_csv","title":"load_dag_from_csv","text":"<pre><code>load_dag_from_csv(filename: str) -&gt; dict[str, set[str]]\n</code></pre> <p>Load dependency graph from a CSV file.</p> <p>Predecessor jobs are across the top. Dependent jobs are the first column.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Source file name.</p> required <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Dependency map.</p>"},{"location":"18-api.html#lava.lib.dag.load_dag_from_db","title":"load_dag_from_db","text":"<pre><code>load_dag_from_db(\n    db_conn, table: str = DEFAULT_TABLE, group: str = None\n) -&gt; dict[str, set[str]]\n</code></pre> <p>Load dependency matrix from a database table.</p> <p>The table must have a schema something like this:</p> <pre><code>CREATE TABLE dag (\n    job_group VARCHAR(50),\n    job VARCHAR(50) NOT NULL,\n    depends_on VARCHAR(50)\n);\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>db_conn</code> <p>A DBAPI 2.0 connection.</p> required <code>table</code> <code>str</code> <p>Source table name.</p> <code>DEFAULT_TABLE</code> <code>group</code> <code>str</code> <p>Filter value to select a group of dependency entries.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Dependency map.</p>"},{"location":"18-api.html#lava.lib.dag.load_dag_from_lava_conn","title":"load_dag_from_lava_conn","text":"<pre><code>load_dag_from_lava_conn(\n    conn_id: str,\n    realm: str,\n    table: str = DEFAULT_TABLE,\n    group: str = None,\n) -&gt; dict[str, set[str]]\n</code></pre> <p>Load dependency matrix from an RDBMS via a lava connector.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Lava connection ID.</p> required <code>realm</code> <code>str</code> <p>Lava realm.</p> required <code>table</code> <code>str</code> <p>Source table in the database.</p> <code>DEFAULT_TABLE</code> <code>group</code> <code>str</code> <p>Filter value to select a group of dependency entries.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Matrix of data.</p>"},{"location":"18-api.html#lava.lib.dag.load_dag_from_sqlite3","title":"load_dag_from_sqlite3","text":"<pre><code>load_dag_from_sqlite3(\n    filename: str,\n    table: str = DEFAULT_TABLE,\n    group: str = None,\n) -&gt; dict[str, set[str]]\n</code></pre> <p>Load dependency matrix from an SQLite3 database.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Source file name.</p> required <code>table</code> <code>str</code> <p>Source table in the database.</p> <code>DEFAULT_TABLE</code> <code>group</code> <code>str</code> <p>Filter value to select a group of dependency entries.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Dependency map.</p>"},{"location":"18-api.html#lava.lib.dag.load_dag_from_xlsx","title":"load_dag_from_xlsx","text":"<pre><code>load_dag_from_xlsx(\n    filename: str, worksheet=None\n) -&gt; dict[str, set[str]]\n</code></pre> <p>Load dependency graph from an Excel (xslx) file.</p> <p>Predecessor jobs are across the top. Dependent jobs are the first column.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Source file name.</p> required <code>worksheet</code> <p>Use the specified worksheet. If not specified use the first one.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Dependency map.</p>"},{"location":"18-api.html#lava.lib.datetime","title":"lava.lib.datetime","text":"<p>Date/time utilities.</p>"},{"location":"18-api.html#lava.lib.datetime.duration_to_seconds","title":"duration_to_seconds","text":"<pre><code>duration_to_seconds(\n    duration: str | int | float | Decimal,\n) -&gt; float\n</code></pre> <p>Convert a string specifying a time duration to a number of seconds.</p> <p>Parameters:</p> Name Type Description Default <code>duration</code> <code>str | int | float | Decimal</code> <p>String in the form nnnX where nnn is an integer or float and X is one of (case sensitive): 'w':    weeks 'd':    days 'h':    hours 'm':    minutes 's':    seconds.  If X is missing then seconds are assumed. Whitespace is ignored. Can also be a float or integer. Note a leading + or - will be handled correctly as will exponentials.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The duration in seconds.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the duration is malformed.</p>"},{"location":"18-api.html#lava.lib.datetime.now_tz","title":"now_tz","text":"<pre><code>now_tz() -&gt; datetime.datetime\n</code></pre> <p>Get the current time as a timezone aware time.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>Return the current time as a timezone aware time rounded down to the nearest second.</p>"},{"location":"18-api.html#lava.lib.datetime.parse_dt","title":"parse_dt","text":"<pre><code>parse_dt(s: str) -&gt; datetime.datetime\n</code></pre> <p>Parse a datetime string and ensure it has a timezone.</p> <p>If one is not yielded by the parsing then the local timezone will be added.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>A string representing a date time.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Timezone aware datetime.</p>"},{"location":"18-api.html#lava.lib.datetime.time_to_str","title":"time_to_str","text":"<pre><code>time_to_str(t: time) -&gt; str\n</code></pre> <p>Convert a time object to a string suitable for use in API responses.</p> <p>Deprecated as of v8.0.0</p> <p>Just use str(t) instead.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>time</code> <p>A time object</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the time</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object is not a time object.</p>"},{"location":"18-api.html#lava.lib.datetime.timedelta_to_hms","title":"timedelta_to_hms","text":"<pre><code>timedelta_to_hms(td: timedelta) -&gt; tuple[int, int, int]\n</code></pre> <p>Convert a timedelta to hours, minutes, seconds (rounded to nearest second).</p> <p>Results may not be quite what you expect if td is negative.</p> <p>Parameters:</p> Name Type Description Default <code>td</code> <code>timedelta</code> <p>A timedelta</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>A triple (hours, minutes, seconds)</p>"},{"location":"18-api.html#lava.lib.datetime.timedelta_to_str","title":"timedelta_to_str","text":"<pre><code>timedelta_to_str(delta: timedelta) -&gt; str\n</code></pre> <p>Convert a timedelta instance to a string.</p> <p>Parameters:</p> Name Type Description Default <code>delta</code> <code>timedelta</code> <p>A timedelta object.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the timedelta.</p>"},{"location":"18-api.html#lava.lib.datetime.timestamp","title":"timestamp","text":"<pre><code>timestamp() -&gt; tuple[str, str]\n</code></pre> <p>Return the current UTC and localtime as a pair of ISO8601 strings.</p> <p>Precision is to the nearest second.</p> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>A tuple of strings: UTC-time, Local-time</p>"},{"location":"18-api.html#lava.lib.db","title":"lava.lib.db","text":"<p>Database utilities.</p> <p>Danger</p> <p>This is not a general purpose database interface layer. Do not attempt to use it for that. It is designed specifically to support lava jobs which have very specific needs and is subject to change.</p>"},{"location":"18-api.html#lava.lib.db.AuroraMySql","title":"AuroraMySql","text":"<pre><code>AuroraMySql(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Database</code></p> <p>Model an Aurora MySQL database.</p> <p>Init.</p>"},{"location":"18-api.html#lava.lib.db.AuroraMySql.copy_from_s3","title":"copy_from_s3","text":"<pre><code>copy_from_s3(\n    schema: str,\n    table: str,\n    bucket: str,\n    key: str,\n    region: str = None,\n    copy_args: list[str] = None,\n    load_columns: list[str] = None,\n    s3_access_keys: dict[str, str] = None,\n    iam_role: str = None,\n    min_size: int = 0,\n) -&gt; list[str]\n</code></pre> <p>Copy a file from S3 to Aurora MySQL with S3 native COPY.</p> <p>At the lava user level we have departed significantly from the native Aurora COPY command to try to give some alignment with Postgres.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Target schema name.</p> required <code>table</code> <code>str</code> <p>Target table names.</p> required <code>copy_args</code> <code>list[str]</code> <p>Copy arguments. In addition to the standard Postrges COPY args the following are also supported: MANIFEST, GZIP.</p> <code>None</code> <code>bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>key</code> <code>str</code> <p>Source key in S3.</p> required <code>region</code> <code>str</code> <p>AWS region containing the bucket.</p> <code>None</code> <code>load_columns</code> <code>list[str]</code> <p>A list of columns to load. May be empty which means load all.</p> <code>None</code> <code>s3_access_keys</code> <code>dict[str, str]</code> <p>Not used.</p> <code>None</code> <code>iam_role</code> <code>str</code> <p>Not used.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Try to avoid loading data files below this size in bytes.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings indicating steps taken.</p>"},{"location":"18-api.html#lava.lib.db.Database","title":"Database","text":"<pre><code>Database(\n    conn_spec: dict[str, Any],\n    realm: str,\n    tmpdir: str,\n    aws_session: Session = None,\n    logger: Logger = None,\n    application_name: str = None,\n)\n</code></pre> <p>Base class for a database handler.</p> <p>This is not a generic database model but rather a specific adaptation for the purposes of lava.</p> <p>Parameters:</p> Name Type Description Default <code>conn_spec</code> <code>dict[str, Any]</code> <p>A database connection specification.</p> required <code>realm</code> <code>str</code> <p>Lava realm.</p> required <code>tmpdir</code> <code>str</code> <p>Temporary directory for junk if required. It is assumed that the directory already exists and that caller will clean this up.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session(). If not specified a default will be created. Default None.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>A logger. If not specified, use the root logger.</p> <code>None</code> <code>application_name</code> <code>str</code> <p>Application name when connecting.</p> <code>None</code> <p>Create a new Database instance.</p>"},{"location":"18-api.html#lava.lib.db.Database.conn","title":"conn  <code>property</code>","text":"<pre><code>conn\n</code></pre> <p>Get a database connection.</p> <p>An existing connection will be reused if available.</p> <p>Default implementation returns a DBAPI 2.0 connection object.</p> <p>Returns:</p> Type Description <p>A database connection.</p>"},{"location":"18-api.html#lava.lib.db.Database.cursor","title":"cursor  <code>property</code>","text":"<pre><code>cursor\n</code></pre> <p>Return a cursor.</p> <p>A cached cursor will be used if available.</p> <p>Returns:</p> Type Description <p>A DBAPI 2.0 cursor.</p>"},{"location":"18-api.html#lava.lib.db.Database.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close any open connections.</p>"},{"location":"18-api.html#lava.lib.db.Database.columns","title":"columns","text":"<pre><code>columns(schema: str, table: str) -&gt; list[str]\n</code></pre> <p>Get the column names for the given table.</p> <p>Works on any DB that supports the information_schema. Results are cached.</p> <p>We can't easily use query parameters without handling all the possible DBAPI 2.0 paramstyle settings. To reduce risk of injection, object names containing single quotes are rejected.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of column names.</p>"},{"location":"18-api.html#lava.lib.db.Database.copy_from_s3","title":"copy_from_s3","text":"<pre><code>copy_from_s3(\n    schema: str,\n    table: str,\n    bucket: str,\n    key: str,\n    region: str = None,\n    copy_args: list[str] = None,\n    load_columns: list[str] = None,\n    s3_access_keys: dict[str, str] = None,\n    iam_role: str = None,\n    min_size: int = 0,\n) -&gt; list[str]\n</code></pre> <p>Copy a file from S3 to a specific database table.</p> <p>No commit is done.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Target schema name.</p> required <code>table</code> <code>str</code> <p>Target table names.</p> required <code>copy_args</code> <code>list[str]</code> <p>Copy arguments.</p> <code>None</code> <code>bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>key</code> <code>str</code> <p>Source key in S3.</p> required <code>region</code> <code>str</code> <p>AWS region containing the bucket.</p> <code>None</code> <code>load_columns</code> <code>list[str]</code> <p>A list of columns to load. May be empty which means load all.</p> <code>None</code> <code>s3_access_keys</code> <code>dict[str, str]</code> <p>Access keys to access S3. Must be a dictionary with aws_access_key_id and aws_secret_access_key.</p> <code>None</code> <code>iam_role</code> <code>str</code> <p>IAM role name to access S3. Some DBs use it, some don't.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Try to avoid loading data files below this size in bytes. Some subclasses may honour this. Some not.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings indicating steps taken.</p>"},{"location":"18-api.html#lava.lib.db.Database.create_table","title":"create_table","text":"<pre><code>create_table(\n    schema: str, table: str, columns: list[str]\n) -&gt; None\n</code></pre> <p>Create a table.</p> <p>Warning</p> <p>There is s still a (low) risk SQL injection here in the column specs, however these are not injected at runtime in lava and are part of the job spec.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>columns</code> <code>list[str]</code> <p>A list of column specifications.</p> required"},{"location":"18-api.html#lava.lib.db.Database.drop_table","title":"drop_table","text":"<pre><code>drop_table(schema: str, table: str) -&gt; None\n</code></pre> <p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required"},{"location":"18-api.html#lava.lib.db.Database.handler","title":"handler  <code>staticmethod</code>","text":"<pre><code>handler(db_type: str, *args, **kwargs) -&gt; Database\n</code></pre> <p>Create a database handler for the specified DB type.</p> <p>Parameters:</p> Name Type Description Default <code>db_type</code> <code>str</code> <p>A name for the database type. eg. 'redshift'.</p> required <p>Returns:</p> Type Description <code>Database</code> <p>A database handler.</p>"},{"location":"18-api.html#lava.lib.db.Database.object_name","title":"object_name","text":"<pre><code>object_name(*name_part: str) -&gt; str\n</code></pre> <p>Sanitise a database object name.</p> <p>This may involve rejecting or quoting unsafe object names. The default implementation just uses ANSI style quoting with double quotes.</p> <p>By default, object names are converted to lower case. This presents a problem with respect to case sensitivity in some cases. To mitigate this, the <code>preserve_case</code> field in the connection specification can be set to prevent case folding.</p> <p>The method is not static to preserve the option for implementations to adjust behaviour based on the conn spec.</p> <p>Parameters:</p> Name Type Description Default <code>name_part</code> <code>str</code> <p>One or more name parts. Empty parts are silently discarded.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>A clean object name composed of the parts.</p>"},{"location":"18-api.html#lava.lib.db.Database.table_exists","title":"table_exists","text":"<pre><code>table_exists(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a table (or view) exists.</p> <p>Visibility of table existence is dependent on the database access permissions of the user owning the connection.</p> <p>Works on any DB that supports the information_schema.</p> <p>We can't easily use query parameters without handling all the possible DBAPI 2.0 paramstyle settings. To reduce risk of injection, object names containing single quotes are rejected.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table exists.</p>"},{"location":"18-api.html#lava.lib.db.Database.table_is_empty","title":"table_is_empty","text":"<pre><code>table_is_empty(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a given table is empty.</p> <p>For some DBs (e.g. Postgres), a simple row count is a really bad idea on big tables (unlike Redshift).</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table is empty. False otherwise.</p>"},{"location":"18-api.html#lava.lib.db.Database.truncate_table","title":"truncate_table","text":"<pre><code>truncate_table(schema: str, table: str) -&gt; None\n</code></pre> <p>Truncate the specified table.</p> <p>DBs tend to vary in terms of handling of transactions around truncates.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name</p> required <code>table</code> <code>str</code> <p>Table name</p> required"},{"location":"18-api.html#lava.lib.db.MsSql","title":"MsSql","text":"<pre><code>MsSql(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Database</code></p> <p>Model a Microsoft SQL Server database.</p> <p>Init.</p>"},{"location":"18-api.html#lava.lib.db.MsSql.copy_from_s3","title":"copy_from_s3","text":"<pre><code>copy_from_s3(\n    schema: str,\n    table: str,\n    bucket: str,\n    key: str,\n    region: str = None,\n    copy_args: list[str] = None,\n    load_columns: list[str] = None,\n    s3_access_keys: dict[str, str] = None,\n    iam_role: str = None,\n    min_size: int = 0,\n) -&gt; list[str]\n</code></pre> <p>Copy a file from S3 to a specific database table.</p> <p>No commit is done.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Target schema name.</p> required <code>table</code> <code>str</code> <p>Target table names.</p> required <code>copy_args</code> <code>list[str]</code> <p>Copy arguments.</p> <code>None</code> <code>bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>key</code> <code>str</code> <p>Source key in S3.</p> required <code>region</code> <code>str</code> <p>AWS region containing the bucket.</p> <code>None</code> <code>load_columns</code> <code>list[str]</code> <p>A list of columns to load. May be empty which means load all.</p> <code>None</code> <code>s3_access_keys</code> <code>dict[str, str]</code> <p>Access keys to access S3. Must be a dictionary with aws_access_key_id and aws_secret_access_key.</p> <code>None</code> <code>iam_role</code> <code>str</code> <p>IAM role name to access S3. Some DBs use it, some don't.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Try to avoid loading data files below this size in bytes. Some subclasses may honour this. Some not.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings indicating steps taken.</p>"},{"location":"18-api.html#lava.lib.db.MsSql.table_is_empty","title":"table_is_empty","text":"<pre><code>table_is_empty(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a given table is empty.</p> <p>For MsSQL, a simple row count is a really bad idea on big tables.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table is empty. False otherwise.</p>"},{"location":"18-api.html#lava.lib.db.Oracle","title":"Oracle","text":"<pre><code>Oracle(\n    conn_spec: dict[str, Any],\n    realm: str,\n    tmpdir: str,\n    aws_session: Session = None,\n    logger: Logger = None,\n    application_name: str = None,\n)\n</code></pre> <p>               Bases: <code>Database</code></p> <p>Model an Oracle database.</p>"},{"location":"18-api.html#lava.lib.db.Oracle.columns","title":"columns","text":"<pre><code>columns(schema: str, table: str) -&gt; list[str]\n</code></pre> <p>Get the column names for the given table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name. Ignored for sqlite3.</p> required <code>table</code> <code>str</code> <p>Table name</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of column names.</p>"},{"location":"18-api.html#lava.lib.db.Oracle.copy_from_s3","title":"copy_from_s3","text":"<pre><code>copy_from_s3(\n    schema: str,\n    table: str,\n    bucket: str,\n    key: str,\n    region: str = None,\n    copy_args: list[str] = None,\n    load_columns: list[str] = None,\n    s3_access_keys: dict[str, str] = None,\n    iam_role: str = None,\n    min_size: int = 0,\n) -&gt; list[str]\n</code></pre> <p>Copy a file from S3 to a specific database table.</p> <p>No commit is done.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Target schema name.</p> required <code>table</code> <code>str</code> <p>Target table names.</p> required <code>copy_args</code> <code>list[str]</code> <p>Copy arguments.</p> <code>None</code> <code>bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>key</code> <code>str</code> <p>Source key in S3.</p> required <code>region</code> <code>str</code> <p>AWS region containing the bucket.</p> <code>None</code> <code>load_columns</code> <code>list[str]</code> <p>A list of columns to load. May be empty which means load all.</p> <code>None</code> <code>s3_access_keys</code> <code>dict[str, str]</code> <p>Access keys to access S3. Must be a dictionary with aws_access_key_id and aws_secret_access_key.</p> <code>None</code> <code>iam_role</code> <code>str</code> <p>IAM role name to access S3. Some DBs use it, some don't.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Try to avoid loading data files below this size in bytes. Some subclasses may honour this. Some not.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings indicating steps taken.</p>"},{"location":"18-api.html#lava.lib.db.Oracle.drop_table","title":"drop_table","text":"<pre><code>drop_table(schema: str, table: str) -&gt; None\n</code></pre> <p>Drop a table.</p> <p>Oracle doesn't have a DROP TABLE IF EXISTS.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required"},{"location":"18-api.html#lava.lib.db.Oracle.object_name","title":"object_name","text":"<pre><code>object_name(*name_part: str) -&gt; str\n</code></pre> <p>Sanitise a database object name.</p> <p>Oracle has funny rules around quoting -- it's not purely a syntax implication. If a column name is quoted at creation it always has to be quoted and vice versa. So its not safe here to just pop quotes around everything. Best we can do is just make sure names don't contain bad characters. This is not a fix-all solution but best we can do.</p> <p>Parameters:</p> Name Type Description Default <code>name_part</code> <code>str</code> <p>One or more name parts. Empty parts are silently discarded.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>A clean object name composed of the parts.</p>"},{"location":"18-api.html#lava.lib.db.Oracle.table_exists","title":"table_exists","text":"<pre><code>table_exists(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a table (or view) exists.</p> <p>Visibility of table existence is dependent on the database access permissions of the user owning the connection.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name. This is the table owner in Oracle.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table exists.</p>"},{"location":"18-api.html#lava.lib.db.Oracle.table_is_empty","title":"table_is_empty","text":"<pre><code>table_is_empty(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a given table is empty.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table is empty. False otherwise.</p>"},{"location":"18-api.html#lava.lib.db.Postgres","title":"Postgres","text":"<pre><code>Postgres(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Database</code></p> <p>Model a conventional Postgres database.</p> <p>Create a Postgres handler instance.</p> <p>Because we need to do a client side copy, we will use a CLI connector for that as the only way to do it is for the client side copy to read from stdin. The COPY FILE and COPY PROGRAM options are really server side and require superuser priveleges.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>As per super.</p> <code>()</code> <code>kwargs</code> <p>As per super.</p> <code>{}</code>"},{"location":"18-api.html#lava.lib.db.Postgres.copy_from_s3","title":"copy_from_s3","text":"<pre><code>copy_from_s3(\n    schema: str,\n    table: str,\n    bucket: str,\n    key: str,\n    region: str = None,\n    copy_args: list[str] = None,\n    load_columns: list[str] = None,\n    s3_access_keys: dict[str, str] = None,\n    iam_role: str = None,\n    min_size: int = 0,\n) -&gt; list[str]\n</code></pre> <p>Copy a file from S3 to Postgres with a client side COPY.</p> <p>There is also a degree of trickery here to support handling of manifests.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Target schema name.</p> required <code>table</code> <code>str</code> <p>Target table names.</p> required <code>copy_args</code> <code>list[str]</code> <p>Copy arguments. In addition to the standard Postrges COPY args the following are also supported: MANIFEST, GZIP</p> <code>None</code> <code>bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>key</code> <code>str</code> <p>Source key in S3.</p> required <code>region</code> <code>str</code> <p>AWS region containing the bucket.</p> <code>None</code> <code>load_columns</code> <code>list[str]</code> <p>A list of columns to load. May be empty which means load all.</p> <code>None</code> <code>s3_access_keys</code> <code>dict[str, str]</code> <p>Not used.</p> <code>None</code> <code>iam_role</code> <code>str</code> <p>Not used.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Try to avoid loading data files below this size in bytes.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings indicating steps taken.</p>"},{"location":"18-api.html#lava.lib.db.Postgres.create_table","title":"create_table","text":"<pre><code>create_table(*args, **kwargs) -&gt; None\n</code></pre> <p>Create a table.</p> <p>For Postgres need to commit after create to avoid deadlock. This is because we are using both the DBAPI 2.0 client as well as the psql client.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>As per super.</p> <code>()</code> <code>kwargs</code> <p>As per super.</p> <code>{}</code>"},{"location":"18-api.html#lava.lib.db.Postgres.truncate_table","title":"truncate_table","text":"<pre><code>truncate_table(schema: str, table: str) -&gt; None\n</code></pre> <p>Truncate the specified table.</p> <p>Postgres seems to deadlock unless we commit after truncate. This is because we are using both the DBAPI 2.0 client as well as the psql client.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name</p> required <code>table</code> <code>str</code> <p>Table name</p> required"},{"location":"18-api.html#lava.lib.db.PostgresRds","title":"PostgresRds","text":"<pre><code>PostgresRds(\n    conn_spec: dict[str, Any],\n    realm: str,\n    tmpdir: str,\n    aws_session: Session = None,\n    logger: Logger = None,\n    application_name: str = None,\n)\n</code></pre> <p>               Bases: <code>Database</code></p> <p>Model a Postgres RDS / Aurora database.</p>"},{"location":"18-api.html#lava.lib.db.PostgresRds.copy_from_s3","title":"copy_from_s3","text":"<pre><code>copy_from_s3(\n    schema: str,\n    table: str,\n    bucket: str,\n    key: str,\n    region: str = None,\n    copy_args: list[str] = None,\n    load_columns: list[str] = None,\n    s3_access_keys: dict[str, str] = None,\n    iam_role: str = None,\n    min_size: int = 0,\n) -&gt; list[str]\n</code></pre> <p>Copy a file from S3 to Postgres RDS / Aurora with S3 native COPY.</p> <p>There is also a degree of trickery here to support handling of manifests.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Target schema name.</p> required <code>table</code> <code>str</code> <p>Target table names.</p> required <code>copy_args</code> <code>list[str]</code> <p>Copy arguments. In addition to the standard Postrges COPY args the following are also supported: MANIFEST, GZIP.</p> <code>None</code> <code>bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>key</code> <code>str</code> <p>Source key in S3.</p> required <code>region</code> <code>str</code> <p>AWS region containing the bucket.</p> <code>None</code> <code>load_columns</code> <code>list[str]</code> <p>A list of columns to load. May be empty which means load all.</p> <code>None</code> <code>s3_access_keys</code> <code>dict[str, str]</code> <p>Access keys to access S3.</p> <code>None</code> <code>iam_role</code> <code>str</code> <p>Not used.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Try to avoid loading data files below this size in bytes.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings indicating steps taken.</p>"},{"location":"18-api.html#lava.lib.db.Redshift","title":"Redshift","text":"<pre><code>Redshift(\n    conn_spec: dict[str, Any],\n    realm: str,\n    tmpdir: str,\n    aws_session: Session = None,\n    logger: Logger = None,\n    application_name: str = None,\n)\n</code></pre> <p>               Bases: <code>Database</code></p> <p>Model a Redshift database.</p>"},{"location":"18-api.html#lava.lib.db.Redshift.columns","title":"columns","text":"<pre><code>columns(schema: str, table: str) -&gt; list[str]\n</code></pre> <p>Get the column names for the given table.</p> <p>Works for external tables too. Results are cached.</p> <p>We can't easily use query parameters without handling all the possible DBAPI 2.0 paramstyle settings. To reduce risk of injection, object names containing single quotes are rejected.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of column names.</p>"},{"location":"18-api.html#lava.lib.db.Redshift.copy_from_s3","title":"copy_from_s3","text":"<pre><code>copy_from_s3(\n    schema: str,\n    table: str,\n    bucket: str,\n    key: str,\n    region: str = None,\n    copy_args: list[str] = None,\n    load_columns: list[str] = None,\n    s3_access_keys: dict[str, str] = None,\n    iam_role: str = None,\n    min_size: int = 0,\n) -&gt; list[str]\n</code></pre> <p>Copy a file from S3 to a specific database table.</p> <p>No commit is done.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Target schema name.</p> required <code>table</code> <code>str</code> <p>Target table names.</p> required <code>copy_args</code> <code>list[str]</code> <p>Copy arguments.</p> <code>None</code> <code>bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>key</code> <code>str</code> <p>Source key in S3.</p> required <code>region</code> <code>str</code> <p>AWS region containing the bucket.</p> <code>None</code> <code>load_columns</code> <code>list[str]</code> <p>A list of columns to load. May be empty which means load all.</p> <code>None</code> <code>s3_access_keys</code> <code>dict[str, str]</code> <p>Access keys to access S3.</p> <code>None</code> <code>iam_role</code> <code>str</code> <p>IAM role name to access S3. Some DBs use it, some don't.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Try to avoid loading data files below this size in bytes.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings indicating steps taken.</p>"},{"location":"18-api.html#lava.lib.db.Redshift.table_exists","title":"table_exists","text":"<pre><code>table_exists(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a table (or view) exists.</p> <p>Visibility of table existence is dependent on the database access permissions of the user owning the connection.</p> <p>Works on any DB that supports the information_schema.</p> <p>We can't easily use query parameters without handling all the possible DBAPI 2.0 paramstyle settings. To reduce risk of injection, object names containing single quotes are rejected.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table exists.</p>"},{"location":"18-api.html#lava.lib.db.Redshift.table_is_empty","title":"table_is_empty","text":"<pre><code>table_is_empty(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a given Redshift table is empty.</p> <p>For Redshift, a row count is ok to do this.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name</p> required <code>table</code> <code>str</code> <p>Table name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table is empty. False otherwise.</p>"},{"location":"18-api.html#lava.lib.db.Sqlite3","title":"Sqlite3","text":"<pre><code>Sqlite3(\n    conn_spec: dict[str, Any],\n    realm: str,\n    tmpdir: str,\n    aws_session: Session = None,\n    logger: Logger = None,\n    application_name: str = None,\n)\n</code></pre> <p>               Bases: <code>Database</code></p> <p>Model an SQLite3 database.</p>"},{"location":"18-api.html#lava.lib.db.Sqlite3.columns","title":"columns","text":"<pre><code>columns(schema: str, table: str) -&gt; list[str]\n</code></pre> <p>Get the column names for the given table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name. Ignored for sqlite3.</p> required <code>table</code> <code>str</code> <p>Table name</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of column names.</p>"},{"location":"18-api.html#lava.lib.db.Sqlite3.copy_from_s3","title":"copy_from_s3","text":"<pre><code>copy_from_s3(\n    schema: str,\n    table: str,\n    bucket: str,\n    key: str,\n    region: str = None,\n    copy_args: list[str] = None,\n    load_columns: list[str] = None,\n    s3_access_keys: dict[str, str] = None,\n    iam_role: str = None,\n    min_size: int = 0,\n) -&gt; list[str]\n</code></pre> <p>Copy a file from S3 to a specific database table.</p> <p>No commit is done.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Target schema name. Ignored for sqlite3.</p> required <code>table</code> <code>str</code> <p>Target table names.</p> required <code>copy_args</code> <code>list[str]</code> <p>Copy arguments.</p> <code>None</code> <code>bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>key</code> <code>str</code> <p>Source key in S3.</p> required <code>region</code> <code>str</code> <p>AWS region containing the bucket.</p> <code>None</code> <code>load_columns</code> <code>list[str]</code> <p>A list of columns to load. May be empty which means load all.</p> <code>None</code> <code>s3_access_keys</code> <code>dict[str, str]</code> <p>Access keys to access S3. Must be a dictionary with aws_access_key_id and aws_secret_access_key.</p> <code>None</code> <code>iam_role</code> <code>str</code> <p>IAM role name to access S3. Some DBs use it, some don't.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Try to avoid loading data files below this size in bytes. Some subclasses may honour this. Some not.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings indicating steps taken.</p>"},{"location":"18-api.html#lava.lib.db.Sqlite3.create_table","title":"create_table","text":"<pre><code>create_table(\n    schema: str, table: str, columns: list[str]\n) -&gt; None\n</code></pre> <p>Create a table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name. Ignored for sqlite3.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>columns</code> <code>list[str]</code> <p>A list of column specifications.</p> required"},{"location":"18-api.html#lava.lib.db.Sqlite3.drop_table","title":"drop_table","text":"<pre><code>drop_table(schema: str, table: str) -&gt; None\n</code></pre> <p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name. Ignored for sqlite3.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required"},{"location":"18-api.html#lava.lib.db.Sqlite3.table_exists","title":"table_exists","text":"<pre><code>table_exists(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a table (or view) exists.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name. Ignored for sqlite3.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table exists.</p>"},{"location":"18-api.html#lava.lib.db.Sqlite3.table_is_empty","title":"table_is_empty","text":"<pre><code>table_is_empty(schema: str, table: str) -&gt; bool\n</code></pre> <p>Check if a given table is empty.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name. Ignored for sqlite3.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table is empty. False otherwise.</p>"},{"location":"18-api.html#lava.lib.db.Sqlite3.truncate_table","title":"truncate_table","text":"<pre><code>truncate_table(schema: str, table: str) -&gt; None\n</code></pre> <p>Truncate the specified table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name. Ignored for sqlite3.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required"},{"location":"18-api.html#lava.lib.db.begin_transaction","title":"begin_transaction","text":"<pre><code>begin_transaction(conn, cursor=None) -&gt; None\n</code></pre> <p>Begin a transaction, trying to navigate the vagaries of DBAPI 2.0.</p> <p>DBAPI 2.0 has no consistent interface for beginning a transaction. Try to do this in a DB agnostic way.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>An open DBAPI database connector.</p> required <code>cursor</code> <p>An optional DB cursor. If not supplied one will be created if required.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.db.db_handler","title":"db_handler","text":"<pre><code>db_handler(*args: str) -&gt; Callable\n</code></pre> <p>Register database handler classes.</p> <p>Usage:</p> <pre><code>@db_handler(db_type1, ...)\na_class(...)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>str</code> <p>A list of database types that the decorated class handles.</p> <code>()</code>"},{"location":"18-api.html#lava.lib.db.query_to_dict","title":"query_to_dict","text":"<pre><code>query_to_dict(cursor, *args, **kwargs) -&gt; Iterator[dict]\n</code></pre> <p>Run a query on the given cursor and stream the results back in dict format.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <p>Database cursor.</p> required <code>args</code> <p>Passed to cursor.execute().</p> <code>()</code> <code>kwargs</code> <p>Passed to cursor.execute().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[dict]</code> <p>An iterator of dictionaries keyed on column name.</p>"},{"location":"18-api.html#lava.lib.db.read_manifest","title":"read_manifest","text":"<pre><code>read_manifest(\n    bucket: str, key: str, aws_session: Session = None\n) -&gt; list[tuple[str, str]]\n</code></pre> <p>Read a manifest file from S3 and return the list of files.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Bucket name</p> required <code>key</code> <code>str</code> <p>S3 key.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 Session.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>A list of S3 object names in the form (bucket, key)</p>"},{"location":"18-api.html#lava.lib.db.redshift_authorization","title":"redshift_authorization","text":"<pre><code>redshift_authorization(\n    s3_iam_role: str = None,\n    aws_access_key_id: str = None,\n    aws_secret_access_key: str = None,\n    aws_session_token: str = None,\n    **_\n) -&gt; str\n</code></pre> <p>Prepare authorization parameters for external resource access (e.g. S3).</p> <p>This is suitable for COPY and UNLOAD.</p> <p>Parameters:</p> Name Type Description Default <code>s3_iam_role</code> <code>str</code> <p>An IAM role name. Either this or the two access key parameters must be provided. The role is preferred.</p> <code>None</code> <code>aws_access_key_id</code> <code>str</code> <p>AWS access key.</p> <code>None</code> <code>aws_secret_access_key</code> <code>str</code> <p>AWS access secret key.</p> <code>None</code> <code>aws_session_token</code> <code>str</code> <p>AWS session token.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>:An S3 credentials string for use in Redshift UNLOAD.</p>"},{"location":"18-api.html#lava.lib.db.redshift_get_column_info","title":"redshift_get_column_info","text":"<pre><code>redshift_get_column_info(\n    cursor, schema: str, relation: str\n) -&gt; list[tuple[str, ...]]\n</code></pre> <p>Get column information for the specified relation.</p> <p>Deprecated as of v7.1.0</p> <p>This will not work for views with no schema binding or external (data    share) tables. Use <code>redshift_get_column_info2()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <p>Database cursor.</p> required <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>relation</code> <code>str</code> <p>Relation (table or view) name.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, ...]]</code> <p>A list of tuples (column name, type) with all components guaranteed to be in lower case.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the relation doesn't exist.</p>"},{"location":"18-api.html#lava.lib.db.redshift_get_column_info2","title":"redshift_get_column_info2","text":"<pre><code>redshift_get_column_info2(\n    cursor, schema: str, relation: str\n) -&gt; tuple[list[str]]\n</code></pre> <p>Get column information for the specified relation in the current database.</p> <p>This does handle views with no schema binding and external (data share) tables.</p> <p>Info</p> <p>This produces similar but slightly different output to that produced by    <code>redshift_get_column_info()</code>. For example, this version does not include    text field lengths. It also produces a tuple of lists instead of a list    of tuples. In most cases the differences are not significant.</p> <p>Info</p> <p>Expects a <code>format</code> paramstyle on the driver (e.g. pg8000).</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <p>Database cursor.</p> required <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>relation</code> <code>str</code> <p>Relation (table or view) name.</p> required <p>Returns:</p> Type Description <code>tuple[list[str]]</code> <p>A tuple of 2 element lists [column name, type].</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the column information cannot be obtained.</p>"},{"location":"18-api.html#lava.lib.db.redshift_oid","title":"redshift_oid","text":"<pre><code>redshift_oid(cursor, schema: str, relation: str) -&gt; int\n</code></pre> <p>Get the OID for the specified relation.</p> <p>Parameters:</p> Name Type Description Default <code>cursor</code> <p>Database cursor.</p> required <code>schema</code> <code>str</code> <p>Schema name.</p> required <code>relation</code> <code>str</code> <p>Relation (table or view) name.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The OID of the object or None if the object doesn't exist.</p>"},{"location":"18-api.html#lava.lib.dbnone","title":"lava.lib.dbnone","text":"<p>Dummy DB API 2.0 module.</p> <p>Typical usage would be:</p> <pre><code>try:\n    import db_module\nexcept ImportError:\n    import dbnone as db_module\n    db_module.alias = 'db_module'\n</code></pre>"},{"location":"18-api.html#lava.lib.dbnone.Connection","title":"Connection","text":"<pre><code>Connection(*args, **kwargs)\n</code></pre> <p>A dummy connection class.</p> <p>Attempting to create an instance will result in an exception.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>Ignored.</p> <code>()</code> <code>kwargs</code> <p>Ignored.</p> <code>{}</code> <p>Create a dummy connection.</p>"},{"location":"18-api.html#lava.lib.dbnone.Cursor","title":"Cursor","text":"<pre><code>Cursor(*args, **kwargs)\n</code></pre> <p>A dummy Cursor class.</p> <p>Attempting to create an instance will result in an exception.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>Ignored.</p> <code>()</code> <code>kwargs</code> <p>Ignored.</p> <code>{}</code> <p>Not implemented.</p>"},{"location":"18-api.html#lava.lib.dbnone.connect","title":"connect","text":"<pre><code>connect(*args, **kwargs)\n</code></pre> <p>Just throws an exception.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>Ignored.</p> <code>()</code> <code>kwargs</code> <p>Ignored.</p> <code>{}</code>"},{"location":"18-api.html#lava.lib.decorators","title":"lava.lib.decorators","text":"<p>Function decorators.</p>"},{"location":"18-api.html#lava.lib.decorators.debug_func","title":"debug_func","text":"<pre><code>debug_func(func) -&gt; Callable\n</code></pre> <p>Print function call details.</p> <p>Details are - parameters names and effective values and return value.</p> <p>Usage:</p> <pre><code>@debug_func\ndef f(...):\n</code></pre>"},{"location":"18-api.html#lava.lib.decorators.static_vars","title":"static_vars","text":"<pre><code>static_vars(**kwargs) -&gt; Callable\n</code></pre> <p>Allow a function to have static variables.</p> <p>Usage:</p> <pre><code>@static_vars(v1=10, v2={}, ...)\ndef f(...):\n    print(f.v1)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Variable names and initial values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Decorated function.</p>"},{"location":"18-api.html#lava.lib.email","title":"lava.lib.email","text":"<p>Lava based support for outbound email.</p> <p>It defines a base class for email sending. Concrete classes use real email sending services (e.g. AWS SES).</p> <p>Alway use the base class, which is also a context manager. Typical use of the base class is:</p> <pre><code>from lava.lib.email import Emailer\n\nwith h as Emailer.handler(conn_spec, realm, sender) as h:\n    h.send(to='x@y.com', subject='Hello world', message='Yahoo')\n</code></pre>"},{"location":"18-api.html#lava.lib.email.AwsSes","title":"AwsSes","text":"<pre><code>AwsSes(\n    conn_spec: dict[str, Any],\n    realm: str,\n    sender: str = None,\n    aws_session: Session = None,\n    logger: Logger = None,\n)\n</code></pre> <p>               Bases: <code>Emailer</code></p> <p>Email sender implementation using AWS SES.</p> <p>Create an Emailer instamnce.</p>"},{"location":"18-api.html#lava.lib.email.AwsSes.send","title":"send","text":"<pre><code>send(\n    subject: str,\n    message: str,\n    to: Iterable[str] | str = None,\n    cc: Iterable[str] | str = None,\n    bcc: Iterable[str] | str = None,\n    sender: str = None,\n    reply_to: Iterable[str] | str = None,\n    attachments: Iterable[EmailAttachment | str] = None,\n) -&gt; None\n</code></pre> <p>Send an email using SES.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>Message subject. Must not be empty.</p> required <code>message</code> <code>str</code> <p>Message body. Must not be empty. If it looks like HTML some handlers will treat it differently.</p> required <code>to</code> <code>Iterable[str] | str</code> <p>Recipient address or iterable of addresses.</p> <code>None</code> <code>cc</code> <code>Iterable[str] | str</code> <p>Cc address or iterable of addresses.</p> <code>None</code> <code>bcc</code> <code>Iterable[str] | str</code> <p>Bcc address or iterable of addresses.</p> <code>None</code> <code>sender</code> <code>str</code> <p>Default From address.</p> <code>None</code> <code>reply_to</code> <code>Iterable[str] | str</code> <p>Default Reply-To address.</p> <code>None</code> <code>attachments</code> <code>Iterable[EmailAttachment | str]</code> <p>An iterable of either filenames to attach or in-memory attachments.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.email.AwsSesLegacy","title":"AwsSesLegacy","text":"<pre><code>AwsSesLegacy(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Emailer</code></p> <p>Email sender implementation using AWS SES.</p> <p>See super class.</p>"},{"location":"18-api.html#lava.lib.email.AwsSesLegacy.send","title":"send","text":"<pre><code>send(\n    subject: str,\n    message: str,\n    to: Iterable[str] | str = None,\n    cc: Iterable[str] | str = None,\n    bcc: Iterable[str] | str = None,\n    sender: str = None,\n    reply_to: Iterable[str] | str = None,\n    attachments: Iterable[EmailAttachment | str] = None,\n) -&gt; None\n</code></pre> <p>Send an email using SES.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>Message subject. Must not be empty.</p> required <code>message</code> <code>str</code> <p>Message body. Must not be empty. If it looks like HTML some handlers will treat it differently.</p> required <code>to</code> <code>Iterable[str] | str</code> <p>Recipient address or iterable of addresses.</p> <code>None</code> <code>cc</code> <code>Iterable[str] | str</code> <p>Cc address or iterable of addresses.</p> <code>None</code> <code>bcc</code> <code>Iterable[str] | str</code> <p>Bcc address or iterable of addresses.</p> <code>None</code> <code>sender</code> <code>str</code> <p>Default From address.</p> <code>None</code> <code>reply_to</code> <code>Iterable[str] | str</code> <p>Default Reply-To address.</p> <code>None</code> <code>attachments</code> <code>Iterable[EmailAttachment | str]</code> <p>An iterable of either filenames to attach or in-memory attachments.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.email.EmailAttachment","title":"EmailAttachment  <code>dataclass</code>","text":"<pre><code>EmailAttachment(name: str, data: str | bytes)\n</code></pre> <p>For in-memory attachments.</p>"},{"location":"18-api.html#lava.lib.email.Emailer","title":"Emailer","text":"<pre><code>Emailer(\n    conn_spec: dict[str, Any],\n    realm: str,\n    sender: str = None,\n    aws_session: Session = None,\n    logger: Logger = None,\n)\n</code></pre> <p>Abstract base class for an outbound email handler.</p> <p>This is not a generic model but rather a specific adaptation for the purposes of lava.</p> <p>This is a context manager and can be used thus:</p> <pre><code>    with h as Emailer.handler(conn_spec, realm, sender) as h:\n        h.send(to='x@y.com', subject='Hello world', message='Yahoo')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>conn_spec</code> <code>dict[str, Any]</code> <p>A database connection specification.</p> required <code>realm</code> <code>str</code> <p>Lava realm.</p> required <code>sender</code> <code>str</code> <p>Default sender. If not specified, the <code>from</code> key in the conn_spec is used if present. Otherwise each handler has to work this out for itself.</p> <code>None</code> <code>aws_session</code> <code>Session</code> <p>A boto3 Session(). If not specified a default will be created if required. Default None.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>A logger. If not specified, use the root logger.</p> <code>None</code> <p>Create an Emailer instamnce.</p>"},{"location":"18-api.html#lava.lib.email.Emailer.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the connection.</p>"},{"location":"18-api.html#lava.lib.email.Emailer.handler","title":"handler  <code>classmethod</code>","text":"<pre><code>handler(\n    conn_spec: dict[str, Any], *args, **kwargs\n) -&gt; Emailer\n</code></pre> <p>Create a handler for the specified emailer type.</p> <p>The appropriate handler is selected by looking at the <code>type</code> and <code>subtype</code> elements of the connection spec.</p> <p>If the <code>type</code> matches a registered handler, that will be used.</p> <p>If <code>type</code> is <code>email</code>, then the <code>subtype</code> is used to find a handler.</p> <p>If <code>type</code> is <code>email</code>, and no <code>subtype</code> is specified, AWS SES is used. (This is a legacy of the email connection handler in lava).</p> <p>Otherwise an exception is raised.</p> <p>Parameters:</p> Name Type Description Default <code>conn_spec</code> <code>dict[str, Any]</code> <p>Lava connection spec. The handler required is determined from the type field.</p> required <p>Returns:</p> Type Description <code>Emailer</code> <p>An emailer handler.</p>"},{"location":"18-api.html#lava.lib.email.Emailer.message","title":"message","text":"<pre><code>message(\n    subject: str,\n    message: str,\n    to: Iterable[str] | str = None,\n    cc: Iterable[str] | str = None,\n    sender: str = None,\n    reply_to: Iterable[str] | str = None,\n    attachments: Iterable[EmailAttachment | str] = None,\n) -&gt; EmailMessage\n</code></pre> <p>Construct an email message.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>Message subject. Must not be empty.</p> required <code>message</code> <code>str</code> <p>Message body. Must not be empty. If it looks like HTML some handlers will treat it differently.</p> required <code>to</code> <code>Iterable[str] | str</code> <p>Recipient address or iterable of addresses.</p> <code>None</code> <code>cc</code> <code>Iterable[str] | str</code> <p>Cc address or iterable of addresses.</p> <code>None</code> <code>sender</code> <code>str</code> <p>Default From address.</p> <code>None</code> <code>reply_to</code> <code>Iterable[str] | str</code> <p>Default Reply-To address.</p> <code>None</code> <code>attachments</code> <code>Iterable[EmailAttachment | str]</code> <p>An iterable of either filenames to attach or in-memory attachments.</p> <code>None</code> <p>Returns:</p> Type Description <code>EmailMessage</code> <p>The message.</p>"},{"location":"18-api.html#lava.lib.email.Emailer.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(*args: str) -&gt; Callable\n</code></pre> <p>Register email handler classes.</p> <p>Usage:</p> <pre><code>@Emailer.register(emailer_type1, ...)\na_class(...)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>str</code> <p>A list of email subsystem types that the decorated class handles. These will correspond to the type/subtype fields in the conn_spec.</p> <code>()</code>"},{"location":"18-api.html#lava.lib.email.Emailer.send","title":"send","text":"<pre><code>send(\n    subject: str,\n    message: str,\n    to: Iterable[str] | str = None,\n    cc: Iterable[str] | str = None,\n    bcc: Iterable[str] | str = None,\n    sender: str = None,\n    reply_to: Iterable[str] | str = None,\n    attachments: Iterable[EmailAttachment | str] = None,\n) -&gt; None\n</code></pre> <p>Send an email.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>Message subject. Must not be empty.</p> required <code>message</code> <code>str</code> <p>Message body. Must not be empty. If it looks like HTML some handlers will treat it differently.</p> required <code>to</code> <code>Iterable[str] | str</code> <p>Recipient address or iterable of addresses.</p> <code>None</code> <code>cc</code> <code>Iterable[str] | str</code> <p>Cc address or iterable of addresses.</p> <code>None</code> <code>bcc</code> <code>Iterable[str] | str</code> <p>Bcc address or iterable of addresses.</p> <code>None</code> <code>sender</code> <code>str</code> <p>Default From address.</p> <code>None</code> <code>reply_to</code> <code>Iterable[str] | str</code> <p>Default Reply-To address.</p> <code>None</code> <code>attachments</code> <code>Iterable[EmailAttachment | str]</code> <p>An iterable of either filenames to attach or in-memory attachments.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.email.SmtpTls","title":"SmtpTls","text":"<pre><code>SmtpTls(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Emailer</code></p> <p>Email sender implementation using SMTP. Includes support for TLS.</p> <p>See super class.</p>"},{"location":"18-api.html#lava.lib.email.SmtpTls.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the connection.</p>"},{"location":"18-api.html#lava.lib.email.SmtpTls.send","title":"send","text":"<pre><code>send(\n    subject: str,\n    message: str,\n    to: Iterable[str] | str = None,\n    cc: Iterable[str] | str = None,\n    bcc: Iterable[str] | str = None,\n    sender: str = None,\n    reply_to: Iterable[str] | str = None,\n    attachments: Iterable[EmailAttachment | str] = None,\n) -&gt; None\n</code></pre> <p>Send an email via SMTP.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>Message subject. Must not be empty.</p> required <code>message</code> <code>str</code> <p>Message body. Must not be empty. If it looks like HTML some handlers will treat it differently.</p> required <code>to</code> <code>Iterable[str] | str</code> <p>Recipient address or iterable of addresses.</p> <code>None</code> <code>cc</code> <code>Iterable[str] | str</code> <p>Cc address or iterable of addresses.</p> <code>None</code> <code>bcc</code> <code>Iterable[str] | str</code> <p>Bcc address or iterable of addresses.</p> <code>None</code> <code>sender</code> <code>str</code> <p>Default From address.</p> <code>None</code> <code>reply_to</code> <code>Iterable[str] | str</code> <p>Default Reply-To address.</p> <code>None</code> <code>attachments</code> <code>Iterable[EmailAttachment | str]</code> <p>An iterable of either filenames to attach or in-memory attachments.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.email.content_type","title":"content_type","text":"<pre><code>content_type(filename: str) -&gt; tuple[str, str]\n</code></pre> <p>Try to guess the content type based on filename suffix.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>File name.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>(maintype, subtype) or a generic default.</p>"},{"location":"18-api.html#lava.lib.fileops","title":"lava.lib.fileops","text":"<p>File operation utilities.</p>"},{"location":"18-api.html#lava.lib.fileops.delete_files","title":"delete_files","text":"<pre><code>delete_files(*args: str) -&gt; None\n</code></pre> <p>Delete the files whose names are specified as arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>str</code> <p>File name(s). Empty values are ignored.</p> <code>()</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If any of the files could not be deleted. Attempting to delete a non-existent file is not an error.</p>"},{"location":"18-api.html#lava.lib.fileops.fsplit","title":"fsplit","text":"<pre><code>fsplit(\n    filename: str,\n    prefix: str,\n    maxsize: int,\n    suffixlen: int = 0,\n    delete: bool = False,\n) -&gt; Iterator[str]\n</code></pre> <p>Rough split a text file into pieces approximately maxsize or below.</p> <p>Pieces are allowed to be slightly larger than maxsize. Returns an iterator of file names. It's the caller's problem to make sure the chunks don't overwrite anything important.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the file to split. Must be a text file.</p> required <code>prefix</code> <code>str</code> <p>A prefix that will be used to generate the names of the split files.</p> required <code>maxsize</code> <code>int</code> <p>Maximum size of each split file.</p> required <code>suffixlen</code> <code>int</code> <p>The number of digits to use in the suffix for the split files. An attempt is made to estimate the number of digits required and the larger of the supplied value and the estimated value is used. Note that this can still fail to allocate enough digits in certain circumstances. An exception occurs when this happens.</p> <code>0</code> <code>delete</code> <code>bool</code> <p>If True, delete the original file after splitting. Default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator returning file names of the split files.</p>"},{"location":"18-api.html#lava.lib.fileops.lock_file","title":"lock_file","text":"<pre><code>lock_file(fname: str) -&gt; bool\n</code></pre> <p>Create a lock file with the given name and write the current process PID.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>Name of lockfile</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if lockfile created, False otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the lock file cannot be created.</p>"},{"location":"18-api.html#lava.lib.fileops.makedir","title":"makedir","text":"<pre><code>makedir(d: str) -&gt; None\n</code></pre> <p>Create the specified directory (and parents) if it doesn't exist.</p> <p>Deprecated as of v8.1.0</p> <p>Just use os.makedirs() instead.</p> <p>It is an error for d to already exist if its not a directory. Note that Python 2.7 makedirs() has no exist_ok support.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>str</code> <p>Directory name.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If d cannot be created or it already exists and is not a directory.</p>"},{"location":"18-api.html#lava.lib.fileops.read_head_or_tail","title":"read_head_or_tail","text":"<pre><code>read_head_or_tail(filename: str, size: int) -&gt; str\n</code></pre> <p>Read a limited size chunk from the beginning (or end) of a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of file.</p> required <code>size</code> <code>int</code> <p>Maximum number of bytes to read. Less data may be read if the file is smaller. If positive, read from the start of the file. If negative, read abs(size) bytes from the end of the file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The data as a string.</p>"},{"location":"18-api.html#lava.lib.fileops.sanitise_filename","title":"sanitise_filename","text":"<pre><code>sanitise_filename(value: str) -&gt; str\n</code></pre> <p>Turn an arbitrary string into something safe as a local filename.</p> <p>This code is based on slugify() from Django.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>String to sanitise.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Sanitised string</p>"},{"location":"18-api.html#lava.lib.fileops.unpack","title":"unpack","text":"<pre><code>unpack(pkg: str, dirname: str, timeout: int = 30) -&gt; None\n</code></pre> <p>Extract the specified package (e.g. tar or zip) into the specified directory.</p> <p>Existing files will be overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>pkg</code> <code>str</code> <p>The filename of the package.</p> required <code>dirname</code> <code>str</code> <p>The target directory. This will be created if it doesn't exist.</p> required <code>timeout</code> <code>int</code> <p>Time limit on extraction in seconds. Default 30 seconds.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.jsonschema","title":"lava.lib.jsonschema","text":"<p>JSONschema utilities.</p> <p>As of v7.1.0 (Pichincha), a full (well... pretty full) JSON schema is provided for the DyanmoDB entries. Over time, lava will be progressively more aggressive in validating against these.</p> <p>Why JSONschema instead of Cerberus, Pydantic ...?</p> <p>I don't know. Seemed like a good idea at the time and I can't be bothered changing it now.</p>"},{"location":"18-api.html#lava.lib.jsonschema.LavaSpecInfo","title":"LavaSpecInfo  <code>dataclass</code>","text":"<pre><code>LavaSpecInfo(table: str, key: str)\n</code></pre> <p>Lava DynamoDB table names and corresponding hash key.</p>"},{"location":"18-api.html#lava.lib.jsonschema.is_iso8601","title":"is_iso8601","text":"<pre><code>is_iso8601(value: str) -&gt; bool\n</code></pre> <p>Check if a string is ISO 8601.</p> <p>This is a bit more tolerant than the built in <code>date-time</code> format check in that it accepts values without a timezone.</p>"},{"location":"18-api.html#lava.lib.jsonschema.jsonschema_resolver_store_from_directory","title":"jsonschema_resolver_store_from_directory","text":"<pre><code>jsonschema_resolver_store_from_directory(\n    dirname: str, fmt: str = \"YAML\", validate=True\n) -&gt; dict[str, dict]\n</code></pre> <p>Create a jsonschema resolver store with contents of a directory pre-loaded.</p> <p>The keys in the store are the '$id' field from the file if present, otherwise the relative filename with the suffix removed and path separators converted to dots. So <code>a.b.c.yaml</code> and <code>a/b/c.yaml</code> both become <code>a.b.c</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dirname</code> <code>str</code> <p>Directory name.</p> required <code>fmt</code> <code>str</code> <p>Either YAML or JSON. Schema files must end with <code>.yaml</code> or <code>.json</code> respectively.</p> <code>'YAML'</code> <code>validate</code> <p>If True, validate schemas as they are loaded.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>A schema resolver store which is basically a mapping from a $ref value to the schema contents.</p>"},{"location":"18-api.html#lava.lib.logging","title":"lava.lib.logging","text":"<p>Logging utilities.</p>"},{"location":"18-api.html#lava.lib.logging.ColourLogHandler","title":"ColourLogHandler","text":"<pre><code>ColourLogHandler(colour: bool = True)\n</code></pre> <p>               Bases: <code>Handler</code></p> <p>Basic stream handler that writes to stderr with colours for log levels.</p> <p>Allow colour to be enabled or disabled.</p> <p>Parameters:</p> Name Type Description Default <code>colour</code> <code>bool</code> <p>If True colour is enabled for log messages. Default True.</p> <code>True</code>"},{"location":"18-api.html#lava.lib.logging.ColourLogHandler.emit","title":"emit","text":"<pre><code>emit(record: LogRecord) -&gt; None\n</code></pre> <p>Print the record to stderr with some colour enhancement.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>Log record</p> required"},{"location":"18-api.html#lava.lib.logging.JsonFormatter","title":"JsonFormatter","text":"<pre><code>JsonFormatter(\n    fields: dict[str, str] = None,\n    extra: dict[str, str] = None,\n    datefmt: str = None,\n    tag: str = None,\n)\n</code></pre> <p>               Bases: <code>Formatter</code></p> <p>Format log records in JSON format.</p> <p>Thanks to: https://www.velebit.ai/blog/tech-blog-python-json-logging/</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <p>A dictionary of fields to use in the log. Keys will be used in the final log record. Values are the names of the attributes from the log record.  values.</p> <code>None</code> <code>extra</code> <code>dict[str, str]</code> <p>Additional fields to add to all records.</p> <code>None</code> <code>datefmt</code> <code>str</code> <p>As per logging.Formatter.</p> <code>None</code> <code>tag</code> <code>str</code> <p>Preceed the JSON encode log record with <code>{tag}:</code>. This is very important when logging to rsyslog as this provides the value of the rsyslog <code>syslogtag</code>. If not specified, rsyslog will try to inseet <code>[pid]</code> before the first space in the JSON blob, which will wreck the JSON. When just logging to a file, this parameter will generally be None to produce a well-formed JSON blob for each line.</p> <code>None</code> <p>Create a JSON formatter.</p>"},{"location":"18-api.html#lava.lib.logging.JsonFormatter.format","title":"format","text":"<pre><code>format(record: LogRecord) -&gt; str\n</code></pre> <p>Format a log record as JSON.</p>"},{"location":"18-api.html#lava.lib.logging.JsonFormatter.formatTime","title":"formatTime","text":"<pre><code>formatTime(record, datefmt=None)\n</code></pre> <p>Return the creation time of the specified LogRecord as formatted text.</p> <p>This is basically the Python 3.9 standard implementation. It is included here to compensate for an issue with the Python 3.8 version that barfs if default_msec_fmt is set to None.</p>"},{"location":"18-api.html#lava.lib.logging.JsonFormatter.isotime","title":"isotime  <code>staticmethod</code>","text":"<pre><code>isotime(record)\n</code></pre> <p>Return the creation time as a precise ISO 8601 string in UTC.</p>"},{"location":"18-api.html#lava.lib.logging.JsonFormatter.usesTime","title":"usesTime","text":"<pre><code>usesTime()\n</code></pre> <p>Check if the format uses the creation time of the record.</p>"},{"location":"18-api.html#lava.lib.logging.get_log_level","title":"get_log_level","text":"<pre><code>get_log_level(s: str) -&gt; int\n</code></pre> <p>Convert string log level to the corresponding integer log level.</p> <p>Raises ValueError if a bad string is provided.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>A string version of a log level (e.g. 'error', 'info'). Case is not significant.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The numeric logLevel equivalent.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the supplied string cannot be converted.</p>"},{"location":"18-api.html#lava.lib.logging.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(\n    level: str,\n    target: str = None,\n    colour: bool = True,\n    name: str = None,\n    prefix: str = None,\n    formatter: Formatter = None,\n) -&gt; None\n</code></pre> <p>Set up logging.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level. The string format of a level (eg 'debug').</p> required <code>target</code> <code>str</code> <p>Logging target. Either a file name or a syslog facility name starting with @ or None. If None, log to stderr.</p> <code>None</code> <code>colour</code> <code>bool</code> <p>If True and logging to the terminal, colourise messages for different logging levels. Default True.</p> <code>True</code> <code>name</code> <code>str</code> <p>The name of the logger to configure. If None, configure the root logger.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Messages are prefixed by this string (with colon+space appended). Defaults to None but it is important this is set when logging to rsyslog otherwise syslog may mangle the message.</p> <code>None</code> <code>formatter</code> <code>Formatter</code> <p>Use the specified logging formatter instead of the default. The default varies a bit depending on log target.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid log level or syslog facility is specified.</p>"},{"location":"18-api.html#lava.lib.logging.syslog_address","title":"syslog_address","text":"<pre><code>syslog_address() -&gt; str | tuple\n</code></pre> <p>Try to work out the syslog address.</p> <p>Returns:</p> Type Description <code>str | tuple</code> <p>A value suitable for use as the address arg for SysLogHandler.</p>"},{"location":"18-api.html#lava.lib.misc","title":"lava.lib.misc","text":"<p>Miscellaneous utilities.</p>"},{"location":"18-api.html#lava.lib.misc.Defer","title":"Defer","text":"<pre><code>Defer(event: str)\n</code></pre> <p>Class to manage deferred tasks.</p> <p>There is only one singleton per event type.</p> <p>This has some similarities to atexit but with a bit more flexibility.</p> <p>Warning</p> <p>Do NOT instantiate this directly. Use Defer.on_event().</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>str</code> <p>A label indicating the event for which the deferred tasks are waiting.</p> required <p>Create a singleton deferral register for a given event type.</p>"},{"location":"18-api.html#lava.lib.misc.Defer.add","title":"add","text":"<pre><code>add(task: Task) -&gt; int\n</code></pre> <p>Add a deferred task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The task to be added.</p> required <p>Returns:</p> Type Description <code>int</code> <p>A unique identifier for the task that can be used to cancel it.</p>"},{"location":"18-api.html#lava.lib.misc.Defer.cancel","title":"cancel","text":"<pre><code>cancel(task_id: int) -&gt; None\n</code></pre> <p>Cancel the specified task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> <p>The ID of the task when it was created.</p> required"},{"location":"18-api.html#lava.lib.misc.Defer.on_event","title":"on_event  <code>classmethod</code>","text":"<pre><code>on_event(event: str) -&gt; Defer\n</code></pre> <p>Create or retrieve the deferred task handler for the given event.</p> <p>This is a factory method to make sure this is a singleton for each event type.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>str</code> <p>A label indicating the event for which the deferred tasks are waiting.</p> required"},{"location":"18-api.html#lava.lib.misc.Defer.run","title":"run","text":"<pre><code>run(logger: Logger = None) -&gt; list[TaskResult]\n</code></pre> <p>Run all the registered tasks in a last in, first out order.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>If specified, activity will be sent to the logger.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.misc.DictChecksum","title":"DictChecksum  <code>dataclass</code>","text":"<pre><code>DictChecksum(\n    hashval: str,\n    algorithm: str = \"sha256\",\n    version: int = CHECKSUM_DEFAULT_VERSION,\n    tag: str = CHECKSUM_DEFAULT_TAG,\n)\n</code></pre> <p>Simple representation of a lava style checksum on a dict.</p>"},{"location":"18-api.html#lava.lib.misc.DictChecksum.for_dict","title":"for_dict  <code>classmethod</code>","text":"<pre><code>for_dict(\n    d: dict[str, Any],\n    ignore: str | Iterable[str] = None,\n    algorithm: str = HASH_ALGORITHM,\n    version=CHECKSUM_DEFAULT_VERSION,\n    tag: str = CHECKSUM_DEFAULT_TAG,\n) -&gt; DictChecksum\n</code></pre> <p>Create a checksum from a dict.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict[str, Any]</code> <p>The dictionary. It must be JSON serialisable.</p> required <code>ignore</code> <code>str | Iterable[str]</code> <p>Ignore any keys that match the specified glob pattern or list of patterns.</p> <code>None</code> <code>algorithm</code> <code>str</code> <p>Hashing algorithm. Must be one of the values supported by <code>hashlib.algorithms_guaranteed</code>.</p> <code>HASH_ALGORITHM</code> <code>version</code> <p>Checksum format version. This version stuff is really just a placeholder in case we change formats in future. Nothing much is done with it at the moment.</p> <code>CHECKSUM_DEFAULT_VERSION</code> <code>tag</code> <code>str</code> <p>The checksum tag. This is helpful to identify the source of the checksum</p> <code>CHECKSUM_DEFAULT_TAG</code>"},{"location":"18-api.html#lava.lib.misc.DictChecksum.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(s: str) -&gt; DictChecksum\n</code></pre> <p>Connvert from representational format.</p>"},{"location":"18-api.html#lava.lib.misc.DictChecksum.is_valid_for","title":"is_valid_for","text":"<pre><code>is_valid_for(\n    d: dict, ignore: str | Iterable[str] = None\n) -&gt; bool\n</code></pre> <p>Check that this checksum is valid for the given dict.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The dictionary. It must be JSON serialisable.</p> required <code>ignore</code> <code>str | Iterable[str]</code> <p>Ignore any keys that match the specified glob pattern or list of patterns.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.misc.Task","title":"Task  <code>dataclass</code>","text":"<pre><code>Task(\n    description: str,\n    action: Callable,\n    args: list[Any] | None = None,\n    kwargs: dict[str, Any] | None = None,\n)\n</code></pre> <p>Basic task model.</p>"},{"location":"18-api.html#lava.lib.misc.TaskResult","title":"TaskResult  <code>dataclass</code>","text":"<pre><code>TaskResult(\n    task: Task, result: Any, exception: Exception | None\n)\n</code></pre> <p>Result of running a task.</p>"},{"location":"18-api.html#lava.lib.misc.TrackedMapping","title":"TrackedMapping","text":"<pre><code>TrackedMapping(\n    data: dict[str, Any], default_factory: Callable = None\n)\n</code></pre> <p>               Bases: <code>MutableMapping</code></p> <p>Catch and record references to dictionary items (known and unknown).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>The data dictionary being tracked.</p> required <p>Catch references to unknown items and attributes and record them.</p>"},{"location":"18-api.html#lava.lib.misc.clean_str","title":"clean_str","text":"<pre><code>clean_str(s: str, safe_chars=None, alternative=None) -&gt; str\n</code></pre> <p>Clean a string by removing or replacing anything except word chars and safe chars.</p> <p>The result will only contain word chars (alphanum + underscore) and the specified safe_chars.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to clean.</p> required <code>safe_chars</code> <p>Safe chars that can remain but not at the beginning or end of the string.</p> <code>None</code> <code>alternative</code> <p>Replace unsafe chars with the specified alternative. Must be a single character string. If not specified, unsafe chars are removed.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The cleaned string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the string cannot be cleansed or the result is empty.</p>"},{"location":"18-api.html#lava.lib.misc.decimal_to_scalar","title":"decimal_to_scalar","text":"<pre><code>decimal_to_scalar(d: Decimal) -&gt; int | float\n</code></pre> <p>Convert a decimal to an int or float, whichever is more appropriate.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Decimal</code> <p>A decimal.</p> required <p>Returns:</p> Type Description <code>int | float</code> <p>An equivalent int or float.</p>"},{"location":"18-api.html#lava.lib.misc.dict_check","title":"dict_check","text":"<pre><code>dict_check(\n    d: dict[str, Any],\n    required: Iterable[str] = None,\n    optional: Iterable[str] = None,\n    ignore: str | Iterable[str] = None,\n) -&gt; None\n</code></pre> <p>Check that the given dictionary has the required keys.</p> <p>Patheric attempt at exculpation ...</p> <p>This is a horrible implementation of good intentions dating from the year dot. Sorry. If we were doing it all again, we'd use a proper object model with Pydantic, or something like that.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict[str, Any]</code> <p>The dict to check.</p> required <code>required</code> <code>Iterable[str]</code> <p>An iterable of mandatory keys. Can be None indicating required keys should not be checked.</p> <code>None</code> <code>optional</code> <code>Iterable[str]</code> <p>An iterable of optional keys. Can be None indicating optional keys should not be checked.</p> <code>None</code> <code>ignore</code> <code>str | Iterable[str]</code> <p>Ignore any keys that match the specified glob pattern or list of patterns.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dict doesn't contain all required keys or does contain disallowed keys.</p>"},{"location":"18-api.html#lava.lib.misc.dict_expand_keys","title":"dict_expand_keys","text":"<pre><code>dict_expand_keys(\n    d: dict[str, Any], sep_pat: str = \"\\\\.\"\n) -&gt; dict[str, Any]\n</code></pre> <p>Expand a dictionary whose keys contain a hierarchy separator pattern.</p> <p>A new hierarchical dictionary is created and the original dictionary is unchanged.</p> <p>For example, a dictionary containing { 'a.b': 10 } would be expanded to {'a': {'b' : 10}}.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict[str, Any]</code> <p>A dictionary with string keys.</p> required <code>sep_pat</code> <code>str</code> <p>A regular expression used to split dictionary keys into hierarchies. Default is a dot. Be very careful with capture groups. It will almost certainly not do what you expect. If you must use groups, then try using the non-capturing style (?:...).</p> <code>'\\\\.'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The expanded dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any keys are not strings or paths conflict</p>"},{"location":"18-api.html#lava.lib.misc.dict_hash","title":"dict_hash","text":"<pre><code>dict_hash(\n    d: dict,\n    ignore: str | Iterable[str] = None,\n    algorithm: str = HASH_ALGORITHM,\n) -&gt; str\n</code></pre> <p>Calculate an ASCII safe hash on a dictionary.</p> <p>Warning</p> <p>This is not cryptographically secure and should not be used for any security related purpose. It's only for change detection without additional cryptographic protection.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The dictionary. It must be JSON serialisable.</p> required <code>ignore</code> <code>str | Iterable[str]</code> <p>Ignore any keys that match the specified glob pattern or list of patterns.</p> <code>None</code> <code>algorithm</code> <code>str</code> <p>Hashing algorithm. Must be one of the values supported by <code>hashlib.new()</code>.</p> <code>HASH_ALGORITHM</code> <p>Returns:</p> Type Description <code>str</code> <p>An ASCII safe hash.</p>"},{"location":"18-api.html#lava.lib.misc.dict_select","title":"dict_select","text":"<pre><code>dict_select(d: dict, *keys: Hashable) -&gt; dict\n</code></pre> <p>Filter a dictionary to create a new dictionary containing only the specified keys.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary.</p> required <code>keys</code> <code>Hashable</code> <p>One or more keys.</p> <code>()</code> <p>Returns:</p> Type Description <code>dict</code> <p>A new dictionary containing only subset of dict wih keys in the given list.</p>"},{"location":"18-api.html#lava.lib.misc.dict_set_deep","title":"dict_set_deep","text":"<pre><code>dict_set_deep(\n    d: dict, keys: list[str] | tuple[str], v: Any\n) -&gt; None\n</code></pre> <p>Set a value in a dict based on a sequence of keys.</p> <p>Subdicts are created on the way as required.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The dictionary.</p> required <code>keys</code> <code>list[str] | tuple[str]</code> <p>A list or tuple of strings.</p> required <code>v</code> <code>Any</code> <p>The value to set.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If one of the elements along the path is not a dict.</p>"},{"location":"18-api.html#lava.lib.misc.dict_strip","title":"dict_strip","text":"<pre><code>dict_strip(d: dict) -&gt; dict\n</code></pre> <p>Return a new dictionary with all None value elements removed.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>Input dictionary.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>New dict with None value keys removed.</p>"},{"location":"18-api.html#lava.lib.misc.format_dict_unescaped","title":"format_dict_unescaped","text":"<pre><code>format_dict_unescaped(obj: dict, _depth=0) -&gt; str\n</code></pre> <p>Create a string representaiton of an object without escaped strings.</p> <p>Use case for this is very specialised. Tread carefully.</p>"},{"location":"18-api.html#lava.lib.misc.format_sequence_unescaped","title":"format_sequence_unescaped","text":"<pre><code>format_sequence_unescaped(\n    seq: list | tuple, _depth=0\n) -&gt; str\n</code></pre> <p>Create a string representaiton of a sequence without escaped strings.</p>"},{"location":"18-api.html#lava.lib.misc.glob_strip","title":"glob_strip","text":"<pre><code>glob_strip(\n    names: Iterable[str], patterns: str | Iterable[str]\n) -&gt; set[str]\n</code></pre> <p>Remove from an iterable of strings any that match any of the given patterns.</p> <p>Patterns are glob style. Case is significant.</p> <p>The result is returned as a set so any ordering is lost.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Iterable[str]</code> <p>An iterable of strings to match.</p> required <code>patterns</code> <code>str | Iterable[str]</code> <p>A glob pattern or iterable of glob patterns.</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>A set containing all input strings that don't match any of the glob patterns.</p>"},{"location":"18-api.html#lava.lib.misc.import_by_name","title":"import_by_name","text":"<pre><code>import_by_name(name: str, parent: str = None) -&gt; ModuleType\n</code></pre> <p>Import a named module from within the named parent.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the required module.</p> required <code>parent</code> <code>str</code> <p>Name of parent module. Default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ModuleType</code> <p>The sender module.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the import fails.</p>"},{"location":"18-api.html#lava.lib.misc.is_html","title":"is_html","text":"<pre><code>is_html(s: str) -&gt; bool\n</code></pre> <p>Try to guess if the given string is HTML.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>A string.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if string appears to be HTML, False otherwise.</p>"},{"location":"18-api.html#lava.lib.misc.is_quoted","title":"is_quoted","text":"<pre><code>is_quoted(s: str, quote: str = \"'\") -&gt; bool\n</code></pre> <p>Return true if the given string is surrounded by the given quote string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to check.</p> required <code>quote</code> <code>str</code> <p>The quote string. Default is single quote.</p> <code>\"'\"</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if quoted, False otherwise.</p>"},{"location":"18-api.html#lava.lib.misc.json_default","title":"json_default","text":"<pre><code>json_default(obj: Any) -&gt; Any\n</code></pre> <p>Serialise non-standard objects for json.dumps().</p> <p>This is a helper function for JSON serialisation with json.dumps() to allow (UTC) datetime and time objects to be serialised. It should be used thus:</p> <pre><code>json_string = json.dumps(object_of_some_kind, default=json_default)\n</code></pre> <p>It is primarily used in API responses.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>An object.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A serialisable version. For datetime objects we just convert them to a string that strptime() could handle.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If obj cannot be serialised.</p>"},{"location":"18-api.html#lava.lib.misc.listify","title":"listify","text":"<pre><code>listify(v: Any) -&gt; list\n</code></pre> <p>Convert the argument to a list.</p> <p>If it's a string, then it becomes a list of one element (the source string). If it's a list, it's returned unchanged. If it is some other kind of iterable, it's converted to a list.</p> <p>Warning</p> <p>Be careful when passing a dict as an argument. It will return the keys in a list, not a list containing the dict as its only element.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Any</code> <p>The source var.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If not iterable.</p>"},{"location":"18-api.html#lava.lib.misc.match_any","title":"match_any","text":"<pre><code>match_any(\n    s: str, globs: list[str], ignore_case: bool = False\n) -&gt; bool\n</code></pre> <p>Check if a string matches any glob pattern in a list of patterns.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to match.</p> required <code>globs</code> <code>list[str]</code> <p>A list of glob style patterns.</p> required <code>ignore_case</code> <code>bool</code> <p>If True ignore case.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the string matches any pattern, False otherwise.</p>"},{"location":"18-api.html#lava.lib.misc.match_none","title":"match_none","text":"<pre><code>match_none(\n    s: str, globs: list[str], ignore_case: bool = False\n) -&gt; bool\n</code></pre> <p>Check that a string matches none of the glob pattern in a list of patterns.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to match.</p> required <code>globs</code> <code>list[str]</code> <p>A list of glob style patterns.</p> required <code>ignore_case</code> <code>bool</code> <p>If True igore case.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>False if the string matches any pattern, True otherwise.</p>"},{"location":"18-api.html#lava.lib.misc.pythonpath_prepended","title":"pythonpath_prepended","text":"<pre><code>pythonpath_prepended(path: str | Path)\n</code></pre> <p>Temporarily prepend a path to PYTHONPATH.</p>"},{"location":"18-api.html#lava.lib.misc.sepjoin","title":"sepjoin","text":"<pre><code>sepjoin(sep: str, *args: str | list[str]) -&gt; str\n</code></pre> <p>Join all the non-empty args with the specified separator.</p> <p>Any list args are expanded.</p> <p>ie. if sep is <code>/</code> and with args of: <code>'a', [], ['b', 'c']'</code> will return <code>a/b/c</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sep</code> <code>str</code> <p>A separator string used as a joiner.</p> required <code>args</code> <code>str | list[str]</code> <p>A string or list of strings.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>Joined up args separated by /</p>"},{"location":"18-api.html#lava.lib.misc.size_to_bytes","title":"size_to_bytes","text":"<pre><code>size_to_bytes(size: str | int) -&gt; int\n</code></pre> <p>Convert a string specifying a data size to a number of bytes.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>str | int</code> <p>String in the form nnnX where nnn is an integer or float and X is one of (case sensitive): 'B'         Bytes 'K', 'KB':  Kilobytes (1000) 'M', 'MB':  Megabytes 'G', 'GB':  Gigabytes 'T', 'TB':  Terabytes 'P', 'PB':  Petabytes. 'KiB':      Kibibytes (1024) 'MiB':      Mebibytes 'GiB':      Gibibytes 'TiB':      Tebibytes 'PiB':      Pebibytes  Whitespace is ignored.  Note a leading + or - will be handled correctly as will exponentials. If no multiplier suffix is provided, bytes are assumed.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The size in bytes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is malformed.</p>"},{"location":"18-api.html#lava.lib.misc.splitext2","title":"splitext2","text":"<pre><code>splitext2(\n    path: str,\n    pathsep: str = os.sep,\n    extsep: str = os.extsep,\n) -&gt; tuple[str, str]\n</code></pre> <p>Split a string into root + extension.</p> <p>This is a variation on os.path.splitext() except that a suffix is defined as everything from the first dot in the basename onwards, unlike splitext() which uses the last dot in the basename..</p> <p>Also splitext() always uses os.sep and os.extsep whereas splitext2 allows these to be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path.</p> required <code>pathsep</code> <code>str</code> <p>Path separator. Defaults to os.sep.</p> <code>sep</code> <code>extsep</code> <code>str</code> <p>Suffix separator. Defaults to os.extsep.</p> <code>extsep</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>A tuple (root, ext) such that root + ext == path</p>"},{"location":"18-api.html#lava.lib.misc.str2bool","title":"str2bool","text":"<pre><code>str2bool(s: str | bool) -&gt; bool\n</code></pre> <p>Convert a string to a boolean.</p> <p>This is a (case insensitive) semantic conversion.</p> <pre><code>'true', 't', 'yes', 'y', non-zero int as str --&gt; True\n'false', 'f', 'no', 'n', zero as str --&gt; False\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str | bool</code> <p>A boolean or a string representing a boolean. Whitespace is stripped. Boolean values are passed back unchanged.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean derived from the input value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value cannot be converted.</p>"},{"location":"18-api.html#lava.lib.os","title":"lava.lib.os","text":"<p>OS related utilities.</p>"},{"location":"18-api.html#lava.lib.os.makedirs","title":"makedirs","text":"<pre><code>makedirs(\n    path: str, mode: int = 511, exist_ok: bool = False\n) -&gt; None\n</code></pre> <p>Create directories.</p> <p>Deprecated as of v8.1.0</p> <p>Just use os.makedirs() instead.</p> <p>Repackaging of os.makedirs() to ignore file exists error.</p> <p>This is required due to a bug in os.makedirs() in Python 3.4.0 which is fixed in 3.4.1.</p> <p>See https://bugs.python.org/issue13498 and  https://bugs.python.org/issue21082</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>As for os.makedirs()</p> required <code>mode</code> <code>int</code> <p>As for os.makedirs()</p> <code>511</code> <code>exist_ok</code> <code>bool</code> <p>As for os.makedirs()</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"18-api.html#lava.lib.os.signame","title":"signame","text":"<pre><code>signame(sig: str | int) -&gt; str\n</code></pre> <p>Convert a signal to the corresponding name.</p> <p>Parameters:</p> Name Type Description Default <code>sig</code> <code>str | int</code> <p>Either a signal number of name. If the latter it is converted to its cannonical form (ie. SIGINT not INT).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The signal name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the signal is not known.</p>"},{"location":"18-api.html#lava.lib.os.signum","title":"signum","text":"<pre><code>signum(sig: str | int) -&gt; int\n</code></pre> <p>Convert a signal name to the corresponding signal number.</p> <p>e.g <code>SIGINT</code> or <code>INT</code> --&gt; 2.</p> <p>Parameters:</p> Name Type Description Default <code>sig</code> <code>str | int</code> <p>Either a signal name or a signal number. If the latter it is checked for validity and returned unchanged if valid.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The signal number.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the signal is not known.</p>"},{"location":"18-api.html#lava.lib.process","title":"lava.lib.process","text":"<p>Process mgmt utilities.</p>"},{"location":"18-api.html#lava.lib.process.killpg","title":"killpg","text":"<pre><code>killpg(pgid: int, signal: int)\n</code></pre> <p>Signal the specified process group but don't complain about non-existing groups.</p>"},{"location":"18-api.html#lava.lib.process.runpg","title":"runpg","text":"<pre><code>runpg(\n    *popenargs,\n    start_new_session=False,\n    input=None,\n    capture_output=False,\n    timeout=None,\n    check=False,\n    kill_event=None,\n    **kwargs\n)\n</code></pre> <p>Run command with arguments and return a CompletedProcess instance.</p> <p>This is almost identical to standard library <code>subprocess.run()</code> except that if <code>start_new_session</code> is True, instead of killing just the child process on termination, it kills the entire process group.</p> <p>However... This creates a second problem. Because child processes are no longer in the same process group as the parent, they don't get cleaned up on exit of the parent. If this is a problem, use the additional <code>kill_event</code> arg.</p> <p>So... If <code>kill_event</code> is set to the name of an event (e.g. \"on_exit\"), a deferred task is set recorded that can be run to kill the child process group by the caller at some later time. This is not an automatic process. The caller has to explicit request any deferred tasks be run at an appropriate time.</p> <p>Thanks to: https://alexandra-zaharia.github.io/posts/kill-subprocess-and-its-children-on-timeout-python</p> <p>Oh ... and DOS support has been removed. Suffer in your jocks DOSburgers.</p>"},{"location":"18-api.html#lava.lib.sharepoint","title":"lava.lib.sharepoint","text":"<p>Model a SharePoint site.</p> <p>Uses the Graph API.</p>"},{"location":"18-api.html#lava.lib.sharepoint.KeyExists","title":"KeyExists","text":"<p>Used for object search function.</p>"},{"location":"18-api.html#lava.lib.sharepoint.SharePointError","title":"SharePointError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for other exceptions.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint","title":"Sharepoint","text":"<pre><code>Sharepoint(\n    org_base_url: str,\n    site_name: str,\n    tenant: str,\n    client_id: str,\n    client_secret: str,\n    user: str,\n    password: str,\n    https_proxy: str = None,\n    logger: Logger = None,\n)\n</code></pre> <p>A SharePoint class.</p> <p>Obtains a valid token (immediately usable) using supplied credentials.</p> <p>Parameters:</p> Name Type Description Default <code>org_base_url</code> <code>str</code> <p>Base URL for the organisation's SharePoint.</p> required <code>site_name</code> <code>str</code> <p>SharePoint site name that we want to put content</p> required <code>tenant</code> <code>str</code> <p>Azure AD registered domain id</p> required <code>client_id</code> <code>str</code> <p>UUID of the Azure AD registered app under the registered Domain (registration has Microsoft graph API credentials)</p> required <code>client_secret</code> <code>str</code> <p>Credentials of the Azure AD registered app.</p> required <code>user</code> <code>str</code> <p>Delegated user credentials to access graph API.</p> required <code>password</code> <code>str</code> <p>Delegated user credentials</p> required <code>https_proxy</code> <code>str</code> <p>HTTPS proxy.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>For logging.</p> <code>None</code> <p>Create a SharePoint instance.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.batch_call","title":"batch_call","text":"<pre><code>batch_call(\n    request_list: list[dict],\n    batch_error: str,\n    individual_requests: bool = False,\n) -&gt; tuple[bool, dict]\n</code></pre> <p>POST as a batch request_list using Microsoft Graph batch calls.</p> <p>Uses MS Graph API batch POST</p> <p>There can only be 20 items at once. The assumption is request_list has no more than 20 values.  Try hard to get the data in. If we have throttled issues we resort to single item inserts.  On http error types we reattempt up to API errors threashold before failing.</p> <p>Parameters:</p> Name Type Description Default <code>request_list</code> <code>list[dict]</code> <p>The list of MS Graph API requests to post in the batch</p> required <code>batch_error</code> <code>str</code> <p>On response error what message is to be raised in the SharePointError</p> required <code>individual_requests</code> <code>bool</code> <p>Process each item on the batch individually, to handle throttling back-off</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[bool, dict]</code> <p>Two values (bool, int): (1) If the batch was throttled at any time so we can implement back-off better and (2) the requests that processed with errors that wouldn't reprocess. This is currently an empty dict as instead we hard fail with a raised exception after trying hard to get data.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.check_refresh_token","title":"check_refresh_token","text":"<pre><code>check_refresh_token() -&gt; None\n</code></pre> <p>Refresh an OAUTH v2 Graph API Token if it has expired.</p> <p>The number of refreshes is limited.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the connection.</p> <p>For SharePoint this is a no-op but it should be called for consistency with other connectors as things may change in future.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.delete_all_list_items","title":"delete_all_list_items","text":"<pre><code>delete_all_list_items(list_id: str, list_name: str) -&gt; None\n</code></pre> <p>Delete all items from a specified SharePoint list.</p> <p>Uses Microsoft Graph calls.</p> <p>Parameters:</p> Name Type Description Default <code>list_id</code> <code>str</code> <p>SharePoint List id</p> required <code>list_name</code> <code>str</code> <p>SharePoint Listname</p> required"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.delete_all_list_items_batch","title":"delete_all_list_items_batch","text":"<pre><code>delete_all_list_items_batch(\n    list_id: str, list_name: str\n) -&gt; None\n</code></pre> <p>Batch delete all items from a specified SharePoint list.</p> <p>Uses MS Graph API batch POST to delete a batch of 20 items at a time.</p> <p>Parameters:</p> Name Type Description Default <code>list_id</code> <code>str</code> <p>SharePoint List id</p> required <code>list_name</code> <code>str</code> <p>SharePoint Listname</p> required"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.dump_col_list","title":"dump_col_list","text":"<pre><code>dump_col_list(list_name: str) -&gt; None\n</code></pre> <p>Dump the json response of a SharePoint list with column information.</p> <p>For debug only purpose. File is dumped as <code>YYYYMMDDHHMMSS.MSS_columnlist_dt.json</code> requires request_dump directory to exist</p> <p>Parameters:</p> Name Type Description Default <code>list_name</code> <code>str</code> <p>SharePoint list name as found on the SharePoint site (existence is verified).</p> required"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.find_doc_library_drive_id","title":"find_doc_library_drive_id","text":"<pre><code>find_doc_library_drive_id(lib_name: str) -&gt; str\n</code></pre> <p>Find the drive_id of a SharePoint documentLibrary by lib_name.</p> <p>Parameters:</p> Name Type Description Default <code>lib_name</code> <code>str</code> <p>Sharepoint documentLibrary name to find.</p> required <p>Returns:</p> Type Description <code>str</code> <p>drive_id that was found.</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>If the upload fails.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.find_list_id","title":"find_list_id","text":"<pre><code>find_list_id(\n    list_name: str, list_type: str = \"only_generic\"\n) -&gt; str\n</code></pre> <p>Find the list_id of a SharePoint list by list_name.</p> <p>Parameters:</p> Name Type Description Default <code>list_name</code> <code>str</code> <p>Sharepoint list name to find.</p> required <code>list_type</code> <code>str</code> <p>list types to return.</p> <code>'only_generic'</code> <p>Returns:</p> Type Description <code>str</code> <p>list_id that was found.</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>If the upload fails.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.get_doc","title":"get_doc","text":"<pre><code>get_doc(lib_name: str, path: str, out_file: str) -&gt; str\n</code></pre> <p>Get a document from SharePoint documentLibrary lib_name.</p> <p>Delegated user requires read access to the documentLibrary.  AzureAD application required Graph API access Sites.ReadWrite.All</p> <p>Parameters:</p> Name Type Description Default <code>lib_name</code> <code>str</code> <p>SharePoint documentLibrary name as found on the SharePoint site (existence is verified).</p> required <code>path</code> <code>str</code> <p>full documentLibrary path (filename included) to fetch. Must be abolute with leading /.</p> required <code>out_file</code> <code>str</code> <p>Name of file we will write data to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Title of the SharePoint document. Can be None if the doc has no title.</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>If the document doesn't exist or cannot be downloaded.</p> <code>ValueError</code> <p>If bad parameters.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.get_doc_by_id","title":"get_doc_by_id","text":"<pre><code>get_doc_by_id(\n    drive_id: str, item_id: str, out_file: str\n) -&gt; None\n</code></pre> <p>Get a document from SharePoint documentLibrary lib_name.</p> <p>For already known drive_id, item_id.  Write to out_file</p> <p>Parameters:</p> Name Type Description Default <code>drive_id</code> <code>str</code> <p>SharePoint documentLibrary Drive ID (obtained previously)</p> required <code>item_id</code> <code>str</code> <p>id of file to download (obtained previously)</p> required <code>out_file</code> <code>str</code> <p>Name of file we will write data to.</p> required <p>Raises:</p> Type Description <code>SharePointError</code> <p>If the document doesn't exist or cannot be downloaded.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.get_doc_list_glob","title":"get_doc_list_glob","text":"<pre><code>get_doc_list_glob(\n    doc_list: list[dict], out_path: str, glob: str = None\n) -&gt; list[str]\n</code></pre> <p>From doc_list download all files or all glob matched files to out_path directory.</p> <p>Enforce DOS style glob matching on the filename, i.e. case insensitive.</p> <p>Parameters:</p> Name Type Description Default <code>doc_list</code> <code>list[dict]</code> <p>List of documents in index 1 onwards to download to out_path.</p> required <code>out_path</code> <code>str</code> <p>directory to download sharepoint file to.</p> required <code>glob</code> <code>str</code> <p>glob pattern to match on.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of files downloaded.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.get_list","title":"get_list","text":"<pre><code>get_list(\n    list_name: str,\n    out_file: str,\n    system_columns: str = None,\n    data_columns: str = None,\n    header: bool = True,\n    **csv_writer_args\n) -&gt; int\n</code></pre> <p>Get a SharePoint List list_name and write as CSV file with header.</p> <p>Delegated user requires read access to the List. AzureAD application requires Graph API access Sites.ReadWrite.All</p> <p>Parameters:</p> Name Type Description Default <code>list_name</code> <code>str</code> <p>SharePoint List name as found on the SharePoint site (existence is verified).</p> required <code>out_file</code> <code>str</code> <p>CSV out_file. Where to write the last as plain text csv</p> required <code>system_columns</code> <code>str</code> <p>Comma separated list of identified system columns to get in addition to data columns.</p> <code>None</code> <code>data_columns</code> <code>str</code> <p>Comma separated list of columns wanted in the export (if system_columns has values they are in the export even if not listed here).</p> <code>None</code> <code>header</code> <code>bool</code> <p>If True, include a header line with column names. Default True.</p> <code>True</code> <code>csv_writer_args</code> <p>All other keyword arguments are assumed to be CSV format params as per csv.writer() and are passed directly to the writer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of data rows exported (including header).</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.get_multi_doc","title":"get_multi_doc","text":"<pre><code>get_multi_doc(\n    lib_name: str,\n    path: str,\n    out_path: str,\n    glob: str = None,\n) -&gt; list[str]\n</code></pre> <p>Get multiple documents from SharePoint documentLibrary lib_name and path.</p> <p>Delegated user requires read access to the documentLibrary.  AzureAD application required Graph API access Sites.ReadWrite.All</p> <p>Parameters:</p> Name Type Description Default <code>lib_name</code> <code>str</code> <p>SharePoint documentLibrary name as found on the SharePoint site (existence is verified).</p> required <code>path</code> <code>str</code> <p>full documentLibrary path (filename included) to fetch. Must be abolute with leading /.</p> required <code>out_path</code> <code>str</code> <p>Name of directory to write files too</p> required <code>glob</code> <code>str</code> <p>The search filename glob pattern to match and download files. e.g. <code>*.csv</code> for all csv</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of filenames downloaded from SharePoint folder.</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>If the document doesn't exist or cannot be downloaded.</p> <code>ValueError</code> <p>If bad parameters.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.get_token","title":"get_token","text":"<pre><code>get_token(\n    tenant: str,\n    client_id: str,\n    client_secret: str,\n    user: str,\n    password: str,\n) -&gt; None\n</code></pre> <p>Get a OAUTH v2 Graph API Token (the client_id app) for delegated user.</p> <p>Parameters:</p> Name Type Description Default <code>tenant</code> <code>str</code> <p>Azure AD registered domain id</p> required <code>client_id</code> <code>str</code> <p>UUID of the Azure AD registered app under the registered domain (registration has Microsoft graph API credentials)</p> required <code>client_secret</code> <code>str</code> <p>Credentials of the Azure AD registered app.</p> required <code>user</code> <code>str</code> <p>Delegated user credentials to access graph API.</p> required <code>password</code> <code>str</code> <p>Delegated user credentials</p> required"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.graphapi_iter_find_id","title":"graphapi_iter_find_id","text":"<pre><code>graphapi_iter_find_id(\n    url: str, search: Iterable[tuple[str, str]], error: str\n) -&gt; str\n</code></pre> <p>Iterate over a MS Graph API list obtaining first id based on search tuple.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>start url to iterate over</p> required <code>search</code> <code>Iterable[tuple[str, str]]</code> <p>search criteria to find the first id of</p> required <code>error</code> <code>str</code> <p>error string to show on error</p> required <p>Returns:</p> Type Description <code>str</code> <p>id of first match</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>if id can't be found using error or when error calling url next chain</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.graphapi_session_req","title":"graphapi_session_req","text":"<pre><code>graphapi_session_req(\n    url: str,\n    method: str,\n    base_error: str,\n    upd_header: dict[str, Any] = None,\n    status_ok: list[int] = None,\n    data: Any = None,\n    dump_req: str = None,\n) -&gt; Any\n</code></pre> <p>Call into MS Graph API session with a http request given in method.</p> <p>Raise errors on error responses Handle all requests and throttle properly.  Handle for ProxyError as though that was graph API hard throttling dropping a connection.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>start url to iterate over</p> required <code>method</code> <code>str</code> <p>the HTTP method for this request</p> required <code>base_error</code> <code>str</code> <p>error string to show on error</p> required <code>upd_header</code> <code>dict[str, Any]</code> <p>request headers to update with (replacing existing session headers)</p> <code>None</code> <code>status_ok</code> <code>list[int]</code> <p>list of http status values that aren't errors</p> <code>None</code> <code>data</code> <code>Any</code> <p>data for the request if required</p> <code>None</code> <code>dump_req</code> <code>str</code> <p>file to dump the request_too (for debugging)</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>payload</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>on any http error</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.iter_drive_children","title":"iter_drive_children","text":"<pre><code>iter_drive_children(\n    url: str, error: str\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Return a JSON list of items.</p> <p>Iterate over this returning a limited useful set of attributes we want.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL request we will get</p> required <code>error</code> <code>str</code> <p>If request rsponse with error what error to raise</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of items with attributes we wanted.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.list_field_types","title":"list_field_types","text":"<pre><code>list_field_types(\n    url: str, base_error: str\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> <p>Get the field types of the specified SharePoint fields.</p> <p>Iterates over the expanded fields MS Graph API call returning type of field it is Raise errors on error responses.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>start url to iterate over</p> required <code>base_error</code> <code>str</code> <p>error string to show on error</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>iterated fields types</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>on any http error</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.list_lib","title":"list_lib","text":"<pre><code>list_lib(lib_name: str, path: str) -&gt; list[dict[str, Any]]\n</code></pre> <p>List the contents of a SharePoint documentLibrary.</p> <p>Delegated user requires read access to the documentLibrary. AzureAD application required Graph API access Sites.ReadWrite.All</p> <p>Parameters:</p> Name Type Description Default <code>lib_name</code> <code>str</code> <p>SharePoint documentLibrary name as found on the SharePoint site (existence is verified).</p> required <code>path</code> <code>str</code> <p>full documentLibrary path (filename included) to fetch. Must be abolute with leading /.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of entries.  TODO: It appears though that the first entry is the drive ID? Need to better explin the return info.</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>If the document doesn't exist or cannot be downloaded.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.list_lib_by_id","title":"list_lib_by_id","text":"<pre><code>list_lib_by_id(\n    drive_id: str, path_id: str\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>List a documentLibrary folder using a known drive_id and path_id.</p> <p>Parameters:</p> Name Type Description Default <code>drive_id</code> <code>str</code> <p>The already found drive_id of the SharePoint documentLibrary SharePoint site (existence is verified).</p> required <code>path_id</code> <code>str</code> <p>full documentLibrary path (filename included) to fetch. Must be abolute with leading /.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of all elements.</p> <p>Raises:</p> Type Description <code>SharePointError</code> <p>If error while listing.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.move_file","title":"move_file","text":"<pre><code>move_file(\n    drive_id: str, file_id: str, path_id: str\n) -&gt; None\n</code></pre> <p>Move documentLibrary (drive_id) file specified by file_id to folder path_id.</p> <p>Parameters:</p> Name Type Description Default <code>drive_id</code> <code>str</code> <p>The already found drive_id of the sharepoint documentLibrary SharePoint site (existence is verified).</p> required <code>file_id</code> <code>str</code> <p>The already known file_id of the file within the documentLibrary.</p> required <code>path_id</code> <code>str</code> <p>The already known path_id of a folder within the documentLibrary to move the file to.</p> required <p>Raises:</p> Type Description <code>SharePointError</code> <p>If error while moving</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.put_doc","title":"put_doc","text":"<pre><code>put_doc(\n    lib_name: str,\n    path: str,\n    src_file: str,\n    title: str = None,\n) -&gt; None\n</code></pre> <p>Put a document into SharePoint documentLibrary lib_name.</p> <p>Delegated user requires write access to the documentLibrary.  AzureAD application required Graph API access Sites.ReadWrite.All Uses am upload session so can handle files &gt; 4MB</p> <p>Parameters:</p> Name Type Description Default <code>lib_name</code> <code>str</code> <p>SharePoint documentLibrary name as found on the SharePoint site (existence is verified).</p> required <code>path</code> <code>str</code> <p>Full path (filename included) to put the file.</p> required <code>src_file</code> <code>str</code> <p>Name of source file to put.</p> required <code>title</code> <code>str</code> <p>The Title metadata values to set in SharePoint</p> <code>None</code> <p>Raises:</p> Type Description <code>SharePointError</code> <p>If the upload fails.</p> <code>ValueError</code> <p>If bad parameters.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.put_doc_no_upload_session","title":"put_doc_no_upload_session","text":"<pre><code>put_doc_no_upload_session(\n    lib_name: str,\n    path: str,\n    src_file: str,\n    title: str = None,\n) -&gt; None\n</code></pre> <p>Put a document into SharePoint documentLibrary lib_name.</p> <p>Delegated user requires write access to the documentLibrary.  AzureAD application required Graph API access <code>Sites.ReadWrite.All</code>.</p> <p>This uses a direct put to the sharepoint document. This is limited to documents &lt;=4MB in size only. Requires upload session to larger files</p> <p>Parameters:</p> Name Type Description Default <code>lib_name</code> <code>str</code> <p>SharePoint documentLibrary name as found on the SharePoint site (existence is verified).</p> required <code>path</code> <code>str</code> <p>Full path (filename included) to put the file.</p> required <code>src_file</code> <code>str</code> <p>Name of source file to put.</p> required <code>title</code> <code>str</code> <p>The Title metadata values to set in SharePoint</p> <code>None</code> <p>Raises:</p> Type Description <code>SharePointError</code> <p>If the upload fails.</p> <code>ValueError</code> <p>If bad parameters.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.put_list","title":"put_list","text":"<pre><code>put_list(\n    list_name: str,\n    src_file: str,\n    mode: str = \"append\",\n    error_missing: bool = False,\n    data_columns: str = None,\n    **csv_reader_args\n) -&gt; int\n</code></pre> <p>Put a document into SharePoint List list_name.</p> <p>Delegated user requires write access to the List.  AzureAD application required Graph API access Sites.ReadWrite.All</p> <p>Uses MS Graph API batch to put 20 items on the list at once.</p> <p>Parameters:</p> Name Type Description Default <code>list_name</code> <code>str</code> <p>SharePoint List name as found on the SharePoint site (existence is verified).</p> required <code>src_file</code> <code>str</code> <p>CSV src_file to put. Requirement is plain text csv source file.</p> required <code>mode</code> <code>str</code> <p>Update mode -- append or replace</p> <code>'append'</code> <code>error_missing</code> <code>bool</code> <p>If True produce error if column exists in data and isn't in the list.  Otherwise just display a warning.</p> <code>False</code> <code>data_columns</code> <code>str</code> <p>Comma seperated list of columns that are to be included in sharepoint list item update or create</p> <code>None</code> <code>csv_reader_args</code> <p>Additional params are passed to the CSV reader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of data rows uploaded.</p>"},{"location":"18-api.html#lava.lib.sharepoint.Sharepoint.refresh_token","title":"refresh_token","text":"<pre><code>refresh_token() -&gt; None\n</code></pre> <p>Refresh an OAUTH v2 Graph API Token.</p>"},{"location":"18-api.html#lava.lib.sharepoint.datetime_val","title":"datetime_val","text":"<pre><code>datetime_val(tv: str) -&gt; datetime.datetime\n</code></pre> <p>Set UTC for datetime without tzinfo.</p> <p>Uses dateutil parser.</p> <p>Parameters:</p> Name Type Description Default <code>tv</code> <code>str</code> <p>date</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>datetime parsed result</p>"},{"location":"18-api.html#lava.lib.sharepoint.datetime_with_utc_tz","title":"datetime_with_utc_tz","text":"<pre><code>datetime_with_utc_tz() -&gt; datetime.datetime\n</code></pre> <p>Return the UTC time datetime set timezone UTC.</p> <p>To convert to a particular TZ use r_date.astimezone(dateutil.tz.gettz(\"Australia/Melbourne\")) or as offset time r_date.astimezone(dateutil.tz.tzstr(\"GMT+11:00\", posix_offset=False))</p> <p>Returns:</p> Type Description <code>datetime</code> <p>Current date and time with UTC timezone</p>"},{"location":"18-api.html#lava.lib.sharepoint.dump_request","title":"dump_request","text":"<pre><code>dump_request(\n    suf: str, r: Response, req: Any = None\n) -&gt; None\n</code></pre> <p>For given requests response value dump the header and JSON payload to a file.</p> <p>For a given req python object dump to json file debug MS Graph API (REST API) purpose only.</p> <p>Parameters:</p> Name Type Description Default <code>suf</code> <code>str</code> <p>suffix of dump file that is written out.</p> required <code>r</code> <code>Response</code> <p>requests response object</p> required <code>req</code> <code>Any</code> <p>python payload that the requests payload</p> <code>None</code>"},{"location":"18-api.html#lava.lib.sharepoint.first_matching","title":"first_matching","text":"<pre><code>first_matching(\n    data: list, match: Iterable[tuple[str, str]]\n) -&gt; Any\n</code></pre> <p>From a tuple or list of dicts return first item that matches all (key, match_val) criteria.</p> <p>We require a TRUE match on all (key, match_val) pairs in match Return the first matching entry</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>Python list we are searching for the first match</p> required <code>match</code> <code>Iterable[tuple[str, str]]</code> <p>Iterable of ( (key, match_val), ... ) pairs where match_val is list like then where the object value at key matches one of match_val</p> required <p>Returns:</p> Type Description <code>T</code> <p>The located object or None if not found.</p>"},{"location":"18-api.html#lava.lib.sharepoint.key_val_match","title":"key_val_match","text":"<pre><code>key_val_match(\n    key: str, val: str | list[str], obj: Any\n) -&gt; bool\n</code></pre> <p>Return true if object at key matches val.</p> <p>We require a TRUE match on all (key, match_val) pairs in match</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>we match on the value of obj[key] key is . separated. key=<code>a.b.c</code> means we match on <code>obj['a']['b']['c]</code>.</p> required <code>val</code> <code>str | list[str]</code> <p>if plain string match directly. if list/tuple, match for any item of val</p> required <code>obj</code> <code>Any</code> <p>object we are matching value of obj[key]</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if object at key matches val.</p>"},{"location":"18-api.html#lava.lib.sharepoint.parse_http_retry_after","title":"parse_http_retry_after","text":"<pre><code>parse_http_retry_after(hd: dict) -&gt; int\n</code></pre> <p>Parse a HTTP Retry-After response header.</p> <p>Return number of seconds to wait. Default to 15 if Retry-Header doesn't exist or isn't integer number of seconds or (limited type) parsable date</p> <p>Parameters:</p> Name Type Description Default <code>hd</code> <code>dict</code> <p>Headers from a http response (requests version)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of seconds to wait</p>"},{"location":"18-api.html#lava.lib.slack","title":"lava.lib.slack","text":"<p>Provides lava based support for sending messages to Slack.</p> <p>It defines a handler class for Slack connectivity.</p>"},{"location":"18-api.html#lava.lib.slack.Slack","title":"Slack","text":"<pre><code>Slack(\n    conn_spec: dict[str, Any],\n    realm: str,\n    sender: str = None,\n    style: str = None,\n    colour: str = None,\n    preamble: str = None,\n    logger: Logger = None,\n)\n</code></pre> <p>Handler class for Slack webhook connections.</p> <p>Parameters:</p> Name Type Description Default <code>conn_spec</code> <code>dict[str, Any]</code> <p>A database connection specification.</p> required <code>realm</code> <code>str</code> <p>Lava realm.</p> required <code>sender</code> <code>str</code> <p>Default sender. If not specified, the <code>from</code> key in the conn_spec is used if present.</p> <code>None</code> <code>style</code> <code>str</code> <p>Default display style for the Slack message. Options are <code>block</code> (default), <code>attachment</code> and <code>plain</code>.</p> <code>None</code> <code>colour</code> <code>str</code> <p>Default colour for the sidebar for Slack messages sent using <code>attachment</code> style. This can be any hex colour code or one of the Slack special values <code>good</code>, <code>warning</code> or <code>danger</code>.</p> <code>None</code> <code>preamble</code> <code>str</code> <p>Default preamble at the start of the message. Useful values include things such as <code>&lt;!here&gt;</code> and <code>&lt;!channel&gt;</code> which will cause Slack to insert <code>@here</code> and <code>@channel</code> alert tags respectively.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>A logger. If not specified, use the root logger.</p> <code>None</code> <p>Create a Slack handler instance.</p>"},{"location":"18-api.html#lava.lib.slack.Slack.send","title":"send","text":"<pre><code>send(\n    message: str,\n    subject: str = None,\n    preamble: str = None,\n    sender: str = None,\n    style=None,\n    colour: str = None,\n) -&gt; None\n</code></pre> <p>Send a formatted message to a slack channel.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message body. Must not be empty.</p> required <code>subject</code> <code>str</code> <p>Message subject. Optional.</p> <code>None</code> <code>preamble</code> <code>str</code> <p>An optional preamble at the start of the message. Useful values include things such as &lt;!here&gt; and &lt;!channel&gt; which will cause Slack to insert @here and @channel alert tags respectively.</p> <code>None</code> <code>sender</code> <code>str</code> <p>Sender name.</p> <code>None</code> <code>style</code> <p>Display style for the Slack message. Options are <code>block</code> (default), <code>attachment</code> and <code>plain</code>.</p> <code>None</code> <code>colour</code> <code>str</code> <p>Colour for the sidebar for Slack messages sent using 'attachment' style. This can be any hex colour code or one of the Slack special values <code>good</code>, <code>warning</code> or <code>danger</code>.</p> <code>None</code>"},{"location":"18-api.html#lava.lib.slack.Slack.send_raw","title":"send_raw","text":"<pre><code>send_raw(slack_msg: dict[str, Any]) -&gt; None\n</code></pre> <p>Send a raw message to a slack channel.</p> <p>See https://api.slack.com/messaging/webhooks</p> <p>Parameters:</p> Name Type Description Default <code>slack_msg</code> <code>dict[str, Any]</code> <p>A Slack message payload. The message structure must conform to the format required by the Slack webhook API.</p> required"},{"location":"18-api.html#lava.lib.smb","title":"lava.lib.smb","text":"<p>SMB related utilities. Relies on pysmb and smbprotocol packages.</p> <p>See:     - pysmb: https://pysmb.readthedocs.io/en/latest/api/smb_SMBConnection.html     - smbprotocol: https://github.com/jborean93/smbprotocol</p>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection","title":"LavaSMBConnection","text":"<p>               Bases: <code>ABC</code></p> <p>A standard interface for SMB connection types.</p>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.connected","title":"connected  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>connected: bool\n</code></pre> <p>Whether the client has made a connection or not.</p>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.close","title":"close  <code>abstractmethod</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Terminate the SMB connection and release any sources held by the socket.</p>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.connect","title":"connect  <code>abstractmethod</code>","text":"<pre><code>connect(\n    ip: str = None, port: int = None, timeout: int = 60\n) -&gt; None\n</code></pre> <p>Connect to an SMB server.</p> <p>Parameters:</p> Name Type Description Default <code>ip</code> <code>str</code> <p>A IP address to use instead of the remote name.</p> <code>None</code> <code>port</code> <code>int</code> <p>A port to connect instead of the default (445).</p> <code>None</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>60</code>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.create_directory","title":"create_directory  <code>abstractmethod</code>","text":"<pre><code>create_directory(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; None\n</code></pre> <p>Create a new directory path on the service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the folder exists, an SMBOperationError will be raised.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.delete_directory","title":"delete_directory  <code>abstractmethod</code>","text":"<pre><code>delete_directory(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; None\n</code></pre> <p>Delete the empty folder at path on service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the dir on the remote server. If the dir does not exist or is not empty, an FileNotFoundError or an OSError is raised respectively.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.delete_files","title":"delete_files  <code>abstractmethod</code>","text":"<pre><code>delete_files(\n    service_name: str,\n    path_file_pattern: str,\n    delete_matching_folders: bool = False,\n    timeout: int = 30,\n) -&gt; None\n</code></pre> <p>Delete one or more regular files.</p> <p>It supports the use of wildcards in file names, allowing for deletion of multiple files, however these won't be in a single request as the smbprotocol library does have support yet.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>Contains the name of the shared folder.</p> required <code>path_file_pattern</code> <code>str</code> <p>The pathname of the files/subfolders to be deleted, relative to the service_name. Wildcards may be used in th filename component of the path.</p> required <code>delete_matching_folders</code> <code>bool</code> <p>If True, delete subfolders that match the path pattern.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.echo","title":"echo  <code>abstractmethod</code>","text":"<pre><code>echo(timeout: int = 10) -&gt; NTStatus\n</code></pre> <p>Send echo request to SMB server.</p> <p>Can be used to actively test connectivity to the SMB server.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>The timeout in seconds to wait for the Echo Response, default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>NTStatus</code> <p>An NTStatus.</p>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.get_attributes","title":"get_attributes  <code>abstractmethod</code>","text":"<pre><code>get_attributes(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; SMBFile\n</code></pre> <p>Retrieve information about the file at path on the service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for reading, an SMBOperationError will be raised.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>SMBFile</code> <p>A SMBFile instance containing the attributes of the file.</p>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.list_path","title":"list_path  <code>abstractmethod</code>","text":"<pre><code>list_path(\n    service_name: str,\n    path: str,\n    pattern: str = \"*\",\n    timeout: int = 30,\n    **kwargs\n) -&gt; list[SMBFile]\n</code></pre> <p>Retrieve a directory listing of files/folders at path.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path relative to the service_name to list subfolders/files.</p> required <code>pattern</code> <code>str</code> <p>The filter to apply to the results before returning to the client.</p> <code>'*'</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>list[SMBFile]</code> <p>A list of SMBFile instances.</p>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.rename","title":"rename  <code>abstractmethod</code>","text":"<pre><code>rename(\n    service_name: str,\n    old_path: str,\n    new_path: str,\n    timeout: int = 30,\n) -&gt; None\n</code></pre> <p>Rename a file or folder at old_path to new_path shared at service_name.</p> <p>Note: that this method cannot be used to rename file/folder across different shared folders.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>old_path</code> <code>str</code> <p>The path of the folder/file to rename.</p> required <code>new_path</code> <code>str</code> <p>The new path of the file or folder.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.retrieve_file","title":"retrieve_file  <code>abstractmethod</code>","text":"<pre><code>retrieve_file(\n    service_name: str,\n    path: str,\n    file_obj: BinaryIO,\n    timeout: int = 30,\n    **kwargs\n) -&gt; tuple[int, int]\n</code></pre> <p>Retrieve the contents of the file from SMB server and write contents to the file_obj.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for reading, an SMBOperationError will be raised.</p> required <code>file_obj</code> <code>BinaryIO</code> <p>A file-like object that has a write method. Data will be written continuously to file_obj until EOF is received from the remote service. In Python3, this file-like object must have a write method which accepts a bytes parameter.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>A 2-element tuple of file attributes (a bitwise-OR of SMBFileAttributes bits) of the file, and the number of bytes written to file_obj.</p>"},{"location":"18-api.html#lava.lib.smb.LavaSMBConnection.store_file","title":"store_file  <code>abstractmethod</code>","text":"<pre><code>store_file(\n    service_name: str,\n    path: str,\n    file_obj: BinaryIO,\n    timeout: int = 30,\n    **kwargs\n) -&gt; int\n</code></pre> <p>Store the contents of the file_obj at path on the service_name.</p> <p>If the file already exists on the remote server, it will be truncated and overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for writing, an SMBOperationError will be raised.</p> required <code>file_obj</code> <code>BinaryIO</code> <p>A file-like object that has a write method. Data will be written continuously to file_obj until EOF is received from the remote service. In Python3, this file-like object must have a write method which accepts a bytes parameter.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of bytes uploaded.</p>"},{"location":"18-api.html#lava.lib.smb.MSCIFSFileAttributes","title":"MSCIFSFileAttributes","text":"<p>               Bases: <code>SMBFileAttributes</code></p> <p>File attributes specific to the SMB servers implementing the MS-CIFS protocol.</p> <p>[MS-CIFS File Attributes] https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-cifs/6008aa8f-d2d8-4366-b775-b81aece05bb1</p>"},{"location":"18-api.html#lava.lib.smb.MSFSCCFileAttributes","title":"MSFSCCFileAttributes","text":"<p>               Bases: <code>SMBFileAttributes</code></p> <p>File attributes specific to the SMB servers implementing the MS-FSCC standard.</p> <p>This standard is used by the MS-SMB2 protocol.</p> <p>[MS-FSCC File Attributes] https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-fscc/ca28ec38-f155-4768-81d6-4bfeb8586fc9</p>"},{"location":"18-api.html#lava.lib.smb.NTStatus","title":"NTStatus","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>NTStatus extension class that allows for reverse lookup.</p> <p>This does not contain every NTStatus code but contains all common and important ones. List comes from smbprotocol: https://github.com/jborean93/smbprotocol/blob/942b005fcf7462cf0c5fed25d15f0594bfc6bd54/src/smbprotocol/header.py#L38</p>"},{"location":"18-api.html#lava.lib.smb.NTStatus.lookup","title":"lookup  <code>classmethod</code>","text":"<pre><code>lookup(ntstatus_code: int) -&gt; NTStatus\n</code></pre> <p>Reverse lookup NTStatus by status code.</p>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection","title":"PySMBConnection","text":"<pre><code>PySMBConnection(\n    username: str,\n    password: str,\n    my_name: str,\n    remote_name: str,\n    domain: str = \"\",\n    use_ntlm_v2: bool = True,\n    sign_options: SMBSigningOptions = SMBSigningOptions.SIGN_WHEN_REQUIRED,\n    is_direct_tcp: bool = True,\n)\n</code></pre> <p>               Bases: <code>LavaSMBConnection</code></p> <p>SMB Lava Connection class using the pysmb module.</p> <p>Create a new PySMBConnection instance.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>Username credential for SMB server login.</p> required <code>password</code> <code>str</code> <p>Password credential for SMB server login.</p> required <code>my_name</code> <code>str</code> <p>A friendly name to identity where the connection originated from. Must not contain spaces any characters in <code>\\\\/:*?\";|+</code>.</p> required <code>remote_name</code> <code>str</code> <p>The remote name or IP of the server.</p> required <code>domain</code> <code>str</code> <p>Domain for connecting to SMB servers.</p> <code>''</code> <code>use_ntlm_v2</code> <code>bool</code> <p>Whether to use NTMLv2 to connect, otherwise negotiate. Default is True.</p> <code>True</code> <code>sign_options</code> <code>SMBSigningOptions</code> <p>Whether SMB messages will be signed. Default is SIGN_WHEN_REQUIRED.</p> <code>SIGN_WHEN_REQUIRED</code> <code>is_direct_tcp</code> <code>bool</code> <p>Connect over TCP/IP port 445, else via NetBIOS over TCP/IP port 139.</p> <code>True</code>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.connected","title":"connected  <code>property</code>","text":"<pre><code>connected: bool\n</code></pre> <p>Whether the connection has made a connection or not.</p>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Terminate the SMB connection and release any sources held by the socket.</p>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.connect","title":"connect","text":"<pre><code>connect(\n    ip: str = None, port: int = None, timeout: int = 60\n) -&gt; None\n</code></pre> <p>Connect to SMB server.</p> <p>Parameters:</p> Name Type Description Default <code>ip</code> <code>str</code> <p>A IP address to use instead of the remote name.</p> <code>None</code> <code>port</code> <code>int</code> <p>A port to connect instead of the default (445).</p> <code>None</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>60</code>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.create_directory","title":"create_directory","text":"<pre><code>create_directory(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; None\n</code></pre> <p>Create a new directory path on the service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the folder exists, an SMBOperationError will be raised.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.delete_directory","title":"delete_directory","text":"<pre><code>delete_directory(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; None\n</code></pre> <p>Delete the empty folder at path on service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the dir on the remote server. If the dir does not exist or is not empty, an FileNotFoundError or an OSError is raised respectively.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.delete_files","title":"delete_files","text":"<pre><code>delete_files(\n    service_name: str,\n    path_file_pattern: str,\n    delete_matching_folders: bool = False,\n    timeout: int = 30,\n) -&gt; None\n</code></pre> <p>Delete one or more regular files.</p> <p>It supports the use of wildcards in file names, allowing for deletion of multiple files, however these won't be in a single request as the smbprotocol library does have support yet.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>Contains the name of the shared folder.</p> required <code>path_file_pattern</code> <code>str</code> <p>The pathname of the files/subfolders to be deleted, relative to the service_name. Wildcards may be used in th filename component of the path.</p> required <code>delete_matching_folders</code> <code>bool</code> <p>If True, delete subfolders that match the path pattern.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.echo","title":"echo","text":"<pre><code>echo(timeout: int = 10) -&gt; NTStatus\n</code></pre> <p>Send echo request to SMB server.</p> <p>Can be used to actively test connectivity to the SMB server.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>The timeout in seconds to wait for the Echo Response, Default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>NTStatus</code> <p>An NTStatus.</p>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.get_attributes","title":"get_attributes","text":"<pre><code>get_attributes(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; SMBFile\n</code></pre> <p>Retrieve information about the file at path on the service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for reading, an SMBOperationError will be raised.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>SMBFile</code> <p>A SMBFile instance containing the attributes of the file.</p>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.list_path","title":"list_path","text":"<pre><code>list_path(\n    service_name: str,\n    path: str,\n    pattern: str = \"*\",\n    timeout: int = 30,\n    search: int = None,\n) -&gt; list[SMBFile]\n</code></pre> <p>Retrieve a directory listing of files/folders at path.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path relative to the service_name to list subfolders/files.</p> required <code>pattern</code> <code>str</code> <p>The filter to apply to the results before returning to the client.</p> <code>'*'</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <code>search</code> <code>int</code> <p>Allows a search=0xYYY to list files with specific SMBFile attributes.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[SMBFile]</code> <p>A list of SMBFile instances.</p>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.rename","title":"rename","text":"<pre><code>rename(\n    service_name: str,\n    old_path: str,\n    new_path: str,\n    timeout: int = 30,\n) -&gt; None\n</code></pre> <p>Rename a file or folder at old_path to new_path shared at service_name.</p> <p>Note: that this method cannot be used to rename file/folder across different shared folders.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>old_path</code> <code>str</code> <p>The path of the folder/file to rename.</p> required <code>new_path</code> <code>str</code> <p>The new path of the file or folder.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.retrieve_file","title":"retrieve_file","text":"<pre><code>retrieve_file(\n    service_name: str,\n    path: str,\n    file_obj: BinaryIO,\n    timeout: int = 30,\n    **kwargs\n) -&gt; tuple[int, int]\n</code></pre> <p>Retrieve the contents of the file from SMB server and write contents to the file_obj.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for reading, an SMBOperationError will be raised.</p> required <code>file_obj</code> <code>BinaryIO</code> <p>A file-like object that has a write method. Data will be written continuously to file_obj until EOF is received from the remote service. In Python3, this file-like object must have a write method which accepts a bytes parameter.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>A 2-element tuple of file attributes (a bitwise-OR of SMB_FILE_ATTRIBUTE_xxx bits) of the file, and the number of bytes written to file_obj.</p>"},{"location":"18-api.html#lava.lib.smb.PySMBConnection.store_file","title":"store_file","text":"<pre><code>store_file(\n    service_name: str,\n    path: str,\n    file_obj: BinaryIO,\n    timeout: int = 30,\n    **kwargs\n) -&gt; int\n</code></pre> <p>Store the contents of the file_obj at path on the service_name.</p> <p>If the file already exists on the remote server, it will be truncated and overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for reading, an SMBOperationError will be raised.</p> required <code>file_obj</code> <code>BinaryIO</code> <p>A file-like object that has a write method. Data will be written continuously to file_obj until EOF is received from the remote service. In Python3, this file-like object must have a write method which accepts a bytes parameter.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <code>kwargs</code> <p>Allows for pysmb args 'show_progress' (bool) and 'tqdm_kwargs' (dict) See docs:</p> <code>{}</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of bytes uploaded.</p>"},{"location":"18-api.html#lava.lib.smb.SMBBaseError","title":"SMBBaseError","text":"<pre><code>SMBBaseError(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>A base exception class for Lava SMB exceptions.</p> <p>Create a SMBBaseException.</p>"},{"location":"18-api.html#lava.lib.smb.SMBConnectionError","title":"SMBConnectionError","text":"<pre><code>SMBConnectionError(*args, **kwargs)\n</code></pre> <p>               Bases: <code>SMBBaseError</code></p> <p>An SMB connection error class.</p> <p>Create a SMBConnectionError.</p>"},{"location":"18-api.html#lava.lib.smb.SMBFile","title":"SMBFile","text":"<pre><code>SMBFile(\n    create_time: float,\n    last_access_time: float,\n    last_write_time: float,\n    last_attr_change_time: float,\n    file_size: int,\n    alloc_size: int,\n    file_attributes: bin,\n    filename: str,\n    attributes_class: type[SMBFileAttributes],\n    short_name: str = None,\n    file_id: int = None,\n)\n</code></pre> <p>Contains information about a file or folder on a shared SMB device.</p> <p>Create an SMBFile object.</p> <p>Parameters:</p> Name Type Description Default <code>create_time</code> <code>float</code> <p>timestamp in epoch seconds for when the file was created on the server</p> required <code>last_access_time</code> <code>float</code> <p>timestamp in epoch seconds for when the file was last accessed</p> required <code>last_write_time</code> <code>float</code> <p>timestamp in epoch seconds for when the file was last modified</p> required <code>last_attr_change_time</code> <code>float</code> <p>timestamp in epoch seconds for when a files attributes changed</p> required <code>file_size</code> <code>int</code> <p>the size of the number in number of bytes</p> required <code>alloc_size</code> <code>int</code> <p>total number of bytes allocated to store the file</p> required <code>short_name</code> <code>str</code> <p>a unicode string containing the short file name (usually in 8.3 notation)</p> <code>None</code> <code>file_attributes</code> <code>bin</code> <p>a bit representation of file attributes</p> required <code>filename</code> <code>str</code> <p>a unicode string containing the file name</p> required <code>file_id</code> <code>int</code> <p>an integer value representing the file reference number for the file</p> <code>None</code>"},{"location":"18-api.html#lava.lib.smb.SMBFile.is_directory","title":"is_directory  <code>property</code>","text":"<pre><code>is_directory: bool\n</code></pre> <p>Determines whether the file is a directory.</p>"},{"location":"18-api.html#lava.lib.smb.SMBFile.is_normal","title":"is_normal  <code>property</code>","text":"<pre><code>is_normal: bool\n</code></pre> <p>Determines if the file is a normal file.</p> <p>Following pysmb definition as a file that is not read-only, archived, hidden, system or a directory. It ignores other attributes like compression, indexed, sparse, temporary and encryption.</p>"},{"location":"18-api.html#lava.lib.smb.SMBFile.is_read_only","title":"is_read_only  <code>property</code>","text":"<pre><code>is_read_only: bool\n</code></pre> <p>Determines whether the file is read only.</p>"},{"location":"18-api.html#lava.lib.smb.SMBFile.get_readable_attributes","title":"get_readable_attributes","text":"<pre><code>get_readable_attributes() -&gt; list[str]\n</code></pre> <p>Get a list of human readable attributes.</p>"},{"location":"18-api.html#lava.lib.smb.SMBFile.has_attribute","title":"has_attribute","text":"<pre><code>has_attribute(attribute: SMBFileAttributes) -&gt; bool\n</code></pre> <p>Determine whether a file or folder has a specific file attribute.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>SMBFileAttributes</code> <p>the SMBCommonFileAttribute to check for</p> required <p>Returns:</p> Type Description <code>bool</code> <p>a boolean which represents wheter the file or folder has the attribute</p>"},{"location":"18-api.html#lava.lib.smb.SMBFileAttributes","title":"SMBFileAttributes","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>File attributes enum that holds bitwise attribute definitions.</p>"},{"location":"18-api.html#lava.lib.smb.SMBOperationError","title":"SMBOperationError","text":"<pre><code>SMBOperationError(\n    *args, ntstatus: int, description: str, **kwargs\n)\n</code></pre> <p>               Bases: <code>SMBBaseError</code></p> <p>An SMB operation error class.</p> <p>Create a SMBOperationError.</p> <p>Parameters:</p> Name Type Description Default <code>ntstatus</code> <code>int</code> <p>the NTStatus code sent back from the server</p> required <code>description</code> <code>str</code> <p>a description of the error</p> required"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection","title":"SMBProtocolConnection","text":"<pre><code>SMBProtocolConnection(\n    username: str,\n    password: str,\n    my_name: str,\n    remote_name: str,\n    port: int = None,\n    domain: str = \"\",\n    use_ntlm_v2: bool = True,\n    sign_options: SMBSigningOptions = SMBSigningOptions.SIGN_WHEN_REQUIRED,\n    is_direct_tcp: bool = True,\n    encrypt: bool = True,\n)\n</code></pre> <p>               Bases: <code>LavaSMBConnection</code></p> <p>SMB Lava Connection class using the smbprotocol module.</p> <p>Create a new SMBProtocolConnection instance.</p> <p>WARNING: Connecting with a domain may not be thread safe due to Singleton nature of smbclient.ClientConfig. If creating a connection with a domain in a multithreaded environment, ensure that proper thread safety measures are applied around the creation of class instances.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>Username credential for SMB server login.</p> required <code>password</code> <code>str</code> <p>Password credential for SMB server login.</p> required <code>my_name</code> <code>str</code> <p>A friendly name to identity where the connection originated from. Must not contain spaces any characters in <code>\\\\/:*?\";|+</code>.</p> required <code>remote_name</code> <code>str</code> <p>The remote name or IP of the server.</p> required <code>port</code> <code>int</code> <p>The port of the server, default uses is_direct_tcp to determine port.</p> <code>None</code> <code>domain</code> <code>str</code> <p>Domain for connecting to SMB servers via DFS. Connects direct if blank.</p> <code>''</code> <code>use_ntlm_v2</code> <code>bool</code> <p>Whether to use NTMLv2 to connect, otherwise negotiate. Default is True.</p> <code>True</code> <code>sign_options</code> <code>SMBSigningOptions</code> <p>Whether SMB messages will be signed. Default is SIGN_WHEN_REQUIRED.</p> <code>SIGN_WHEN_REQUIRED</code> <code>is_direct_tcp</code> <code>bool</code> <p>Connect over TCP/IP port 445, else via NetBIOS over TCP/IP port 139.</p> <code>True</code> <code>encrypt</code> <code>bool</code> <p>Whether to force encryption to remote server. Default is True.</p> <code>True</code>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.connected","title":"connected  <code>property</code>","text":"<pre><code>connected: bool\n</code></pre> <p>Whether the connection has made a connection or not.</p>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Terminate the SMB connection and release any sources held by the socket.</p>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.connect","title":"connect","text":"<pre><code>connect(\n    ip: str = None, port: int = None, timeout: int = 60\n) -&gt; None\n</code></pre> <p>Connect to SMB server.</p> <p>Parameters:</p> Name Type Description Default <code>ip</code> <code>str</code> <p>A IP address to use instead of the remote name.</p> <code>None</code> <code>port</code> <code>int</code> <p>A port to connect instead of the default (445).</p> <code>None</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>60</code>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.create_directory","title":"create_directory","text":"<pre><code>create_directory(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; None\n</code></pre> <p>Create a new directory path on the service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the folder exists, an SMBOperationError will be raised.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.delete_directory","title":"delete_directory","text":"<pre><code>delete_directory(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; None\n</code></pre> <p>Delete the empty folder at path on service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the dir on the remote server. If the dir does not exist or is not empty, an FileNotFoundError or an OSError is raised respectively.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.delete_files","title":"delete_files","text":"<pre><code>delete_files(\n    service_name: str,\n    path_file_pattern: str,\n    delete_matching_folders: bool = False,\n    timeout: int = 30,\n) -&gt; None\n</code></pre> <p>Delete one or more regular files.</p> <p>It supports the use of wildcards in file names, allowing for deletion of multiple files, however these won't be in a single request as the smbprotocol library does have support yet.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>Contains the name of the shared folder.</p> required <code>path_file_pattern</code> <code>str</code> <p>The pathname of the files/subfolders to be deleted, relative to the service_name. Wildcards may be used in th filename component of the path.</p> required <code>delete_matching_folders</code> <code>bool</code> <p>If True, delete subfolders that match the path pattern.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.echo","title":"echo","text":"<pre><code>echo(timeout: int = 10) -&gt; NTStatus\n</code></pre> <p>Send echo request to SMB server.</p> <p>Can be used to actively test connectivity to the SMB server.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>The timeout in seconds to wait for the Echo Response, Default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>NTStatus</code> <p>An NTStatus.</p>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.get_attributes","title":"get_attributes","text":"<pre><code>get_attributes(\n    service_name: str, path: str, timeout: int = 30\n) -&gt; SMBFile\n</code></pre> <p>Retrieve information about the file at path on the service_name.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for reading, an SMBOperationError will be raised.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>SMBFile</code> <p>A SMBFile instance containing the attributes of the file.</p>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.list_path","title":"list_path","text":"<pre><code>list_path(\n    service_name: str,\n    path: str,\n    pattern: str = \"*\",\n    timeout: int = 30,\n    search: int = None,\n) -&gt; list[SMBFile]\n</code></pre> <p>Retrieve a directory listing of files/folders at path.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path relative to the service_name to list subfolders/files.</p> required <code>pattern</code> <code>str</code> <p>The filter to apply to the results before returning to the client.</p> <code>'*'</code> <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <code>search</code> <code>int</code> <p>Allows a search=0xYYY to list files with specific SMBFile attributes.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[SMBFile]</code> <p>A list of SMBFile instances.</p>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.rename","title":"rename","text":"<pre><code>rename(\n    service_name: str,\n    old_path: str,\n    new_path: str,\n    timeout: int = 30,\n) -&gt; None\n</code></pre> <p>Rename a file or folder at old_path to new_path shared at service_name.</p> <p>Note: that this method cannot be used to rename file/folder across different shared folders.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>old_path</code> <code>str</code> <p>The path of the folder/file to rename.</p> required <code>new_path</code> <code>str</code> <p>The new path of the file or folder.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.retrieve_file","title":"retrieve_file","text":"<pre><code>retrieve_file(\n    service_name: str,\n    path: str,\n    file_obj: BinaryIO,\n    timeout: int = 30,\n    **kwargs\n) -&gt; tuple[int, int]\n</code></pre> <p>Retrieve the contents of the file from SMB server and write contents to the file_obj.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for reading, an SMBOperationError will be raised.</p> required <code>file_obj</code> <code>BinaryIO</code> <p>A file-like object that has a write method. Data will be written continuously to file_obj until EOF is received from the remote service. In Python3, this file-like object must have a write method which accepts a bytes parameter.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>A 2-element tuple of file attributes (a bitwise-OR of SMBFileAttributes bits) of the file, and the number of bytes written to file_obj.</p>"},{"location":"18-api.html#lava.lib.smb.SMBProtocolConnection.store_file","title":"store_file","text":"<pre><code>store_file(\n    service_name: str,\n    path: str,\n    file_obj: BinaryIO,\n    timeout: int = 30,\n    **kwargs\n) -&gt; int\n</code></pre> <p>Store the contents of the file_obj at path on the service_name.</p> <p>If the file already exists on the remote server, it will be truncated and overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the shared folder for the path.</p> required <code>path</code> <code>str</code> <p>Path of the file on the remote server. If the file cannot be opened for reading, an SMBOperationError will be raised.</p> required <code>file_obj</code> <code>BinaryIO</code> <p>A file-like object that has a write method. Data will be written continuously to file_obj until EOF is received from the remote service. In Python3, this file-like object must have a write method which accepts a bytes parameter.</p> required <code>timeout</code> <code>int</code> <p>A timeout for the request. Default is 30s.</p> <code>30</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of bytes uploaded.</p>"},{"location":"18-api.html#lava.lib.smb.SMBSigningOptions","title":"SMBSigningOptions","text":"<p>               Bases: <code>Enum</code></p> <p>SMB Signing Options.</p>"},{"location":"18-api.html#lava.lib.smb.SMBTimeoutError","title":"SMBTimeoutError","text":"<pre><code>SMBTimeoutError(*args, **kwargs)\n</code></pre> <p>               Bases: <code>SMBBaseError</code></p> <p>An SMB timeout exception class.</p> <p>Create a SMBTimeoutError.</p>"},{"location":"18-api.html#lava.lib.smb.smb_dir_exists","title":"smb_dir_exists","text":"<pre><code>smb_dir_exists(\n    conn: LavaSMBConnection, share_name: str, path: str\n) -&gt; bool\n</code></pre> <p>Check that the specified directory exists on the given SMB file share.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>LavaSMBConnection</code> <p>An SMB connection.</p> required <code>share_name</code> <code>str</code> <p>The SMB share name.</p> required <code>path</code> <code>str</code> <p>Target directory.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if it exists, False otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the target exists but is not a directory.</p>"},{"location":"18-api.html#lava.lib.smb.smb_mkdirs","title":"smb_mkdirs","text":"<pre><code>smb_mkdirs(\n    conn: LavaSMBConnection, share_name: str, path: str\n) -&gt; None\n</code></pre> <p>Create a directory on an SMB file share if it doesn't already exist.</p> <p>All necessary parent directories will also be created.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>LavaSMBConnection</code> <p>An SMB connection.</p> required <code>share_name</code> <code>str</code> <p>The SMB share name.</p> required <code>path</code> <code>str</code> <p>Target directory.</p> required"},{"location":"18-api.html#lava.lib.state","title":"lava.lib.state","text":"<p>Lava state manager API.</p>"},{"location":"18-api.html#lava.lib.state.LavaStateItem","title":"LavaStateItem","text":"<pre><code>LavaStateItem(\n    state_id: str,\n    realm: str,\n    value: Any,\n    state_type: str = None,\n    publisher: str = None,\n    ttl: str | int | float = None,\n    aws_session: Session = None,\n    **kwargs\n)\n</code></pre> <p>Base class for different state item types.</p> <p>Use the factory methods <code>new()</code>/ <code>get()</code> rather than the constructor.</p> <p>Parameters:</p> Name Type Description Default <code>state_id</code> <code>str</code> <p>State ID.</p> required <code>realm</code> <code>str</code> <p>Lava realm.</p> required <code>value</code> <code>Any</code> <p>State value.</p> required <code>state_type</code> <code>str</code> <p>State storage type (e.g. <code>json</code>, <code>raw</code>, <code>secure</code>).</p> <code>None</code> <code>publisher</code> <code>str</code> <p>An arbitrary label for the identity of the state item creator. Lava itself doesn't use this.</p> <code>None</code> <code>ttl</code> <code>str | int | float</code> <p>Time to live for the state item. Can be a value in seconds or a duration (e.g. <code>2h</code>). If greater than the maximum specified for the realm, it will be silently reduced to that value.</p> <code>None</code> <code>aws_session</code> <code>Session</code> <p>A boto3 session. One is created if not specified.</p> <code>None</code> <code>kwargs</code> <p>Vestigial. This is not the parameter you're looking for.</p> <code>{}</code> <p>Use the factory methods <code>new()</code>/ <code>get()</code> rather than the constructor.</p>"},{"location":"18-api.html#lava.lib.state.LavaStateItem.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(\n    state_id: str, realm: str, aws_session: Session = None\n) -&gt; LavaStateItemType\n</code></pre> <p>Retrieve an existing state item from DynamoDB.</p> <p>Parameters:</p> Name Type Description Default <code>state_id</code> <code>str</code> <p>State ID.</p> required <code>realm</code> <code>str</code> <p>Lava realm.</p> required <code>aws_session</code> <code>Session</code> <p>A boto3 session. One is created if not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>LavaStateItemType</code> <p>A state handler for the specified state type with the value loaded.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the state item doesn't exist.</p> <code>LavaError</code> <p>For other errors.</p>"},{"location":"18-api.html#lava.lib.state.LavaStateItem.new","title":"new  <code>classmethod</code>","text":"<pre><code>new(state_type=None, *args, **kwargs) -&gt; LavaStateItemType\n</code></pre> <p>Create a new state item.</p> <p>Parameters:</p> Name Type Description Default <code>state_type</code> <p>The state type.</p> <code>None</code> <code>args</code> <p>Passed to the constructor.</p> <code>()</code> <code>kwargs</code> <p>Passed to the constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LavaStateItemType</code> <p>A state handler for the specified state type.</p>"},{"location":"18-api.html#lava.lib.state.LavaStateItem.put","title":"put","text":"<pre><code>put()\n</code></pre> <p>Encode the value and put it in DynamoDB.</p>"},{"location":"18-api.html#lava.lib.state.LavaStateJson","title":"LavaStateJson","text":"<pre><code>LavaStateJson(\n    state_id: str,\n    realm: str,\n    value: Any,\n    state_type: str = None,\n    publisher: str = None,\n    ttl: str | int | float = None,\n    aws_session: Session = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LavaStateItem</code></p> <p>State entry with body as JSON encoded string (don't use for sensitive data).</p>"},{"location":"18-api.html#lava.lib.state.LavaStateRaw","title":"LavaStateRaw","text":"<pre><code>LavaStateRaw(\n    state_id: str,\n    realm: str,\n    value: Any,\n    state_type: str = None,\n    publisher: str = None,\n    ttl: str | int | float = None,\n    aws_session: Session = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LavaStateItem</code></p> <p>State entry with no transformation (don't use for sensitive data).</p>"},{"location":"18-api.html#lava.lib.state.LavaStateSecure","title":"LavaStateSecure","text":"<pre><code>LavaStateSecure(*args, kms_key: str = None, **kwargs)\n</code></pre> <p>               Bases: <code>LavaStateItem</code></p> <p>State entry with body as encrypted JSON encoded string.</p> <p>Parameter are as for the superclass with the addition of kms_key</p> <p>Parameters:</p> Name Type Description Default <code>kms_key</code> <code>str</code> <p>ARN or alias (as <code>alias/...</code>) of KMS key to encrypt the value. Defaults to the realm system key.</p> <code>None</code> <p>As for super but with kms_key.</p>"},{"location":"18-api.html#lava.lib.state.state","title":"state","text":"<pre><code>state(*args: str) -&gt; Callable\n</code></pre> <p>Register handler classes for different state types.</p> <p>Usage:</p> <pre><code>@state_type(type1, ...)\na_class(...)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>str</code> <p>A list of state types that the decorated class handles.</p> <code>()</code>"},{"location":"18-api.html#lava.lib.state.state_types","title":"state_types","text":"<pre><code>state_types() -&gt; list[str]\n</code></pre> <p>Return a list of available state item types.</p>"},{"location":"18-api.html#lava.version","title":"lava.version","text":"<p>Lava version.</p> <p>This is used in a number of places (including docker builds) so don't forget to update.</p> <p>This file can be run directly to print version info to stdout.</p> <p>Info</p> <p>As of v8.2, lava has changed from semantic versioning to PEP440 versioning. You would have to be doing something pretty unusual to notice the difference. The change was made to simplify working with PyPI. The semantic versioning support code has been left in, just in case, but lava itself no longer uses it.</p>"},{"location":"18-api.html#lava.version.SemanticVersion","title":"lava.version.SemanticVersion","text":"<pre><code>SemanticVersion(semver: str)\n</code></pre> <p>Model semantic versions.</p> <p>Parameters:</p> Name Type Description Default <code>semver</code> <code>str</code> <p>See https://semver.org</p> required <p>Create.</p>"},{"location":"18-api.html#lava.version.version","title":"lava.version.version","text":"<pre><code>version() -&gt; tuple[str, str]\n</code></pre> <p>Get the lava version number and name.</p> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>A tuple (version number, version name)</p>"},{"location":"19-cloudformation-templates.html","title":"CloudFormation Templates","text":"<p>Note</p> <p>Pre-built versions of the CloudFormation templates are provided as part of a release on GitHub.</p> <p>See also Building the CloudFormation Templates.</p>"},{"location":"19-cloudformation-templates.html#lava-commoncfnjson","title":"lava-common.cfn.json","text":"<p>This template builds components that are shared across all lava realms. There should be a very limited number of these.</p>"},{"location":"19-cloudformation-templates.html#parameters","title":"Parameters","text":"Parameter Type Description Version String Lava version (read only)."},{"location":"19-cloudformation-templates.html#outputs","title":"Outputs","text":"Id Export as Description Version <code>\"lava:version\"</code> Lava common stack version"},{"location":"19-cloudformation-templates.html#resources","title":"Resources","text":"<ul> <li>Logs Resources</li> <li>SSM Resources</li> </ul>"},{"location":"19-cloudformation-templates.html#logs-resources","title":"Logs Resources","text":"Id Type Description logsPolicyToLogEvents ResourcePolicy Allow EventBridge rules to write to log group aws/events/lava. This is generally not needed as EventBridge will have already added a broader permission but just in case."},{"location":"19-cloudformation-templates.html#ssm-resources","title":"SSM Resources","text":"Id Type Description ssmDocumentLavaReboot Document SSM command document to do controlled reboot on a lava instance. ssmDocumentLavaStop Document SSM command document to stop lava daemons on a worker instance ssmDocumentSecUpdate Document SSM command document to check if security updates are available for a lava worker instance and install and reboot if there are."},{"location":"19-cloudformation-templates.html#resource-details","title":"Resource Details","text":""},{"location":"19-cloudformation-templates.html#logspolicytologevents","title":"logsPolicyToLogEvents","text":"DetailsSource Property Value Type AWS::Logs::ResourcePolicy Group Logs Resources (Logs) Description Allow EventBridge rules to write to log group aws/events/lava. This is generally not needed as EventBridge will have already added a broader permission but just in case. <pre><code>{\n    \"Properties\": {\n        \"PolicyDocument\": {\n            \"Fn::Sub\": \"{\\\"Statement\\\": [{\\\"Action\\\": [\\\"logs:CreateLogStream\\\", \\\"logs:PutLogEvents\\\"], \\\"Effect\\\": \\\"Allow\\\", \\\"Principal\\\": {\\\"Service\\\": [\\\"delivery.logs.amazonaws.com\\\", \\\"events.amazonaws.com\\\"]}, \\\"Resource\\\": \\\"arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/events/lava:*\\\", \\\"Sid\\\": \\\"TrustEventsToStoreLavaLogEvent\\\"}], \\\"Version\\\": \\\"2012-10-17\\\"}\"\n        },\n        \"PolicyName\": \"TrustEventsToStoreLavaLogEvents\"\n    },\n    \"Type\": \"AWS::Logs::ResourcePolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#ssmdocumentlavareboot","title":"ssmDocumentLavaReboot","text":"DetailsSource Property Value Type AWS::SSM::Document Group SSM Resources (SSM) Description SSM command document to do controlled reboot on a lava instance. <pre><code>{\n    \"Properties\": {\n        \"Content\": {\n            \"description\": \"Controlled reboot of a lava instance (includes security updates).\",\n            \"mainSteps\": [\n                {\n                    \"action\": \"aws:runShellScript\",\n                    \"inputs\": {\n                        \"runCommand\": [\n                            \"LOG='logger -s -p local0.info -t lava-reboot'\",\n                            \"[ -f /tmp/no-reboot ] &amp;&amp; /bin/rm -f /tmp/no-reboot &amp;&amp; $LOG Reboot complete &amp;&amp; exit 0\",\n                            \"$LOG Installing any available security updates\",\n                            \"yum update --security -y\",\n                            \"$LOG Stopping lava worker daemons\",\n                            \"lava-stop --level '{{LogLevel}}' --log @local0 --signal '{{Signal}}' --wait '{{Wait}}'\",\n                            \"touch /tmp/no-reboot || exit 1\",\n                            \"$LOG Rebooting\",\n                            \"exit 194\"\n                        ],\n                        \"timeoutSeconds\": \"{{ExecutionTimeout}}\"\n                    },\n                    \"name\": \"rebootWorkerInstance\"\n                }\n            ],\n            \"parameters\": {\n                \"ExecutionTimeout\": {\n                    \"allowedPattern\": \"^\\\\d+$\",\n                    \"default\": \"3600\",\n                    \"description\": \"Execution timeout in seconds\",\n                    \"type\": \"String\"\n                },\n                \"LogLevel\": {\n                    \"allowedValues\": [\n                        \"debug\",\n                        \"info\",\n                        \"warning\"\n                    ],\n                    \"default\": \"info\",\n                    \"description\": \"Logging level\",\n                    \"type\": \"String\"\n                },\n                \"Signal\": {\n                    \"allowedValues\": [\n                        \"SIGHUP\",\n                        \"SIGKILL\"\n                    ],\n                    \"default\": \"SIGHUP\",\n                    \"description\": \"SIGHUP for controlled shutdown. SIGKILL for hard kill.\",\n                    \"type\": \"String\"\n                },\n                \"Wait\": {\n                    \"allowedPattern\": \"^\\\\d+[hms]?$\",\n                    \"default\": \"15m\",\n                    \"description\": \"Wait for specified duration for lava workers to stop voluntarily before killing them.\",\n                    \"type\": \"String\"\n                }\n            },\n            \"schemaVersion\": \"2.2\"\n        },\n        \"DocumentFormat\": \"JSON\",\n        \"DocumentType\": \"Command\",\n        \"Name\": \"lava-RebootWorkerInstance\",\n        \"TargetType\": \"/AWS::EC2::Instance\"\n    },\n    \"Type\": \"AWS::SSM::Document\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#ssmdocumentlavastop","title":"ssmDocumentLavaStop","text":"DetailsSource Property Value Type AWS::SSM::Document Group SSM Resources (SSM) Description SSM command document to stop lava daemons on a worker instance <pre><code>{\n    \"Properties\": {\n        \"Content\": {\n            \"description\": \"Signal lava daemons to stop and wait for them to finish.\",\n            \"mainSteps\": [\n                {\n                    \"action\": \"aws:runShellScript\",\n                    \"inputs\": {\n                        \"runCommand\": [\n                            \"LOG='logger -s -p local0.info -t lava-stop'\",\n                            \"[ '{{StopDispatch}}' == 'yes' ] &amp;&amp; STOP_DISPATCH=--no-dispatch\",\n                            \"lava-stop --level '{{LogLevel}}' --log @local0 --signal '{{Signal}}' --wait '{{Wait}}' $STOP_DISPATCH\",\n                            \"$LOG lava daemons stopped\"\n                        ],\n                        \"timeoutSeconds\": \"{{ExecutionTimeout}}\"\n                    },\n                    \"name\": \"stopWorkerDaemons\"\n                }\n            ],\n            \"parameters\": {\n                \"ExecutionTimeout\": {\n                    \"allowedPattern\": \"^\\\\d+$\",\n                    \"default\": \"3600\",\n                    \"description\": \"Execution timeout in seconds\",\n                    \"type\": \"String\"\n                },\n                \"LogLevel\": {\n                    \"allowedValues\": [\n                        \"debug\",\n                        \"info\",\n                        \"warning\"\n                    ],\n                    \"default\": \"info\",\n                    \"description\": \"Logging level\",\n                    \"type\": \"String\"\n                },\n                \"Signal\": {\n                    \"allowedValues\": [\n                        \"SIGHUP\",\n                        \"SIGKILL\"\n                    ],\n                    \"default\": \"SIGHUP\",\n                    \"description\": \"SIGHUP for controlled shutdown. SIGKILL for hard kill.\",\n                    \"type\": \"String\"\n                },\n                \"StopDispatch\": {\n                    \"allowedValues\": [\n                        \"yes\",\n                        \"no\"\n                    ],\n                    \"default\": \"yes\",\n                    \"description\": \"Prevent scheduled dispatches from this instance?\",\n                    \"type\": \"String\"\n                },\n                \"Wait\": {\n                    \"allowedPattern\": \"^[0-9]+[hms]?$\",\n                    \"default\": \"15m\",\n                    \"description\": \"Wait for specified duration for lava workers to stop voluntarily before killing them.\",\n                    \"type\": \"String\"\n                }\n            },\n            \"schemaVersion\": \"2.2\"\n        },\n        \"DocumentFormat\": \"JSON\",\n        \"DocumentType\": \"Command\",\n        \"Name\": \"lava-StopWorkerDaemons\",\n        \"TargetType\": \"/AWS::EC2::Instance\"\n    },\n    \"Type\": \"AWS::SSM::Document\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#ssmdocumentsecupdate","title":"ssmDocumentSecUpdate","text":"DetailsSource Property Value Type AWS::SSM::Document Group SSM Resources (SSM) Description SSM command document to check if security updates are available for a lava worker instance and install and reboot if there are. <pre><code>{\n    \"Properties\": {\n        \"Content\": {\n            \"description\": \"If security updates available for lava instance, apply them and reboot\",\n            \"mainSteps\": [\n                {\n                    \"action\": \"aws:runShellScript\",\n                    \"inputs\": {\n                        \"runCommand\": [\n                            \"LOG='logger -s -p local0.info -t lava-secupdate'\",\n                            \"$LOG Starting\",\n                            \"INSTANCE=$(ec2-metadata -i | cut -d' ' -f2)\",\n                            \"export AWS_DEFAULT_REGION=$(ec2-metadata -z | cut -d' ' -f2 | sed -e 's/.$//')\",\n                            \"NAME=$(aws ec2 describe-tags --filters \\\"Name=resource-id,Values=$INSTANCE\\\" \\\"Name=key,Values=Name\\\" --query 'Tags[0].Value' --output text)\",\n                            \"EVENT_MSG='{\\\"Source\\\":\\\"lava\\\",\\\"DetailType\\\":\\\"Lava Worker Instance Patching Notification\\\",\\\"Detail\\\":\\\"{\\\\\\\"instance-id\\\\\\\":\\\\\\\"'$INSTANCE'\\\\\\\",\\\\\\\"instance-name\\\\\\\":\\\\\\\"'$NAME'\\\\\\\",\\\\\\\"info\\\\\\\":\\\\\\\"Reboot complete\\\\\\\"}\\\"}'\",\n                            \"[ -f /tmp/no-reboot ] &amp;&amp; aws events put-events --entries \\\"$EVENT_MSG\\\"\",\n                            \"[ -f /tmp/no-reboot ] &amp;&amp; /bin/rm -f /tmp/no-reboot &amp;&amp; $LOG Reboot complete &amp;&amp; exit 0\",\n                            \"UPDAYS=$(awk '{ printf(\\\"%d\\\", $1/60/60/24); }' /proc/uptime)\",\n                            \"[ $UPDAYS -lt {{MinUpDays}} ] &amp;&amp; $LOG Instance has only been up $UPDAYS days - skip &amp;&amp; exit 0\",\n                            \"yum check-update --security --quiet\",\n                            \"[ $? -ne 100 ] &amp;&amp; $LOG No security updates available - skip &amp;&amp; exit 0\",\n                            \"$LOG Installing security updates\",\n                            \"yum update --security --quiet -y\",\n                            \"needs-restarting -r &amp;&amp; $LOG No reboot required &amp;&amp; exit 0\",\n                            \"$LOG Stopping lava worker daemons\",\n                            \"lava-stop --level '{{LogLevel}}' --log @local0 --signal '{{Signal}}' --wait '{{Wait}}'\",\n                            \"touch /tmp/no-reboot || exit 1\",\n                            \"EVENT_MSG='{\\\"Source\\\":\\\"lava\\\",\\\"DetailType\\\":\\\"Lava Worker Instance Patching Notification\\\",\\\"Detail\\\":\\\"{\\\\\\\"instance-id\\\\\\\":\\\\\\\"'$INSTANCE'\\\\\\\",\\\\\\\"instance-name\\\\\\\":\\\\\\\"'$NAME'\\\\\\\",\\\\\\\"info\\\\\\\":\\\\\\\"Rebooting after security patching\\\\\\\"}\\\"}'\",\n                            \"aws events put-events --entries \\\"$EVENT_MSG\\\"\",\n                            \"$LOG Rebooting\",\n                            \"exit 194\"\n                        ],\n                        \"timeoutSeconds\": \"{{ExecutionTimeout}}\"\n                    },\n                    \"name\": \"secUpdates\"\n                }\n            ],\n            \"parameters\": {\n                \"ExecutionTimeout\": {\n                    \"allowedPattern\": \"^\\\\d+$\",\n                    \"default\": \"3600\",\n                    \"description\": \"Execution timeout in seconds\",\n                    \"type\": \"String\"\n                },\n                \"LogLevel\": {\n                    \"allowedValues\": [\n                        \"debug\",\n                        \"info\",\n                        \"warning\"\n                    ],\n                    \"default\": \"info\",\n                    \"description\": \"Logging level\",\n                    \"type\": \"String\"\n                },\n                \"MinUpDays\": {\n                    \"allowedPattern\": \"^\\\\d+$\",\n                    \"default\": \"0\",\n                    \"description\": \"Skip if instance hasn't been up for this many days.\",\n                    \"type\": \"String\"\n                },\n                \"Signal\": {\n                    \"allowedValues\": [\n                        \"SIGHUP\",\n                        \"SIGKILL\"\n                    ],\n                    \"default\": \"SIGHUP\",\n                    \"description\": \"SIGHUP for controlled shutdown of lava daemons. SIGKILL for hard kill.\",\n                    \"type\": \"String\"\n                },\n                \"Wait\": {\n                    \"allowedPattern\": \"^\\\\d+[hms]?$\",\n                    \"default\": \"15m\",\n                    \"description\": \"Wait for specified duration for lava workers to stop voluntarily before killing them.\",\n                    \"type\": \"String\"\n                }\n            },\n            \"schemaVersion\": \"2.2\"\n        },\n        \"DocumentFormat\": \"JSON\",\n        \"DocumentType\": \"Command\",\n        \"Name\": \"lava-SecurityUpdate\",\n        \"TargetType\": \"/AWS::EC2::Instance\"\n    },\n    \"Type\": \"AWS::SSM::Document\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lava-realmcfnjson","title":"lava-realm.cfn.json","text":"<p>This template builds the core components for a new Lava realm. It does not create any Lava workers or add the required entry to the realms table.</p>"},{"location":"19-cloudformation-templates.html#parameters_1","title":"Parameters","text":"Parameter Type Description Version String Lava version (read only). autoscalingHeartbeatMinutes Number Send auto scaling heartbeats at this frequency when workers are terminating. Must be between 1 and 30. createRealmsTable String Should the realms table be created? kmsKeyAdmin String IAM user name of KMS key administrator Must be specified. lambdaArchitecture String Lambda machine architecture lambdaMemory Number Memory for the lambdas (Mb) A minimum of 160Mb is recommended for py3.11. Py3.12/3.13 will use more. lambdaMetricsSchedule String Enable the scheduler for the metrics lambda? lambdaRuntime String Lambda runtime lambdaTimeout Number Timeout for the lambdas (seconds) Must be between 15 and 900. lambdaVersion String Code version of lambda bundles (e.g. 2.3.1). If blank, no lambdas are deployed. lavaBucketName String Name of S3 bucket for the realm. Must be specified. lavaGroupTag String Value for the lava:group tag on resources (e.g. prod). logBucketName String Name of S3 bucket for S3 logs. Must be specified. readCapacityDataTables Number Read capacity for the Dynamo DB data tables. Must be &gt;= 1 readCapacityEventTable Number Read capacity for the Dynamo DB event table. Must be &gt;= 1 readCapacityStateTable Number Read capacity for the Dynamo DB state table. Must be &gt;= 1 realm String Name of the realm s3CodeBucket String S3 bucket containining Lava code. Must be specified. s3CodePrefix String Prefix in S3 bucket containining Lava code. Must be specified. tmpExpiryDays Number Expire temp area of lava bucket after this many days. Must be &gt;=1 workerStopMinutes Number Allow workers this many minutes to stop gracefully. Must be between 0 and 720 (12 hours). writeCapacityDataTables Number Write capacity for the Dynamo DB data tables. Must be &gt;= 1 writeCapacityEventTable Number Write capacity for the Dynamo DB event table. Must be &gt;= 1 writeCapacityStateTable Number Write capacity for the Dynamo DB state table. Must be &gt;= 1"},{"location":"19-cloudformation-templates.html#outputs_1","title":"Outputs","text":"Id Export as Description Version <code>{\"Fn::Sub\": \"lava:${realm}:version\"}</code> Lava version iamLavaWorkerPolicy <code>{\"Fn::Sub\": \"lava:${realm}:iamLavaWorkerPolicyArn\"}</code> ARN of the lava worker IAM policy. kmsUserKeyAlias <code>{\"Fn::Sub\": \"lava:${realm}:kmsUserKeyAlias\"}</code> Alias for the KMS user key for the realm. lambdaLavaStop <code>{\"Fn::Sub\": \"lava:${realm}:lambdaLavaStop\"}</code> ARN of the lava stop lambda for controlled worker shutdown lavaGroupTag <code>{\"Fn::Sub\": \"lava:${realm}:lavaGroupTag\"}</code> Value for the lava:group tag on resources. s3CodeBucket <code>{\"Fn::Sub\": \"lava:${realm}:s3CodeBucket\"}</code> S3 bucket where Lava base code resides. s3CodePrefix <code>{\"Fn::Sub\": \"lava:${realm}:s3CodePrefix\"}</code> S3 prefix where Lava base code resides. s3LavaBucket <code>{\"Fn::Sub\": \"lava:${realm}:s3LavaBucketName\"}</code> Name of lava realm bucket for payloads and tmp space."},{"location":"19-cloudformation-templates.html#resources_1","title":"Resources","text":"<ul> <li>DynamoDB Resources</li> <li>Events Resources</li> <li>IAM Resources</li> <li>KMS Resources</li> <li>Lambda Resources</li> <li>S3 Resources</li> <li>SNS Resources</li> </ul>"},{"location":"19-cloudformation-templates.html#dynamodb-resources","title":"DynamoDB Resources","text":"Id Type Description dynTableConnections Table Lava connections table. There must be one of these per lava realm. dynTableEvents Table Lava events table. There must be one of these per lava realm. dynTableJobs Table Lava jobs table. There must be one of these per lava realm. dynTableRealm Table Lava realms table. There must be one of these per AWS account. dynTableS3Triggers Table Lava s3triggers table. There can be one of these per lava realm. dynTableState Table Lava transient state table. There must be one of these per lava realm."},{"location":"19-cloudformation-templates.html#events-resources","title":"Events Resources","text":"Id Type Description eventsScheduleMetricsLambda Rule EventBridge rule for running metrics lambda function."},{"location":"19-cloudformation-templates.html#iam-resources","title":"IAM Resources","text":"Id Type Description iamDispatchLambdaInstanceProfile InstanceProfile iamLavaAdminGroup Group Admin group for lave realm. iamLavaAdminPolicy ManagedPolicy Admin policy for lava realm - part 1. iamLavaAdminPolicy2 ManagedPolicy Admin policy for lava realm - part 2. iamLavaDispatchLambdaPolicy ManagedPolicy Policy for the dispatching lambdas for the lava realm. iamLavaDispatchLambdaRole Role IAM role for dispatching Lambda functions. iamLavaMetricsLambdaPolicy ManagedPolicy Policy for the metrics lambda for the lava realm. iamLavaMetricsLambdaRole Role IAM role for metrics Lambda function. iamLavaOperator Group Operator group for lave realm. iamLavaOperatorPolicy ManagedPolicy Operator (incremental) access policy for lava realm. iamLavaReaderGroup Group Reader group for lave realm. iamLavaReaderPolicy ManagedPolicy Read access policy for lava realm. iamLavaStopLambdaPolicy ManagedPolicy Policy for the node stopper lambda for the lava realm. iamLavaStopLambdaRole Role IAM role for node stopper Lambda function. iamLavaWorkerPolicy ManagedPolicy Worker policy for lava realm."},{"location":"19-cloudformation-templates.html#kms-resources","title":"KMS Resources","text":"Id Type Description kmsSysAlias Alias System KMS key alias for lava realm. kmsSysKey Key System KMS key for lava realm. kmsUserAlias Alias User KMS key alias for lava realm. kmsUserKey Key User KMS key for lava realm."},{"location":"19-cloudformation-templates.html#lambda-resources","title":"Lambda Resources","text":"Id Type Description lambdaDispatch Function Dispatch Lambda function for the realm. lambdaLavaMetrics Function Lambda function to produce CloudWatch metrics. lambdaLavaMetricsPermission Permission Allow EventBridge to run the schedule for the metrics lambda. lambdaLavaStop Function Lambda function to stop daemons on a worker node lambdaS3Trigger Function S3trigger Lambda function for the realm. lambdaS3TriggerPermission Permission Allow EventBridge rules to invoke the s3trigger lambda. lamdaDispatchPolicy Permission"},{"location":"19-cloudformation-templates.html#s3-resources","title":"S3 Resources","text":"Id Type Description s3LavaBucket Bucket Lava realm bucket for payloads and tmp space."},{"location":"19-cloudformation-templates.html#sns-resources","title":"SNS Resources","text":"Id Type Description snsDispatchHelper Topic SNS topic for the dispatch helper lambda to receive dispatch requests. snsLavaNotices Topic SNS topic for lava notices. Whether or not it is used depends on lava jobs. snsSubscribeDispatch Subscription Subscription for the dispatch helper Lambda to the dispatch helper SNS topic."},{"location":"19-cloudformation-templates.html#resource-details_1","title":"Resource Details","text":""},{"location":"19-cloudformation-templates.html#dyntableconnections","title":"dynTableConnections","text":"DetailsSource Property Value Type AWS::DynamoDB::Table Group DynamoDB Resources (DynamoDB) Description Lava connections table. There must be one of these per lava realm. <pre><code>{\n    \"Properties\": {\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"conn_id\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"KeySchema\": [\n            {\n                \"AttributeName\": \"conn_id\",\n                \"KeyType\": \"HASH\"\n            }\n        ],\n        \"PointInTimeRecoverySpecification\": {\n            \"PointInTimeRecoveryEnabled\": true\n        },\n        \"ProvisionedThroughput\": {\n            \"ReadCapacityUnits\": {\n                \"Ref\": \"readCapacityDataTables\"\n            },\n            \"WriteCapacityUnits\": {\n                \"Ref\": \"writeCapacityDataTables\"\n            }\n        },\n        \"TableName\": {\n            \"Fn::Sub\": \"lava.${realm}.connections\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:realm\",\n                \"Value\": {\n                    \"Ref\": \"realm\"\n                }\n            },\n            {\n                \"Key\": \"lava:group\",\n                \"Value\": {\n                    \"Ref\": \"lavaGroupTag\"\n                }\n            }\n        ]\n    },\n    \"Type\": \"AWS::DynamoDB::Table\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#dyntableevents","title":"dynTableEvents","text":"DetailsSource Property Value Type AWS::DynamoDB::Table Group DynamoDB Resources (DynamoDB) Description Lava events table. There must be one of these per lava realm. <pre><code>{\n    \"Properties\": {\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"job_id\",\n                \"AttributeType\": \"S\"\n            },\n            {\n                \"AttributeName\": \"run_id\",\n                \"AttributeType\": \"S\"\n            },\n            {\n                \"AttributeName\": \"tu_event\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"KeySchema\": [\n            {\n                \"AttributeName\": \"job_id\",\n                \"KeyType\": \"HASH\"\n            },\n            {\n                \"AttributeName\": \"run_id\",\n                \"KeyType\": \"RANGE\"\n            }\n        ],\n        \"LocalSecondaryIndexes\": [\n            {\n                \"IndexName\": \"job_id-tu_event-index\",\n                \"KeySchema\": [\n                    {\n                        \"AttributeName\": \"job_id\",\n                        \"KeyType\": \"HASH\"\n                    },\n                    {\n                        \"AttributeName\": \"tu_event\",\n                        \"KeyType\": \"RANGE\"\n                    }\n                ],\n                \"Projection\": {\n                    \"ProjectionType\": \"ALL\"\n                }\n            }\n        ],\n        \"ProvisionedThroughput\": {\n            \"ReadCapacityUnits\": {\n                \"Ref\": \"readCapacityEventTable\"\n            },\n            \"WriteCapacityUnits\": {\n                \"Ref\": \"writeCapacityEventTable\"\n            }\n        },\n        \"TableName\": {\n            \"Fn::Sub\": \"lava.${realm}.events\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:realm\",\n                \"Value\": {\n                    \"Ref\": \"realm\"\n                }\n            },\n            {\n                \"Key\": \"lava:group\",\n                \"Value\": {\n                    \"Ref\": \"lavaGroupTag\"\n                }\n            }\n        ],\n        \"TimeToLiveSpecification\": {\n            \"AttributeName\": \"ttl\",\n            \"Enabled\": true\n        }\n    },\n    \"Type\": \"AWS::DynamoDB::Table\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#dyntablejobs","title":"dynTableJobs","text":"DetailsSource Property Value Type AWS::DynamoDB::Table Group DynamoDB Resources (DynamoDB) Description Lava jobs table. There must be one of these per lava realm. <pre><code>{\n    \"Properties\": {\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"dispatcher\",\n                \"AttributeType\": \"S\"\n            },\n            {\n                \"AttributeName\": \"job_id\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"GlobalSecondaryIndexes\": [\n            {\n                \"IndexName\": \"dispatcher-index\",\n                \"KeySchema\": [\n                    {\n                        \"AttributeName\": \"dispatcher\",\n                        \"KeyType\": \"HASH\"\n                    },\n                    {\n                        \"AttributeName\": \"job_id\",\n                        \"KeyType\": \"RANGE\"\n                    }\n                ],\n                \"Projection\": {\n                    \"NonKeyAttributes\": [\n                        \"worker\",\n                        \"schedule\"\n                    ],\n                    \"ProjectionType\": \"INCLUDE\"\n                },\n                \"ProvisionedThroughput\": {\n                    \"ReadCapacityUnits\": {\n                        \"Ref\": \"readCapacityDataTables\"\n                    },\n                    \"WriteCapacityUnits\": {\n                        \"Ref\": \"writeCapacityDataTables\"\n                    }\n                }\n            }\n        ],\n        \"KeySchema\": [\n            {\n                \"AttributeName\": \"job_id\",\n                \"KeyType\": \"HASH\"\n            }\n        ],\n        \"PointInTimeRecoverySpecification\": {\n            \"PointInTimeRecoveryEnabled\": true\n        },\n        \"ProvisionedThroughput\": {\n            \"ReadCapacityUnits\": {\n                \"Ref\": \"readCapacityDataTables\"\n            },\n            \"WriteCapacityUnits\": {\n                \"Ref\": \"writeCapacityDataTables\"\n            }\n        },\n        \"TableName\": {\n            \"Fn::Sub\": \"lava.${realm}.jobs\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:realm\",\n                \"Value\": {\n                    \"Ref\": \"realm\"\n                }\n            },\n            {\n                \"Key\": \"lava:group\",\n                \"Value\": {\n                    \"Ref\": \"lavaGroupTag\"\n                }\n            }\n        ]\n    },\n    \"Type\": \"AWS::DynamoDB::Table\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#dyntablerealm","title":"dynTableRealm","text":"DetailsSource Property Value Type AWS::DynamoDB::Table Group DynamoDB Resources (DynamoDB) Description Lava realms table. There must be one of these per AWS account. <pre><code>{\n    \"Condition\": \"ifCreateRealmsTable\",\n    \"Properties\": {\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"realm\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"KeySchema\": [\n            {\n                \"AttributeName\": \"realm\",\n                \"KeyType\": \"HASH\"\n            }\n        ],\n        \"PointInTimeRecoverySpecification\": {\n            \"PointInTimeRecoveryEnabled\": true\n        },\n        \"ProvisionedThroughput\": {\n            \"ReadCapacityUnits\": {\n                \"Ref\": \"readCapacityDataTables\"\n            },\n            \"WriteCapacityUnits\": {\n                \"Ref\": \"writeCapacityDataTables\"\n            }\n        },\n        \"TableName\": \"lava.realms\",\n        \"Tags\": [\n            {\n                \"Key\": \"lava:group\",\n                \"Value\": \"*\"\n            }\n        ]\n    },\n    \"Type\": \"AWS::DynamoDB::Table\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#dyntables3triggers","title":"dynTableS3Triggers","text":"DetailsSource Property Value Type AWS::DynamoDB::Table Group DynamoDB Resources (DynamoDB) Description Lava s3triggers table. There can be one of these per lava realm. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"trigger_id\",\n                \"AttributeType\": \"S\"\n            },\n            {\n                \"AttributeName\": \"bucket\",\n                \"AttributeType\": \"S\"\n            },\n            {\n                \"AttributeName\": \"prefix\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"GlobalSecondaryIndexes\": [\n            {\n                \"IndexName\": \"s3trigger-index\",\n                \"KeySchema\": [\n                    {\n                        \"AttributeName\": \"bucket\",\n                        \"KeyType\": \"HASH\"\n                    },\n                    {\n                        \"AttributeName\": \"prefix\",\n                        \"KeyType\": \"RANGE\"\n                    }\n                ],\n                \"Projection\": {\n                    \"ProjectionType\": \"ALL\"\n                },\n                \"ProvisionedThroughput\": {\n                    \"ReadCapacityUnits\": {\n                        \"Ref\": \"readCapacityDataTables\"\n                    },\n                    \"WriteCapacityUnits\": {\n                        \"Ref\": \"writeCapacityDataTables\"\n                    }\n                }\n            }\n        ],\n        \"KeySchema\": [\n            {\n                \"AttributeName\": \"trigger_id\",\n                \"KeyType\": \"HASH\"\n            }\n        ],\n        \"PointInTimeRecoverySpecification\": {\n            \"PointInTimeRecoveryEnabled\": true\n        },\n        \"ProvisionedThroughput\": {\n            \"ReadCapacityUnits\": {\n                \"Ref\": \"readCapacityDataTables\"\n            },\n            \"WriteCapacityUnits\": {\n                \"Ref\": \"writeCapacityDataTables\"\n            }\n        },\n        \"TableName\": {\n            \"Fn::Sub\": \"lava.${realm}.s3triggers\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:realm\",\n                \"Value\": {\n                    \"Ref\": \"realm\"\n                }\n            },\n            {\n                \"Key\": \"lava:group\",\n                \"Value\": {\n                    \"Ref\": \"lavaGroupTag\"\n                }\n            }\n        ]\n    },\n    \"Type\": \"AWS::DynamoDB::Table\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#dyntablestate","title":"dynTableState","text":"DetailsSource Property Value Type AWS::DynamoDB::Table Group DynamoDB Resources (DynamoDB) Description Lava transient state table. There must be one of these per lava realm. <pre><code>{\n    \"Properties\": {\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"state_id\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"KeySchema\": [\n            {\n                \"AttributeName\": \"state_id\",\n                \"KeyType\": \"HASH\"\n            }\n        ],\n        \"PointInTimeRecoverySpecification\": {\n            \"PointInTimeRecoveryEnabled\": true\n        },\n        \"ProvisionedThroughput\": {\n            \"ReadCapacityUnits\": {\n                \"Ref\": \"readCapacityStateTable\"\n            },\n            \"WriteCapacityUnits\": {\n                \"Ref\": \"writeCapacityStateTable\"\n            }\n        },\n        \"TableName\": {\n            \"Fn::Sub\": \"lava.${realm}.state\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:realm\",\n                \"Value\": {\n                    \"Ref\": \"realm\"\n                }\n            },\n            {\n                \"Key\": \"lava:group\",\n                \"Value\": {\n                    \"Ref\": \"lavaGroupTag\"\n                }\n            }\n        ],\n        \"TimeToLiveSpecification\": {\n            \"AttributeName\": \"ttl\",\n            \"Enabled\": true\n        }\n    },\n    \"Type\": \"AWS::DynamoDB::Table\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#eventsschedulemetricslambda","title":"eventsScheduleMetricsLambda","text":"DetailsSource Property Value Type AWS::Events::Rule Group Events Resources (Events) Description EventBridge rule for running metrics lambda function. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Schedule lava-${realm}-metrics lambda\"\n        },\n        \"EventBusName\": \"default\",\n        \"Name\": {\n            \"Fn::Sub\": \"lava-${realm}---metrics-schedule\"\n        },\n        \"ScheduleExpression\": \"rate(1 minute)\",\n        \"State\": {\n            \"Ref\": \"lambdaMetricsSchedule\"\n        },\n        \"Targets\": [\n            {\n                \"Arn\": {\n                    \"Fn::GetAtt\": [\n                        \"lambdaLavaMetrics\",\n                        \"Arn\"\n                    ]\n                },\n                \"Id\": {\n                    \"Fn::Sub\": \"lava-${realm}---metrics-lambda\"\n                }\n            }\n        ]\n    },\n    \"Type\": \"AWS::Events::Rule\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamdispatchlambdainstanceprofile","title":"iamDispatchLambdaInstanceProfile","text":"DetailsSource Property Value Type AWS::IAM::InstanceProfile Group IAM Resources (IAM) <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Roles\": [\n            {\n                \"Ref\": \"iamLavaDispatchLambdaRole\"\n            }\n        ]\n    },\n    \"Type\": \"AWS::IAM::InstanceProfile\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavaadmingroup","title":"iamLavaAdminGroup","text":"DetailsSource Property Value Type AWS::IAM::Group Group IAM Resources (IAM) Description Admin group for lave realm. <pre><code>{\n    \"Properties\": {\n        \"GroupName\": {\n            \"Fn::Sub\": \"lava-${realm}-admin\"\n        },\n        \"ManagedPolicyArns\": [\n            {\n                \"Ref\": \"iamLavaAdminPolicy\"\n            },\n            {\n                \"Ref\": \"iamLavaAdminPolicy2\"\n            }\n        ],\n        \"Path\": \"/\"\n    },\n    \"Type\": \"AWS::IAM::Group\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavaadminpolicy","title":"iamLavaAdminPolicy","text":"DetailsSource Property Value Type AWS::IAM::ManagedPolicy Group IAM Resources (IAM) Description Admin policy for lava realm - part 1. <pre><code>{\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Admin policy for lava realm ${realm}\"\n        },\n        \"ManagedPolicyName\": {\n            \"Fn::Sub\": \"lava-${realm}-admin\"\n        },\n        \"PolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": \"s3:ListAllMyBuckets\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListAllBuckets\"\n                },\n                {\n                    \"Action\": [\n                        \"s3:ListBucket\",\n                        \"s3:GetBucket*\",\n                        \"s3:GetEncryptionConfiguration\",\n                        \"s3:GetInventoryConfiguration\",\n                        \"s3:GetLifecycleConfiguration\",\n                        \"s3:GetMetricsConfiguration\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::GetAtt\": [\n                                \"s3LavaBucket\",\n                                \"Arn\"\n                            ]\n                        }\n                    ],\n                    \"Sid\": \"GetLavaBucketInfo\"\n                },\n                {\n                    \"Action\": [\n                        \"s3:GetObject*\",\n                        \"s3:PutObject\",\n                        \"s3:ListMultipartUploadParts\",\n                        \"s3:DeleteObject\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:s3:::${s3LavaBucket}/*\"\n                        }\n                    ],\n                    \"Sid\": \"ReadWriteLavaBucket\"\n                },\n                {\n                    \"Action\": \"sqs:ListQueues\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListSqsQueues\"\n                },\n                {\n                    \"Action\": [\n                        \"sqs:DeleteMessage\",\n                        \"sqs:GetQueueUrl\",\n                        \"sqs:ReceiveMessage\",\n                        \"sqs:SendMessage\",\n                        \"sqs:GetQueueAttributes\",\n                        \"sqs:ListQueueTags\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:lava-${realm}-*\"\n                    },\n                    \"Sid\": \"AccessSqsQueues\"\n                },\n                {\n                    \"Action\": \"ssm:DescribeParameters\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"DescribeSsmParams\"\n                },\n                {\n                    \"Action\": [\n                        \"ssm:GetParameter\",\n                        \"ssm:GetParameters\",\n                        \"ssm:PutParameter\",\n                        \"ssm:DeleteParameter\",\n                        \"ssm:DeleteParameters\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/lava/${realm}/*\"\n                    },\n                    \"Sid\": \"ManageSsmParams\"\n                },\n                {\n                    \"Action\": [\n                        \"secretsmanager:ListSecrets\",\n                        \"secretsmanager:GetRandomPassword\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListSecrets\"\n                },\n                {\n                    \"Action\": [\n                        \"secretsmanager:CancelRotateSecret\",\n                        \"secretsmanager:CreateSecret\",\n                        \"secretsmanager:DeleteResourcePolicy\",\n                        \"secretsmanager:DeleteSecret\",\n                        \"secretsmanager:DescribeSecret\",\n                        \"secretsmanager:GetResourcePolicy\",\n                        \"secretsmanager:GetSecretValue\",\n                        \"secretsmanager:ListSecretVersionIds\",\n                        \"secretsmanager:PutResourcePolicy\",\n                        \"secretsmanager:PutSecretValue\",\n                        \"secretsmanager:RestoreSecret\",\n                        \"secretsmanager:RotateSecret\",\n                        \"secretsmanager:TagResource\",\n                        \"secretsmanager:UntagResource\",\n                        \"secretsmanager:UpdateSecret\",\n                        \"secretsmanager:UpdateSecretVersionStage\",\n                        \"secretsmanager:ValidateResourcePolicy\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:/lava/${realm}/*\"\n                    },\n                    \"Sid\": \"ManageSecrets\"\n                },\n                {\n                    \"Action\": [\n                        \"dynamodb:ListTables\",\n                        \"dynamodb:ListBackups\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListAllDynamoTables\"\n                },\n                {\n                    \"Action\": [\n                        \"dynamodb:ListTagsOfResource\",\n                        \"dynamodb:GetItem\",\n                        \"dynamodb:BatchGetItem\",\n                        \"dynamodb:DescribeTable\",\n                        \"dynamodb:DescribeTimeToLive\",\n                        \"dynamodb:DescribeContinuousBackups\",\n                        \"dynamodb:Query\",\n                        \"dynamodb:Scan\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.realms\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.jobs/index/*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.events/index/*\"\n                        }\n                    ],\n                    \"Sid\": \"AccessDynamoDBLavaTables\"\n                },\n                {\n                    \"Action\": [\n                        \"dynamodb:BatchWriteItem\",\n                        \"dynamodb:DeleteItem\",\n                        \"dynamodb:PutItem\",\n                        \"dynamodb:UpdateItem\",\n                        \"dynamodb:PartiQLUpdate\",\n                        \"dynamodb:PartiQLInsert\",\n                        \"dynamodb:PartiQLDelete\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.*\"\n                    },\n                    \"Sid\": \"UpdateDynamoDBLavaTables\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:ListKeys\",\n                        \"kms:ListAliases\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListKMSkeys\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\",\n                        \"kms:ReEncrypt*\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:DescribeKey\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::GetAtt\": [\n                                \"kmsSysKey\",\n                                \"Arn\"\n                            ]\n                        },\n                        {\n                            \"Fn::GetAtt\": [\n                                \"kmsUserKey\",\n                                \"Arn\"\n                            ]\n                        }\n                    ],\n                    \"Sid\": \"UseKMSkeys\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:CreateGrant\",\n                        \"kms:ListGrants\",\n                        \"kms:RevokeGrant\"\n                    ],\n                    \"Condition\": {\n                        \"Bool\": {\n                            \"kms:GrantIsForAWSResource\": \"true\"\n                        }\n                    },\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::GetAtt\": [\n                                \"kmsSysKey\",\n                                \"Arn\"\n                            ]\n                        },\n                        {\n                            \"Fn::GetAtt\": [\n                                \"kmsUserKey\",\n                                \"Arn\"\n                            ]\n                        }\n                    ],\n                    \"Sid\": \"KMSattachPersistentResources\"\n                },\n                {\n                    \"Action\": \"sns:ListTopics\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListSNStopics\"\n                },\n                {\n                    \"Fn::If\": [\n                        \"ifCreateLambdas\",\n                        {\n                            \"Action\": [\n                                \"sns:Publish\",\n                                \"sns:GetTopicAttributes\"\n                            ],\n                            \"Effect\": \"Allow\",\n                            \"Resource\": {\n                                \"Ref\": \"snsDispatchHelper\"\n                            },\n                            \"Sid\": \"PublishToSNSdispatchHelper\"\n                        },\n                        {\n                            \"Ref\": \"AWS::NoValue\"\n                        }\n                    ]\n                },\n                {\n                    \"Action\": \"ecr:GetAuthorizationToken\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"getEcrAuthToken\"\n                },\n                {\n                    \"Action\": [\n                        \"ecr:DescribeRegistry\",\n                        \"ecr:DescribeRepositories\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ecrDescribeRepos\"\n                },\n                {\n                    \"Action\": [\n                        \"ecr:GetLifecyclePolicyPreview\",\n                        \"ecr:GetDownloadUrlForLayer\",\n                        \"ecr:BatchGetImage\",\n                        \"ecr:DescribeImages\",\n                        \"ecr:ListTagsForResource\",\n                        \"ecr:ListImages\",\n                        \"ecr:BatchCheckLayerAvailability\",\n                        \"ecr:GetLifecyclePolicy\",\n                        \"ecr:GetRepositoryPolicy\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/dist/lava/*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/lava/${realm}/*\"\n                        }\n                    ],\n                    \"Sid\": \"ReadEcrLavaRepos\"\n                },\n                {\n                    \"Action\": [\n                        \"ecr:BatchDeleteImage\",\n                        \"ecr:CompleteLayerUpload\",\n                        \"ecr:CreateRepository\",\n                        \"ecr:DeleteLifecyclePolicy\",\n                        \"ecr:DeleteRepository\",\n                        \"ecr:InitiateLayerUpload\",\n                        \"ecr:PutImage\",\n                        \"ecr:PutImageScanningConfiguration\",\n                        \"ecr:PutImageTagMutability\",\n                        \"ecr:PutLifecyclePolicy\",\n                        \"ecr:StartImageScan\",\n                        \"ecr:StartLifecyclePolicyPreview\",\n                        \"ecr:TagResource\",\n                        \"ecr:UntagResource\",\n                        \"ecr:UploadLayerPart\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/lava/${realm}/*\"\n                    },\n                    \"Sid\": \"WriteEcrRealmRepo\"\n                },\n                {\n                    \"Action\": [\n                        \"events:List*\",\n                        \"events:TestEventPattern\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"EventsListAll\"\n                },\n                {\n                    \"Action\": [\n                        \"events:DeleteRule\",\n                        \"events:DescribeRule\",\n                        \"events:DisableRule\",\n                        \"events:EnableRule\",\n                        \"events:PutEvents\",\n                        \"events:PutRule\",\n                        \"events:PutTargets\",\n                        \"events:RemoveTargets\",\n                        \"events:TagResource\",\n                        \"events:UntagResource\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/lava.${realm}.*\"\n                    },\n                    \"Sid\": \"EventsReadWriteRules\"\n                },\n                {\n                    \"Action\": [\n                        \"logs:DescribeLogGroups\",\n                        \"logs:GetLogRecord\",\n                        \"logs:GetQueryResults\",\n                        \"logs:StopQuery\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"AccessLogGroups\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::IAM::ManagedPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavaadminpolicy2","title":"iamLavaAdminPolicy2","text":"DetailsSource Property Value Type AWS::IAM::ManagedPolicy Group IAM Resources (IAM) Description Admin policy for lava realm - part 2. <pre><code>{\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Admin policy for lava realm ${realm} - part 2\"\n        },\n        \"ManagedPolicyName\": {\n            \"Fn::Sub\": \"lava-${realm}-admin2\"\n        },\n        \"PolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": [\n                        \"logs:DescribeLogStreams\",\n                        \"logs:FilterLogEvents\",\n                        \"logs:GetLogEvents\",\n                        \"logs:GetLogGroupFields\",\n                        \"logs:StartQuery\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/var/log/lava/${realm}\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/var/log/lava/${realm}:*\"\n                        }\n                    ],\n                    \"Sid\": \"ReadLavaWorkerLogs\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::IAM::ManagedPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavadispatchlambdapolicy","title":"iamLavaDispatchLambdaPolicy","text":"DetailsSource Property Value Type AWS::IAM::ManagedPolicy Group IAM Resources (IAM) Description Policy for the dispatching lambdas for the lava realm. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Policy for dispatching Lambdas for ${realm} realm\"\n        },\n        \"ManagedPolicyName\": {\n            \"Fn::Sub\": \"lava-${realm}-dispatch-lambdas\"\n        },\n        \"PolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": \"sqs:ListQueues\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListSqsQueues\"\n                },\n                {\n                    \"Action\": [\n                        \"sqs:GetQueueUrl\",\n                        \"sqs:GetQueueAttributes\",\n                        \"sqs:SendMessage\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:lava-${realm}-*\"\n                    },\n                    \"Sid\": \"AccessSqsQueues\"\n                },\n                {\n                    \"Action\": [\n                        \"dynamodb:GetItem\",\n                        \"dynamodb:Query\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.*\"\n                        }\n                    ],\n                    \"Sid\": \"AccessDynamoDBLavaTables\"\n                },\n                {\n                    \"Action\": [\n                        \"logs:CreateLogGroup\",\n                        \"logs:CreateLogStream\",\n                        \"logs:PutLogEvents\",\n                        \"logs:DescribeLogStreams\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"LogToCloudWatch\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:Decrypt\",\n                        \"kms:Encrypt\",\n                        \"kms:GenerateDataKey*\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::GetAtt\": [\n                            \"kmsUserKey\",\n                            \"Arn\"\n                        ]\n                    },\n                    \"Sid\": \"UseKmsUserKey\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::IAM::ManagedPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavadispatchlambdarole","title":"iamLavaDispatchLambdaRole","text":"DetailsSource Property Value Type AWS::IAM::Role Group IAM Resources (IAM) Description IAM role for dispatching Lambda functions. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": \"sts:AssumeRole\",\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"Service\": \"lambda.amazonaws.com\"\n                    }\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        },\n        \"ManagedPolicyArns\": [\n            {\n                \"Ref\": \"iamLavaDispatchLambdaPolicy\"\n            }\n        ],\n        \"RoleName\": {\n            \"Fn::Sub\": \"lava-${realm}-dispatch-lambda\"\n        }\n    },\n    \"Type\": \"AWS::IAM::Role\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavametricslambdapolicy","title":"iamLavaMetricsLambdaPolicy","text":"DetailsSource Property Value Type AWS::IAM::ManagedPolicy Group IAM Resources (IAM) Description Policy for the metrics lambda for the lava realm. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Policy for metrics Lambda for ${realm} realm\"\n        },\n        \"ManagedPolicyName\": {\n            \"Fn::Sub\": \"lava-${realm}-metricslambda\"\n        },\n        \"PolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": [\n                        \"logs:CreateLogGroup\",\n                        \"logs:CreateLogStream\",\n                        \"logs:PutLogEvents\",\n                        \"logs:DescribeLogStreams\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"LogToCloudWatch\"\n                },\n                {\n                    \"Action\": \"autoscaling:DescribeAutoScalingGroups\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\"\n                },\n                {\n                    \"Action\": \"cloudwatch:PutMetricData\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"PutCloudWatchMetricData\"\n                },\n                {\n                    \"Action\": \"sqs:ListQueues\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListSqsQueues\"\n                },\n                {\n                    \"Action\": [\n                        \"sqs:GetQueueUrl\",\n                        \"sqs:GetQueueAttributes\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:lava-${realm}-*\"\n                    },\n                    \"Sid\": \"AccessSqsQueues\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::IAM::ManagedPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavametricslambdarole","title":"iamLavaMetricsLambdaRole","text":"DetailsSource Property Value Type AWS::IAM::Role Group IAM Resources (IAM) Description IAM role for metrics Lambda function. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": \"sts:AssumeRole\",\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"Service\": \"lambda.amazonaws.com\"\n                    }\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        },\n        \"ManagedPolicyArns\": [\n            {\n                \"Ref\": \"iamLavaMetricsLambdaPolicy\"\n            }\n        ],\n        \"RoleName\": {\n            \"Fn::Sub\": \"lava-${realm}-metrics-lambda\"\n        }\n    },\n    \"Type\": \"AWS::IAM::Role\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavaoperator","title":"iamLavaOperator","text":"DetailsSource Property Value Type AWS::IAM::Group Group IAM Resources (IAM) Description Operator group for lave realm. <pre><code>{\n    \"Properties\": {\n        \"GroupName\": {\n            \"Fn::Sub\": \"lava-${realm}-operator\"\n        },\n        \"ManagedPolicyArns\": [\n            {\n                \"Ref\": \"iamLavaReaderPolicy\"\n            },\n            {\n                \"Ref\": \"iamLavaOperatorPolicy\"\n            }\n        ],\n        \"Path\": \"/\"\n    },\n    \"Type\": \"AWS::IAM::Group\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavaoperatorpolicy","title":"iamLavaOperatorPolicy","text":"DetailsSource Property Value Type AWS::IAM::ManagedPolicy Group IAM Resources (IAM) Description Operator (incremental) access policy for lava realm. <pre><code>{\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Operator add-on policy for lava realm ${realm} : Operator = Reader + this policy\"\n        },\n        \"ManagedPolicyName\": {\n            \"Fn::Sub\": \"lava-${realm}-operator\"\n        },\n        \"PolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Fn::If\": [\n                        \"ifCreateLambdas\",\n                        {\n                            \"Action\": [\n                                \"sns:Publish\",\n                                \"sns:GetTopicAttributes\"\n                            ],\n                            \"Effect\": \"Allow\",\n                            \"Resource\": {\n                                \"Ref\": \"snsDispatchHelper\"\n                            },\n                            \"Sid\": \"PublishToSNSdispatchHelper\"\n                        },\n                        {\n                            \"Ref\": \"AWS::NoValue\"\n                        }\n                    ]\n                },\n                {\n                    \"Action\": [\n                        \"logs:DescribeLogGroups\",\n                        \"logs:GetLogRecord\",\n                        \"logs:GetQueryResults\",\n                        \"logs:StopQuery\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"AccessLogGroups\"\n                },\n                {\n                    \"Action\": [\n                        \"logs:DescribeLogStreams\",\n                        \"logs:FilterLogEvents\",\n                        \"logs:GetLogEvents\",\n                        \"logs:GetLogGroupFields\",\n                        \"logs:StartQuery\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/var/log/lava/${realm}\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/var/log/lava/${realm}:*\"\n                        }\n                    ],\n                    \"Sid\": \"ReadLavaWorkerLogs\"\n                },\n                {\n                    \"Action\": [\n                        \"sqs:ListQueues\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListSqsQueues\"\n                },\n                {\n                    \"Action\": [\n                        \"sqs:GetQueueUrl\",\n                        \"sqs:GetQueueAttributes\",\n                        \"sqs:ListQueueTags\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:lava-${realm}-*\"\n                    },\n                    \"Sid\": \"ReadSqsQueues\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::IAM::ManagedPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavareadergroup","title":"iamLavaReaderGroup","text":"DetailsSource Property Value Type AWS::IAM::Group Group IAM Resources (IAM) Description Reader group for lave realm. <pre><code>{\n    \"Properties\": {\n        \"GroupName\": {\n            \"Fn::Sub\": \"lava-${realm}-reader\"\n        },\n        \"ManagedPolicyArns\": [\n            {\n                \"Ref\": \"iamLavaReaderPolicy\"\n            }\n        ],\n        \"Path\": \"/\"\n    },\n    \"Type\": \"AWS::IAM::Group\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavareaderpolicy","title":"iamLavaReaderPolicy","text":"DetailsSource Property Value Type AWS::IAM::ManagedPolicy Group IAM Resources (IAM) Description Read access policy for lava realm. <pre><code>{\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Reader policy for lava realm ${realm}\"\n        },\n        \"ManagedPolicyName\": {\n            \"Fn::Sub\": \"lava-${realm}-reader\"\n        },\n        \"PolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": [\n                        \"s3:ListBucket\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::GetAtt\": [\n                                \"s3LavaBucket\",\n                                \"Arn\"\n                            ]\n                        }\n                    ],\n                    \"Sid\": \"GetLavaBucketInfo\"\n                },\n                {\n                    \"Action\": [\n                        \"s3:GetObject*\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:s3:::${s3LavaBucket}/*\"\n                        }\n                    ],\n                    \"Sid\": \"ReadLavaBucket\"\n                },\n                {\n                    \"Action\": [\n                        \"dynamodb:ListTables\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListAllDynamoTables\"\n                },\n                {\n                    \"Action\": [\n                        \"dynamodb:GetItem\",\n                        \"dynamodb:BatchGetItem\",\n                        \"dynamodb:DescribeTable\",\n                        \"dynamodb:DescribeContinuousBackups\",\n                        \"dynamodb:Query\",\n                        \"dynamodb:Scan\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.realms\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.jobs/index/*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.events/index/*\"\n                        }\n                    ],\n                    \"Sid\": \"AccessDynamoDBLavaTables\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:ListKeys\",\n                        \"kms:ListAliases\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListKMSkeys\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:Decrypt\",\n                        \"kms:DescribeKey\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::GetAtt\": [\n                                \"kmsUserKey\",\n                                \"Arn\"\n                            ]\n                        }\n                    ],\n                    \"Sid\": \"UseKMSkeys\"\n                },\n                {\n                    \"Action\": \"ecr:GetAuthorizationToken\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"getEcrAuthToken\"\n                },\n                {\n                    \"Action\": [\n                        \"ecr:DescribeRegistry\",\n                        \"ecr:DescribeRepositories\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ecrDescribeRepos\"\n                },\n                {\n                    \"Action\": [\n                        \"ecr:GetLifecyclePolicyPreview\",\n                        \"ecr:GetDownloadUrlForLayer\",\n                        \"ecr:BatchGetImage\",\n                        \"ecr:DescribeImages\",\n                        \"ecr:ListTagsForResource\",\n                        \"ecr:ListImages\",\n                        \"ecr:BatchCheckLayerAvailability\",\n                        \"ecr:GetLifecyclePolicy\",\n                        \"ecr:GetRepositoryPolicy\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/dist/lava/*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/lava/${realm}/*\"\n                        }\n                    ],\n                    \"Sid\": \"ReadEcrLavaRepos\"\n                },\n                {\n                    \"Action\": [\n                        \"events:List*\",\n                        \"events:TestEventPattern\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"EventsListAll\"\n                },\n                {\n                    \"Action\": \"events:DescribeRule\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/lava.${realm}.*\"\n                    },\n                    \"Sid\": \"EventsReadRules\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::IAM::ManagedPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavastoplambdapolicy","title":"iamLavaStopLambdaPolicy","text":"DetailsSource Property Value Type AWS::IAM::ManagedPolicy Group IAM Resources (IAM) Description Policy for the node stopper lambda for the lava realm. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Policy for node stopping Lambda for ${realm} realm\"\n        },\n        \"ManagedPolicyName\": {\n            \"Fn::Sub\": \"lava-${realm}-stop-lambda\"\n        },\n        \"PolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": [\n                        \"logs:CreateLogGroup\",\n                        \"logs:CreateLogStream\",\n                        \"logs:PutLogEvents\",\n                        \"logs:DescribeLogStreams\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"LogToCloudWatch\"\n                },\n                {\n                    \"Action\": [\n                        \"ssm:SendCommand\"\n                    ],\n                    \"Condition\": {\n                        \"StringLike\": {\n                            \"ssm:resourceTag/LavaRealm\": [\n                                {\n                                    \"Ref\": \"realm\"\n                                }\n                            ]\n                        }\n                    },\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        \"arn:aws:ec2:*:*:instance/*\"\n                    ],\n                    \"Sid\": \"SsmSendCommandToInstance\"\n                },\n                {\n                    \"Action\": \"ssm:SendCommand\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:ssm:${AWS::Region}::document/AWS-RunShellScript\"\n                        }\n                    ],\n                    \"Sid\": \"SsmSendCommandDocument\"\n                },\n                {\n                    \"Action\": \"ec2:Describe*\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::IAM::ManagedPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavastoplambdarole","title":"iamLavaStopLambdaRole","text":"DetailsSource Property Value Type AWS::IAM::Role Group IAM Resources (IAM) Description IAM role for node stopper Lambda function. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": \"sts:AssumeRole\",\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"Service\": \"lambda.amazonaws.com\"\n                    }\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        },\n        \"ManagedPolicyArns\": [\n            {\n                \"Ref\": \"iamLavaStopLambdaPolicy\"\n            }\n        ],\n        \"RoleName\": {\n            \"Fn::Sub\": \"lava-${realm}-stop-lambda\"\n        }\n    },\n    \"Type\": \"AWS::IAM::Role\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavaworkerpolicy","title":"iamLavaWorkerPolicy","text":"DetailsSource Property Value Type AWS::IAM::ManagedPolicy Group IAM Resources (IAM) Description Worker policy for lava realm. <pre><code>{\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"Worker policy for lava realm ${realm}\"\n        },\n        \"ManagedPolicyName\": {\n            \"Fn::Sub\": \"lava-${realm}-worker\"\n        },\n        \"PolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": \"s3:ListAllMyBuckets\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListAllBuckets\"\n                },\n                {\n                    \"Action\": [\n                        \"s3:ListBucket\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::GetAtt\": [\n                                \"s3LavaBucket\",\n                                \"Arn\"\n                            ]\n                        }\n                    ],\n                    \"Sid\": \"GetLavaBucketInfo\"\n                },\n                {\n                    \"Action\": [\n                        \"s3:GetObject*\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:s3:::${s3LavaBucket}/*\"\n                        }\n                    ],\n                    \"Sid\": \"ReadLavaBucket\"\n                },\n                {\n                    \"Action\": \"s3:PutObject\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:s3:::${s3LavaBucket}/tmp/*\"\n                    },\n                    \"Sid\": \"WriteLavaBucketTmp\"\n                },\n                {\n                    \"Action\": [\n                        \"s3:ListBucket\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:s3:::${s3CodeBucket}\"\n                    },\n                    \"Sid\": \"GetCodeBucketInfo\"\n                },\n                {\n                    \"Action\": \"s3:GetObject*\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:s3:::${s3CodeBucket}/${s3CodePrefix}/*\"\n                    },\n                    \"Sid\": \"ReadCodeBucket\"\n                },\n                {\n                    \"Action\": \"sqs:ListQueues\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"ListSqsQueues\"\n                },\n                {\n                    \"Action\": [\n                        \"sqs:DeleteMessage\",\n                        \"sqs:GetQueueUrl\",\n                        \"sqs:GetQueueAttributes\",\n                        \"sqs:ReceiveMessage\",\n                        \"sqs:SendMessage\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:lava-${realm}-*\"\n                    },\n                    \"Sid\": \"AccessSqsQueues\"\n                },\n                {\n                    \"Action\": [\n                        \"ssm:GetParameter\",\n                        \"ssm:GetParameters\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/lava/${realm}/*\"\n                    },\n                    \"Sid\": \"ReadSsmParams\"\n                },\n                {\n                    \"Action\": [\n                        \"secretsmanager:DescribeSecret\",\n                        \"secretsmanager:GetSecretValue\",\n                        \"secretsmanager:ListSecretVersionIds\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:/lava/${realm}/*\"\n                    },\n                    \"Sid\": \"ReadSecrets\"\n                },\n                {\n                    \"Action\": [\n                        \"dynamodb:GetItem\",\n                        \"dynamodb:Query\",\n                        \"dynamodb:Scan\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.realms\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.jobs/index/*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.events/index/*\"\n                        }\n                    ],\n                    \"Sid\": \"AccessDynamoDBLavaTables\"\n                },\n                {\n                    \"Action\": [\n                        \"dynamodb:BatchWriteItem\",\n                        \"dynamodb:PutItem\",\n                        \"dynamodb:UpdateItem\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.events\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.events/index/*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/lava.${realm}.state\"\n                        }\n                    ],\n                    \"Sid\": \"WriteDynamoDBLavaTables\"\n                },\n                {\n                    \"Action\": \"cloudwatch:PutMetricData\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"PutCloudWatchMetricData\"\n                },\n                {\n                    \"Action\": [\n                        \"logs:CreateLogGroup\",\n                        \"logs:CreateLogStream\",\n                        \"logs:PutLogEvents\",\n                        \"logs:DescribeLogGroups\",\n                        \"logs:DescribeLogStreams\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"LogToCloudWatch\"\n                },\n                {\n                    \"Action\": [\n                        \"iam:ListUsers\",\n                        \"iam:GetGroup\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"iamLimitedRead\"\n                },\n                {\n                    \"Action\": [\n                        \"iam:ListSSHPublicKeys\",\n                        \"iam:GetSSHPublicKey\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"iamGetSshPublicKeys\"\n                },\n                {\n                    \"Action\": \"sns:Publish\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Ref\": \"snsLavaNotices\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:mbot\"\n                        }\n                    ],\n                    \"Sid\": \"PublishSNS\"\n                },\n                {\n                    \"Action\": \"ec2:Describe*\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"DescribeEc2\"\n                },\n                {\n                    \"Action\": [\n                        \"ses:SendEmail\",\n                        \"ses:SendRawEmail\",\n                        \"ses:ListIdentities\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"SendSesEmail\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:Decrypt\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::GetAtt\": [\n                            \"kmsSysKey\",\n                            \"Arn\"\n                        ]\n                    },\n                    \"Sid\": \"UseKmsSysKey\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:Decrypt\",\n                        \"kms:Encrypt\",\n                        \"kms:GenerateDataKey*\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::GetAtt\": [\n                            \"kmsUserKey\",\n                            \"Arn\"\n                        ]\n                    },\n                    \"Sid\": \"UseKmsUserKey\"\n                },\n                {\n                    \"Action\": \"ecr:GetAuthorizationToken\",\n                    \"Effect\": \"Allow\",\n                    \"Resource\": \"*\",\n                    \"Sid\": \"getEcrAuthToken\"\n                },\n                {\n                    \"Action\": [\n                        \"ecr:GetLifecyclePolicyPreview\",\n                        \"ecr:GetDownloadUrlForLayer\",\n                        \"ecr:BatchGetImage\",\n                        \"ecr:DescribeImages\",\n                        \"ecr:DescribeRepositories\",\n                        \"ecr:ListTagsForResource\",\n                        \"ecr:ListImages\",\n                        \"ecr:BatchCheckLayerAvailability\",\n                        \"ecr:GetLifecyclePolicy\",\n                        \"ecr:GetRepositoryPolicy\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": [\n                        {\n                            \"Fn::Sub\": \"arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/dist/lava/*\"\n                        },\n                        {\n                            \"Fn::Sub\": \"arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/lava/${realm}/*\"\n                        }\n                    ],\n                    \"Sid\": \"ReadEcrLavaRepos\"\n                },\n                {\n                    \"Action\": [\n                        \"autoscaling:CompleteLifecycleAction\",\n                        \"autoscaling:RecordLifecycleActionHeartbeat\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:autoscaling:${AWS::Region}:${AWS::AccountId}:autoScalingGroup:*:autoScalingGroupName/lava-${realm}-*\"\n                    },\n                    \"Sid\": \"AutoscalingLifecycleMgmt\"\n                },\n                {\n                    \"Action\": [\n                        \"events:PutEvents\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Resource\": {\n                        \"Fn::Sub\": \"arn:aws:events:${AWS::Region}:${AWS::AccountId}:event-bus/default\"\n                    },\n                    \"Sid\": \"PutEvents\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::IAM::ManagedPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#kmssysalias","title":"kmsSysAlias","text":"DetailsSource Property Value Type AWS::KMS::Alias Group KMS Resources (KMS) Description System KMS key alias for lava realm. <pre><code>{\n    \"Properties\": {\n        \"AliasName\": {\n            \"Fn::Sub\": \"alias/lava-${realm}-sys\"\n        },\n        \"TargetKeyId\": {\n            \"Ref\": \"kmsSysKey\"\n        }\n    },\n    \"Type\": \"AWS::KMS::Alias\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#kmssyskey","title":"kmsSysKey","text":"DetailsSource Property Value Type AWS::KMS::Key Group KMS Resources (KMS) Description System KMS key for lava realm. <pre><code>{\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"System key for lava realm ${realm}\"\n        },\n        \"EnableKeyRotation\": true,\n        \"Enabled\": true,\n        \"KeyPolicy\": {\n            \"Id\": \"key-consolepolicy-3\",\n            \"Statement\": [\n                {\n                    \"Action\": \"kms:*\",\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"AWS\": {\n                            \"Fn::Sub\": \"arn:aws:iam::${AWS::AccountId}:root\"\n                        }\n                    },\n                    \"Resource\": \"*\",\n                    \"Sid\": \"Enable IAM User Permissions\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:Create*\",\n                        \"kms:Describe*\",\n                        \"kms:Enable*\",\n                        \"kms:List*\",\n                        \"kms:Put*\",\n                        \"kms:Update*\",\n                        \"kms:Revoke*\",\n                        \"kms:Disable*\",\n                        \"kms:Get*\",\n                        \"kms:Delete*\",\n                        \"kms:TagResource\",\n                        \"kms:UntagResource\",\n                        \"kms:ScheduleKeyDeletion\",\n                        \"kms:CancelKeyDeletion\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"AWS\": {\n                            \"Fn::Sub\": \"arn:aws:iam::${AWS::AccountId}:user/${kmsKeyAdmin}\"\n                        }\n                    },\n                    \"Resource\": \"*\",\n                    \"Sid\": \"Allow access for Key Administrators\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::KMS::Key\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#kmsuseralias","title":"kmsUserAlias","text":"DetailsSource Property Value Type AWS::KMS::Alias Group KMS Resources (KMS) Description User KMS key alias for lava realm. <pre><code>{\n    \"Properties\": {\n        \"AliasName\": {\n            \"Fn::Sub\": \"alias/lava-${realm}-user\"\n        },\n        \"TargetKeyId\": {\n            \"Ref\": \"kmsUserKey\"\n        }\n    },\n    \"Type\": \"AWS::KMS::Alias\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#kmsuserkey","title":"kmsUserKey","text":"DetailsSource Property Value Type AWS::KMS::Key Group KMS Resources (KMS) Description User KMS key for lava realm. <pre><code>{\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"User key for lava realm ${realm}\"\n        },\n        \"EnableKeyRotation\": true,\n        \"Enabled\": true,\n        \"KeyPolicy\": {\n            \"Id\": \"key-consolepolicy-3\",\n            \"Statement\": [\n                {\n                    \"Action\": \"kms:*\",\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"AWS\": {\n                            \"Fn::Sub\": \"arn:aws:iam::${AWS::AccountId}:root\"\n                        }\n                    },\n                    \"Resource\": \"*\",\n                    \"Sid\": \"Enable IAM User Permissions\"\n                },\n                {\n                    \"Action\": [\n                        \"kms:Create*\",\n                        \"kms:Describe*\",\n                        \"kms:Enable*\",\n                        \"kms:List*\",\n                        \"kms:Put*\",\n                        \"kms:Update*\",\n                        \"kms:Revoke*\",\n                        \"kms:Disable*\",\n                        \"kms:Get*\",\n                        \"kms:Delete*\",\n                        \"kms:TagResource\",\n                        \"kms:UntagResource\",\n                        \"kms:ScheduleKeyDeletion\",\n                        \"kms:CancelKeyDeletion\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"AWS\": {\n                            \"Fn::Sub\": \"arn:aws:iam::${AWS::AccountId}:user/${kmsKeyAdmin}\"\n                        }\n                    },\n                    \"Resource\": \"*\",\n                    \"Sid\": \"Allow access for Key Administrators\"\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        }\n    },\n    \"Type\": \"AWS::KMS::Key\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lambdadispatch","title":"lambdaDispatch","text":"DetailsSource Property Value Type AWS::Lambda::Function Group Lambda Resources (Lambda) Description Dispatch Lambda function for the realm. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Architectures\": [\n            {\n                \"Ref\": \"lambdaArchitecture\"\n            }\n        ],\n        \"Code\": {\n            \"S3Bucket\": {\n                \"Ref\": \"s3CodeBucket\"\n            },\n            \"S3Key\": {\n                \"Fn::Sub\": \"${s3CodePrefix}/_dist_/lambda/dispatch-${lambdaVersion}.zip\"\n            }\n        },\n        \"Description\": {\n            \"Fn::Sub\": \"Dispatch Lambda function for the lava ${realm} realm.\"\n        },\n        \"Environment\": {\n            \"Variables\": {\n                \"LAVA_REALM\": {\n                    \"Ref\": \"realm\"\n                },\n                \"LOGLEVEL\": \"info\"\n            }\n        },\n        \"FunctionName\": {\n            \"Fn::Sub\": \"lava-${realm}-dispatch\"\n        },\n        \"Handler\": \"dispatch.lambda_handler\",\n        \"MemorySize\": {\n            \"Ref\": \"lambdaMemory\"\n        },\n        \"Role\": {\n            \"Fn::GetAtt\": [\n                \"iamLavaDispatchLambdaRole\",\n                \"Arn\"\n            ]\n        },\n        \"Runtime\": {\n            \"Ref\": \"lambdaRuntime\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:function\",\n                \"Value\": \"realm.dispatch-helper\"\n            }\n        ],\n        \"Timeout\": {\n            \"Ref\": \"lambdaTimeout\"\n        }\n    },\n    \"Type\": \"AWS::Lambda::Function\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lambdalavametrics","title":"lambdaLavaMetrics","text":"DetailsSource Property Value Type AWS::Lambda::Function Group Lambda Resources (Lambda) Description Lambda function to produce CloudWatch metrics. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Architectures\": [\n            {\n                \"Ref\": \"lambdaArchitecture\"\n            }\n        ],\n        \"Code\": {\n            \"S3Bucket\": {\n                \"Ref\": \"s3CodeBucket\"\n            },\n            \"S3Key\": {\n                \"Fn::Sub\": \"${s3CodePrefix}/_dist_/lambda/metrics-${lambdaVersion}.zip\"\n            }\n        },\n        \"Description\": {\n            \"Fn::Sub\": \"Lambda function to produce CloudWatch metrics for the lava ${realm} realm.\"\n        },\n        \"Environment\": {\n            \"Variables\": {\n                \"LAVA_REALM\": {\n                    \"Ref\": \"realm\"\n                },\n                \"LOGLEVEL\": \"info\"\n            }\n        },\n        \"FunctionName\": {\n            \"Fn::Sub\": \"lava-${realm}-metrics\"\n        },\n        \"Handler\": \"metrics.lambda_handler\",\n        \"MemorySize\": {\n            \"Ref\": \"lambdaMemory\"\n        },\n        \"Role\": {\n            \"Fn::GetAtt\": [\n                \"iamLavaMetricsLambdaRole\",\n                \"Arn\"\n            ]\n        },\n        \"Runtime\": {\n            \"Ref\": \"lambdaRuntime\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:function\",\n                \"Value\": \"realm.metrics\"\n            }\n        ],\n        \"Timeout\": {\n            \"Ref\": \"lambdaTimeout\"\n        }\n    },\n    \"Type\": \"AWS::Lambda::Function\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lambdalavametricspermission","title":"lambdaLavaMetricsPermission","text":"DetailsSource Property Value Type AWS::Lambda::Permission Group Lambda Resources (Lambda) Description Allow EventBridge to run the schedule for the metrics lambda. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Action\": \"lambda:InvokeFunction\",\n        \"FunctionName\": {\n            \"Fn::GetAtt\": [\n                \"lambdaLavaMetrics\",\n                \"Arn\"\n            ]\n        },\n        \"Principal\": \"events.amazonaws.com\",\n        \"SourceArn\": {\n            \"Fn::GetAtt\": [\n                \"eventsScheduleMetricsLambda\",\n                \"Arn\"\n            ]\n        }\n    },\n    \"Type\": \"AWS::Lambda::Permission\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lambdalavastop","title":"lambdaLavaStop","text":"DetailsSource Property Value Type AWS::Lambda::Function Group Lambda Resources (Lambda) Description Lambda function to stop daemons on a worker node <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Architectures\": [\n            {\n                \"Ref\": \"lambdaArchitecture\"\n            }\n        ],\n        \"Code\": {\n            \"S3Bucket\": {\n                \"Ref\": \"s3CodeBucket\"\n            },\n            \"S3Key\": {\n                \"Fn::Sub\": \"${s3CodePrefix}/_dist_/lambda/stop-${lambdaVersion}.zip\"\n            }\n        },\n        \"Description\": {\n            \"Fn::Sub\": \"Worker node stop Lambda function for the lava ${realm} realm.\"\n        },\n        \"Environment\": {\n            \"Variables\": {\n                \"LAVA_REALM\": {\n                    \"Ref\": \"realm\"\n                },\n                \"LIFECYCLE_HEARTBEAT\": {\n                    \"Fn::Sub\": \"${autoscalingHeartbeatMinutes}m\"\n                },\n                \"LOGLEVEL\": \"info\",\n                \"WAIT_TIME\": {\n                    \"Fn::Sub\": \"${workerStopMinutes}m\"\n                }\n            }\n        },\n        \"FunctionName\": {\n            \"Fn::Sub\": \"lava-${realm}-stop\"\n        },\n        \"Handler\": \"stop.lambda_handler\",\n        \"MemorySize\": {\n            \"Ref\": \"lambdaMemory\"\n        },\n        \"Role\": {\n            \"Fn::GetAtt\": [\n                \"iamLavaStopLambdaRole\",\n                \"Arn\"\n            ]\n        },\n        \"Runtime\": {\n            \"Ref\": \"lambdaRuntime\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:function\",\n                \"Value\": \"realm.worker-stop\"\n            }\n        ],\n        \"Timeout\": {\n            \"Ref\": \"lambdaTimeout\"\n        }\n    },\n    \"Type\": \"AWS::Lambda::Function\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lambdas3trigger","title":"lambdaS3Trigger","text":"DetailsSource Property Value Type AWS::Lambda::Function Group Lambda Resources (Lambda) Description S3trigger Lambda function for the realm. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Architectures\": [\n            {\n                \"Ref\": \"lambdaArchitecture\"\n            }\n        ],\n        \"Code\": {\n            \"S3Bucket\": {\n                \"Ref\": \"s3CodeBucket\"\n            },\n            \"S3Key\": {\n                \"Fn::Sub\": \"${s3CodePrefix}/_dist_/lambda/s3trigger-${lambdaVersion}.zip\"\n            }\n        },\n        \"Description\": {\n            \"Fn::Sub\": \"S3trigger Lambda function for the lava ${realm} realm.\"\n        },\n        \"Environment\": {\n            \"Variables\": {\n                \"LAVA_REALM\": {\n                    \"Ref\": \"realm\"\n                },\n                \"LOGLEVEL\": \"info\"\n            }\n        },\n        \"FunctionName\": {\n            \"Fn::Sub\": \"lava-${realm}-s3trigger\"\n        },\n        \"Handler\": \"s3trigger.lambda_handler\",\n        \"MemorySize\": {\n            \"Ref\": \"lambdaMemory\"\n        },\n        \"Role\": {\n            \"Fn::GetAtt\": [\n                \"iamLavaDispatchLambdaRole\",\n                \"Arn\"\n            ]\n        },\n        \"Runtime\": {\n            \"Ref\": \"lambdaRuntime\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:function\",\n                \"Value\": \"realm.s3trigger\"\n            }\n        ],\n        \"Timeout\": {\n            \"Ref\": \"lambdaTimeout\"\n        }\n    },\n    \"Type\": \"AWS::Lambda::Function\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lambdas3triggerpermission","title":"lambdaS3TriggerPermission","text":"DetailsSource Property Value Type AWS::Lambda::Permission Group Lambda Resources (Lambda) Description Allow EventBridge rules to invoke the s3trigger lambda. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Action\": \"lambda:InvokeFunction\",\n        \"FunctionName\": {\n            \"Fn::GetAtt\": [\n                \"lambdaS3Trigger\",\n                \"Arn\"\n            ]\n        },\n        \"Principal\": \"events.amazonaws.com\",\n        \"SourceAccount\": {\n            \"Ref\": \"AWS::AccountId\"\n        },\n        \"SourceArn\": {\n            \"Fn::Sub\": \"arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/lava.${realm}.*\"\n        }\n    },\n    \"Type\": \"AWS::Lambda::Permission\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lamdadispatchpolicy","title":"lamdaDispatchPolicy","text":"DetailsSource Property Value Type AWS::Lambda::Permission Group Lambda Resources (Lambda) <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"DependsOn\": \"lambdaDispatch\",\n    \"Properties\": {\n        \"Action\": \"lambda:InvokeFunction\",\n        \"FunctionName\": {\n            \"Fn::Sub\": \"lava-${realm}-dispatch\"\n        },\n        \"Principal\": \"sns.amazonaws.com\",\n        \"SourceArn\": {\n            \"Ref\": \"snsDispatchHelper\"\n        }\n    },\n    \"Type\": \"AWS::Lambda::Permission\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#s3lavabucket","title":"s3LavaBucket","text":"DetailsSource Property Value Type AWS::S3::Bucket Group S3 Resources (S3) Description Lava realm bucket for payloads and tmp space. <pre><code>{\n    \"Properties\": {\n        \"AccessControl\": \"Private\",\n        \"BucketEncryption\": {\n            \"ServerSideEncryptionConfiguration\": [\n                {\n                    \"ServerSideEncryptionByDefault\": {\n                        \"KMSMasterKeyID\": {\n                            \"Ref\": \"kmsUserKey\"\n                        },\n                        \"SSEAlgorithm\": \"aws:kms\"\n                    }\n                }\n            ]\n        },\n        \"BucketName\": {\n            \"Ref\": \"lavaBucketName\"\n        },\n        \"LifecycleConfiguration\": {\n            \"Rules\": [\n                {\n                    \"AbortIncompleteMultipartUpload\": {\n                        \"DaysAfterInitiation\": 1\n                    },\n                    \"Id\": \"AbortIncompleteMultipartUpload\",\n                    \"Status\": \"Enabled\"\n                },\n                {\n                    \"ExpirationInDays\": {\n                        \"Ref\": \"tmpExpiryDays\"\n                    },\n                    \"Id\": \"Expire tmp area\",\n                    \"Prefix\": \"tmp/\",\n                    \"Status\": \"Enabled\"\n                }\n            ]\n        },\n        \"LoggingConfiguration\": {\n            \"DestinationBucketName\": {\n                \"Ref\": \"logBucketName\"\n            },\n            \"LogFilePrefix\": {\n                \"Fn::Sub\": \"${lavaBucketName}/\"\n            }\n        },\n        \"PublicAccessBlockConfiguration\": {\n            \"BlockPublicAcls\": true,\n            \"BlockPublicPolicy\": true,\n            \"IgnorePublicAcls\": true,\n            \"RestrictPublicBuckets\": true\n        }\n    },\n    \"Type\": \"AWS::S3::Bucket\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#snsdispatchhelper","title":"snsDispatchHelper","text":"DetailsSource Property Value Type AWS::SNS::Topic Group SNS Resources (SNS) Description SNS topic for the dispatch helper lambda to receive dispatch requests. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"DisplayName\": \"LavaDsptch\",\n        \"KmsMasterKeyId\": {\n            \"Ref\": \"kmsUserAlias\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:function\",\n                \"Value\": \"realm.dispatch-helper\"\n            }\n        ],\n        \"TopicName\": {\n            \"Fn::Sub\": \"lava-${realm}-dispatch\"\n        }\n    },\n    \"Type\": \"AWS::SNS::Topic\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#snslavanotices","title":"snsLavaNotices","text":"DetailsSource Property Value Type AWS::SNS::Topic Group SNS Resources (SNS) Description SNS topic for lava notices. Whether or not it is used depends on lava jobs. <pre><code>{\n    \"Properties\": {\n        \"DisplayName\": \"LavaNotice\",\n        \"Tags\": [\n            {\n                \"Key\": \"lava:function\",\n                \"Value\": \"realm.notices\"\n            }\n        ],\n        \"TopicName\": {\n            \"Fn::Sub\": \"lava-${realm}-notices\"\n        }\n    },\n    \"Type\": \"AWS::SNS::Topic\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#snssubscribedispatch","title":"snsSubscribeDispatch","text":"DetailsSource Property Value Type AWS::SNS::Subscription Group SNS Resources (SNS) Description Subscription for the dispatch helper Lambda to the dispatch helper SNS topic. <pre><code>{\n    \"Condition\": \"ifCreateLambdas\",\n    \"Properties\": {\n        \"Endpoint\": {\n            \"Fn::GetAtt\": [\n                \"lambdaDispatch\",\n                \"Arn\"\n            ]\n        },\n        \"Protocol\": \"lambda\",\n        \"TopicArn\": {\n            \"Ref\": \"snsDispatchHelper\"\n        }\n    },\n    \"Type\": \"AWS::SNS::Subscription\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lava-workercfnjson","title":"lava-worker.cfn.json","text":"<p>This template builds the core components for a new Lava worker. The realm must have been created previously.</p>"},{"location":"19-cloudformation-templates.html#parameters_2","title":"Parameters","text":"Parameter Type Description Version String Lava version (read only). alarmTopic String Name of SNS topic for alarms. SNS topic name must be specified. amiId String Image ID for the latest lava (preferred) or SAK AMI. autoscalingActionTopic String Name of SNS topic for normal autoscaling activity. autoscalingControlledTermination String Autoscaling controlled termination on worker nodes? createHeartBeatAlarm String Should I create a worker heartbeat alarm? createWorkerInstance String Should I create resources required for a worker compute instance? dockerVolumeSize Number Size in GB of volume for docker. Set to 0 to remove. Volume size must be between 0 and 2000 GB. keyPairName AWS::EC2::KeyPair::KeyName Key pair name for the instances. Key pair name must be specified. maxAllowedQueueDepth Number Create an alarm if queue depth exceeds this value. 0 for no alarm. messageRetentionPeriod Number Message retention period on the worker queue (seconds). Default 1 day. Must be &gt;= 1800 (30 minutes). queueDepthMinutes Number Minutes queue depth exceeds max before alarm (1..300). realm String Name of the realm realmLambdasDeployed String Are the realm lambda functions deployed? rootVolumeSize Number Size in GB of root volume. Set to 0 for AMI default. Volume size must be &gt;=0. secGroups List&lt;AWS::EC2::SecurityGroup::Id&gt; Security groups for the worker. subnets List&lt;AWS::EC2::Subnet::Id&gt; Subnets for worker instances. swapSize Number Swap size in Gibibytes (0 = no swapping). Swap size must be &gt;= 0. tmpVolumeSize Number Size in GB of second volume mounted on /tmp. Set to 0 to remove. Volume size must be between 0 and 2000 GB. visibilityTimeout Number Visibility timeout on the worker queue (seconds). Default 1 hour. Must be &gt;= 300 (5 minutes). worker String Name of the worker workerAllowedInstances CommaDelimitedList Comma separated list of allowed instance types (GLOBs allowed) workerBacklogScalingTarget Number Autoscaling worker backlog (0 == disable backlog scaling) workerBurstable String Include burstable instance types (e.g. t-series) for workers (included / excluded / required) workerInstanceType String EC2 instance type. Leave blank for capability based provisioning. workerInstancesDesired Number How many worker nodes should I run now? Must be between 0 and 10. workerInstancesMax Number Maximum number of worker instances (must be 1 for dispatcher nodes). Must be between 0 and 10. workerInstancesMin Number Minimum number of worker instances. Must be between 0 and 5. workerLocalStorage String Local (not EBS) storage for workers (included / excluded / required) workerMemoryMax Number Maximum memory in MiB (0 == no limit) workerMemoryMin Number Minimum memory in MiB for the worker workerPublicIp String Assign public IP to workers. workerVCpuMax Number Maximum number of vCPUs for the worker (0 == no limit) workerVCpuMin Number Minimum number of vCPUs for the worker"},{"location":"19-cloudformation-templates.html#outputs_2","title":"Outputs","text":"Id Export as Description Version <code>{\"Fn::Sub\": \"lava:${realm}:${worker}:version\"}</code> Lava version"},{"location":"19-cloudformation-templates.html#resources_2","title":"Resources","text":"<ul> <li>CloudWatch Resources</li> <li>AutoScaling Resources</li> <li>Events Resources</li> <li>IAM Resources</li> <li>Lambda Resources</li> <li>EC2 Resources</li> <li>Logs Resources</li> <li>SQS Resources</li> </ul>"},{"location":"19-cloudformation-templates.html#cloudwatch-resources","title":"CloudWatch Resources","text":"Id Type Description alarmLavaHeartbeat Alarm Heartbeat alarm for Lava worker. alarmLavaWorker Alarm Status check failed alarm for Lava worker. alarmQueueDepth Alarm Queue depth alarm for Lava worker."},{"location":"19-cloudformation-templates.html#autoscaling-resources","title":"AutoScaling Resources","text":"Id Type Description asgLavaWorker AutoScalingGroup Auto scaling group for Lava worker asgWorkerScalingPolicy ScalingPolicy Target tracking scaling policy for worker"},{"location":"19-cloudformation-templates.html#events-resources_1","title":"Events Resources","text":"Id Type Description eventsWorkerTerminating Rule EventBridge rule for terminating worker."},{"location":"19-cloudformation-templates.html#iam-resources_1","title":"IAM Resources","text":"Id Type Description iamLavaWorkerRole Role IAM role for worker instances. iamWorkerInstanceProfile InstanceProfile"},{"location":"19-cloudformation-templates.html#lambda-resources_1","title":"Lambda Resources","text":"Id Type Description lambdaLavaStopPermission Permission Allow EventBridge to run the stop lambda."},{"location":"19-cloudformation-templates.html#ec2-resources","title":"EC2 Resources","text":"Id Type Description launchtemplateLavaWorker LaunchTemplate Launch template for Lava worker."},{"location":"19-cloudformation-templates.html#logs-resources_1","title":"Logs Resources","text":"Id Type Description logFilterWorkerHeartbeat MetricFilter Metric filter on /var/log/lava/&lt;REALM&gt; log group to find heartbeat messages."},{"location":"19-cloudformation-templates.html#sqs-resources","title":"SQS Resources","text":"Id Type Description sqsWorkerQueue Queue SQS queue for the lava worker to receive dispatched jobs."},{"location":"19-cloudformation-templates.html#resource-details_2","title":"Resource Details","text":""},{"location":"19-cloudformation-templates.html#alarmlavaheartbeat","title":"alarmLavaHeartbeat","text":"DetailsSource Property Value Type AWS::CloudWatch::Alarm Group CloudWatch Resources (CloudWatch) Description Heartbeat alarm for Lava worker. <pre><code>{\n    \"Condition\": \"ifCreateHeatbeatAlarm\",\n    \"Properties\": {\n        \"ActionsEnabled\": \"true\",\n        \"AlarmActions\": [\n            {\n                \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:${alarmTopic}\"\n            }\n        ],\n        \"AlarmDescription\": {\n            \"Fn::Sub\": \"No heartbeat for lava-${realm}-${worker}\"\n        },\n        \"AlarmName\": {\n            \"Fn::Sub\": \"lava-${realm}-${worker}-heartbeat\"\n        },\n        \"ComparisonOperator\": \"LessThanThreshold\",\n        \"DatapointsToAlarm\": 5,\n        \"Dimensions\": [],\n        \"EvaluationPeriods\": 5,\n        \"InsufficientDataActions\": [],\n        \"MetricName\": {\n            \"Fn::Sub\": \"lava-${realm}-${worker}-heartbeat\"\n        },\n        \"Namespace\": \"LogMetrics\",\n        \"OKActions\": [\n            {\n                \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:${alarmTopic}\"\n            }\n        ],\n        \"Period\": 60,\n        \"Statistic\": \"SampleCount\",\n        \"Threshold\": \"1.0\",\n        \"TreatMissingData\": \"breaching\"\n    },\n    \"Type\": \"AWS::CloudWatch::Alarm\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#alarmlavaworker","title":"alarmLavaWorker","text":"DetailsSource Property Value Type AWS::CloudWatch::Alarm Group CloudWatch Resources (CloudWatch) Description Status check failed alarm for Lava worker. <pre><code>{\n    \"Condition\": \"ifCreateInstance\",\n    \"Properties\": {\n        \"ActionsEnabled\": \"true\",\n        \"AlarmActions\": [\n            {\n                \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:${alarmTopic}\"\n            }\n        ],\n        \"AlarmDescription\": {\n            \"Fn::Sub\": \"Lava realm ${realm}, worker ${worker} status check failed for 2 minutes\"\n        },\n        \"AlarmName\": {\n            \"Fn::Sub\": \"lava-${realm}-${worker} Status Check Failed\"\n        },\n        \"ComparisonOperator\": \"GreaterThanThreshold\",\n        \"Dimensions\": [\n            {\n                \"Name\": \"AutoScalingGroupName\",\n                \"Value\": {\n                    \"Ref\": \"asgLavaWorker\"\n                }\n            }\n        ],\n        \"EvaluationPeriods\": 2,\n        \"InsufficientDataActions\": [],\n        \"MetricName\": \"StatusCheckFailed\",\n        \"Namespace\": \"AWS/EC2\",\n        \"OKActions\": [\n            {\n                \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:${alarmTopic}\"\n            }\n        ],\n        \"Period\": 60,\n        \"Statistic\": \"Average\",\n        \"Threshold\": \"0.0\",\n        \"TreatMissingData\": \"notBreaching\"\n    },\n    \"Type\": \"AWS::CloudWatch::Alarm\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#alarmqueuedepth","title":"alarmQueueDepth","text":"DetailsSource Property Value Type AWS::CloudWatch::Alarm Group CloudWatch Resources (CloudWatch) Description Queue depth alarm for Lava worker. <pre><code>{\n    \"Condition\": \"ifCreateQueueDepthAlarm\",\n    \"Properties\": {\n        \"ActionsEnabled\": \"true\",\n        \"AlarmActions\": [\n            {\n                \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:${alarmTopic}\"\n            }\n        ],\n        \"AlarmDescription\": {\n            \"Fn::Sub\": \"Too many messages queued for lava-${realm}-${worker}\"\n        },\n        \"AlarmName\": {\n            \"Fn::Sub\": \"lava-${realm}-${worker}-queue-depth\"\n        },\n        \"ComparisonOperator\": \"GreaterThanThreshold\",\n        \"DatapointsToAlarm\": {\n            \"Ref\": \"queueDepthMinutes\"\n        },\n        \"Dimensions\": [\n            {\n                \"Name\": \"QueueName\",\n                \"Value\": {\n                    \"Fn::Sub\": \"lava-${realm}-${worker}\"\n                }\n            }\n        ],\n        \"EvaluationPeriods\": {\n            \"Ref\": \"queueDepthMinutes\"\n        },\n        \"InsufficientDataActions\": [],\n        \"MetricName\": \"ApproximateNumberOfMessagesVisible\",\n        \"Namespace\": \"AWS/SQS\",\n        \"OKActions\": [\n            {\n                \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:${alarmTopic}\"\n            }\n        ],\n        \"Period\": 60,\n        \"Statistic\": \"Maximum\",\n        \"Threshold\": {\n            \"Fn::Sub\": \"${maxAllowedQueueDepth}.0\"\n        },\n        \"TreatMissingData\": \"ignore\"\n    },\n    \"Type\": \"AWS::CloudWatch::Alarm\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#asglavaworker","title":"asgLavaWorker","text":"DetailsSource Property Value Type AWS::AutoScaling::AutoScalingGroup Group AutoScaling Resources (AutoScaling) Description Auto scaling group for Lava worker <pre><code>{\n    \"Condition\": \"ifCreateInstance\",\n    \"Properties\": {\n        \"AutoScalingGroupName\": {\n            \"Fn::Sub\": \"lava-${realm}-${worker}\"\n        },\n        \"DesiredCapacity\": {\n            \"Ref\": \"workerInstancesDesired\"\n        },\n        \"HealthCheckGracePeriod\": 300,\n        \"LaunchTemplate\": {\n            \"Fn::If\": [\n                \"ifInstanceType\",\n                {\n                    \"LaunchTemplateId\": {\n                        \"Ref\": \"launchtemplateLavaWorker\"\n                    },\n                    \"Version\": {\n                        \"Fn::GetAtt\": [\n                            \"launchtemplateLavaWorker\",\n                            \"LatestVersionNumber\"\n                        ]\n                    }\n                },\n                {\n                    \"Ref\": \"AWS::NoValue\"\n                }\n            ]\n        },\n        \"LifecycleHookSpecificationList\": [\n            {\n                \"DefaultResult\": \"CONTINUE\",\n                \"HeartbeatTimeout\": 1800,\n                \"LifecycleHookName\": \"terminate\",\n                \"LifecycleTransition\": \"autoscaling:EC2_INSTANCE_TERMINATING\",\n                \"NotificationMetadata\": {\n                    \"Fn::Sub\": \"{\\\"realm\\\": \\\"${realm}\\\",\\\"worker\\\":\\\"${worker}\\\"}\"\n                }\n            }\n        ],\n        \"MaxSize\": {\n            \"Ref\": \"workerInstancesMax\"\n        },\n        \"MetricsCollection\": [\n            {\n                \"Granularity\": \"1Minute\"\n            }\n        ],\n        \"MinSize\": {\n            \"Ref\": \"workerInstancesMin\"\n        },\n        \"MixedInstancesPolicy\": {\n            \"Fn::If\": [\n                \"ifInstanceType\",\n                {\n                    \"Ref\": \"AWS::NoValue\"\n                },\n                {\n                    \"InstancesDistribution\": {\n                        \"OnDemandAllocationStrategy\": \"lowest-price\",\n                        \"OnDemandBaseCapacity\": 0,\n                        \"OnDemandPercentageAboveBaseCapacity\": 100\n                    },\n                    \"LaunchTemplate\": {\n                        \"LaunchTemplateSpecification\": {\n                            \"LaunchTemplateId\": {\n                                \"Ref\": \"launchtemplateLavaWorker\"\n                            },\n                            \"Version\": {\n                                \"Fn::GetAtt\": [\n                                    \"launchtemplateLavaWorker\",\n                                    \"LatestVersionNumber\"\n                                ]\n                            }\n                        },\n                        \"Overrides\": [\n                            {\n                                \"InstanceRequirements\": {\n                                    \"AllowedInstanceTypes\": {\n                                        \"Ref\": \"workerAllowedInstances\"\n                                    },\n                                    \"BurstablePerformance\": {\n                                        \"Ref\": \"workerBurstable\"\n                                    },\n                                    \"InstanceGenerations\": [\n                                        \"current\",\n                                        \"previous\"\n                                    ],\n                                    \"LocalStorage\": {\n                                        \"Ref\": \"workerLocalStorage\"\n                                    },\n                                    \"MemoryMiB\": {\n                                        \"Max\": {\n                                            \"Fn::If\": [\n                                                \"ifMaxMemory\",\n                                                {\n                                                    \"Ref\": \"workerMemoryMax\"\n                                                },\n                                                {\n                                                    \"Ref\": \"AWS::NoValue\"\n                                                }\n                                            ]\n                                        },\n                                        \"Min\": {\n                                            \"Ref\": \"workerMemoryMin\"\n                                        }\n                                    },\n                                    \"VCpuCount\": {\n                                        \"Max\": {\n                                            \"Fn::If\": [\n                                                \"ifMaxVCpu\",\n                                                {\n                                                    \"Ref\": \"workerVCpuMax\"\n                                                },\n                                                {\n                                                    \"Ref\": \"AWS::NoValue\"\n                                                }\n                                            ]\n                                        },\n                                        \"Min\": {\n                                            \"Ref\": \"workerVCpuMin\"\n                                        }\n                                    }\n                                }\n                            }\n                        ]\n                    }\n                }\n            ]\n        },\n        \"NotificationConfigurations\": [\n            {\n                \"NotificationTypes\": [\n                    \"autoscaling:EC2_INSTANCE_LAUNCH_ERROR\",\n                    \"autoscaling:EC2_INSTANCE_TERMINATE_ERROR\",\n                    \"autoscaling:TEST_NOTIFICATION\"\n                ],\n                \"TopicARN\": {\n                    \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:${alarmTopic}\"\n                }\n            },\n            {\n                \"NotificationTypes\": [\n                    \"autoscaling:EC2_INSTANCE_LAUNCH\",\n                    \"autoscaling:EC2_INSTANCE_TERMINATE\",\n                    \"autoscaling:TEST_NOTIFICATION\"\n                ],\n                \"TopicARN\": {\n                    \"Fn::Sub\": \"arn:aws:sns:${AWS::Region}:${AWS::AccountId}:${autoscalingActionTopic}\"\n                }\n            }\n        ],\n        \"Tags\": [\n            {\n                \"Key\": \"Name\",\n                \"PropagateAtLaunch\": \"true\",\n                \"Value\": {\n                    \"Fn::Sub\": \"lava-${realm}-${worker}\"\n                }\n            },\n            {\n                \"Key\": \"LavaRealm\",\n                \"PropagateAtLaunch\": \"true\",\n                \"Value\": {\n                    \"Ref\": \"realm\"\n                }\n            },\n            {\n                \"Key\": \"LavaWorker\",\n                \"PropagateAtLaunch\": \"true\",\n                \"Value\": {\n                    \"Ref\": \"worker\"\n                }\n            }\n        ],\n        \"VPCZoneIdentifier\": {\n            \"Ref\": \"subnets\"\n        }\n    },\n    \"Type\": \"AWS::AutoScaling::AutoScalingGroup\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#asgworkerscalingpolicy","title":"asgWorkerScalingPolicy","text":"DetailsSource Property Value Type AWS::AutoScaling::ScalingPolicy Group AutoScaling Resources (AutoScaling) Description Target tracking scaling policy for worker <pre><code>{\n    \"Condition\": \"ifWorkerBacklogScaling\",\n    \"Properties\": {\n        \"AutoScalingGroupName\": {\n            \"Ref\": \"asgLavaWorker\"\n        },\n        \"EstimatedInstanceWarmup\": 240,\n        \"PolicyType\": \"TargetTrackingScaling\",\n        \"TargetTrackingConfiguration\": {\n            \"CustomizedMetricSpecification\": {\n                \"Dimensions\": [\n                    {\n                        \"Name\": \"Realm\",\n                        \"Value\": {\n                            \"Ref\": \"realm\"\n                        }\n                    },\n                    {\n                        \"Name\": \"Worker\",\n                        \"Value\": {\n                            \"Ref\": \"worker\"\n                        }\n                    }\n                ],\n                \"MetricName\": \"WorkerBacklog\",\n                \"Namespace\": \"Lava\",\n                \"Statistic\": \"Average\"\n            },\n            \"TargetValue\": {\n                \"Ref\": \"workerBacklogScalingTarget\"\n            }\n        }\n    },\n    \"Type\": \"AWS::AutoScaling::ScalingPolicy\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#eventsworkerterminating","title":"eventsWorkerTerminating","text":"DetailsSource Property Value Type AWS::Events::Rule Group Events Resources (Events) Description EventBridge rule for terminating worker. <pre><code>{\n    \"Condition\": \"ifRealmLambdasDeployed\",\n    \"Properties\": {\n        \"Description\": {\n            \"Fn::Sub\": \"lava-${realm}-${worker} worker node is terminating\"\n        },\n        \"EventBusName\": \"default\",\n        \"EventPattern\": {\n            \"Fn::Sub\": \"{\\\"source\\\": [\\\"aws.autoscaling\\\"], \\\"detail-type\\\": [\\\"EC2 Instance-terminate Lifecycle Action\\\"], \\\"detail\\\": {\\\"AutoScalingGroupName\\\": [\\\"lava-${realm}-${worker}\\\"] }}\"\n        },\n        \"Name\": {\n            \"Fn::Sub\": \"lava-${realm}-${worker}-terminating\"\n        },\n        \"State\": {\n            \"Ref\": \"autoscalingControlledTermination\"\n        },\n        \"Targets\": [\n            {\n                \"Arn\": {\n                    \"Fn::ImportValue\": {\n                        \"Fn::Sub\": \"lava:${realm}:lambdaLavaStop\"\n                    }\n                },\n                \"Id\": {\n                    \"Fn::Sub\": \"lava-${realm}-${worker}-stop-lambda\"\n                },\n                \"InputPath\": \"$.detail\"\n            },\n            {\n                \"Arn\": {\n                    \"Fn::Sub\": \"arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/events/lava\"\n                },\n                \"Id\": {\n                    \"Fn::Sub\": \"lava-${realm}-${worker}-log-events\"\n                }\n            }\n        ]\n    },\n    \"Type\": \"AWS::Events::Rule\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamlavaworkerrole","title":"iamLavaWorkerRole","text":"DetailsSource Property Value Type AWS::IAM::Role Group IAM Resources (IAM) Description IAM role for worker instances. <pre><code>{\n    \"Condition\": \"ifCreateInstance\",\n    \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n            \"Statement\": [\n                {\n                    \"Action\": \"sts:AssumeRole\",\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"Service\": \"ec2.amazonaws.com\"\n                    }\n                }\n            ],\n            \"Version\": \"2012-10-17\"\n        },\n        \"ManagedPolicyArns\": [\n            {\n                \"Fn::ImportValue\": {\n                    \"Fn::Sub\": \"lava:${realm}:iamLavaWorkerPolicyArn\"\n                }\n            },\n            \"arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\",\n            \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n        ],\n        \"RoleName\": {\n            \"Fn::Sub\": \"lava-${realm}-worker-${worker}\"\n        }\n    },\n    \"Type\": \"AWS::IAM::Role\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#iamworkerinstanceprofile","title":"iamWorkerInstanceProfile","text":"DetailsSource Property Value Type AWS::IAM::InstanceProfile Group IAM Resources (IAM) <pre><code>{\n    \"Condition\": \"ifCreateInstance\",\n    \"Properties\": {\n        \"Roles\": [\n            {\n                \"Ref\": \"iamLavaWorkerRole\"\n            }\n        ]\n    },\n    \"Type\": \"AWS::IAM::InstanceProfile\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#lambdalavastoppermission","title":"lambdaLavaStopPermission","text":"DetailsSource Property Value Type AWS::Lambda::Permission Group Lambda Resources (Lambda) Description Allow EventBridge to run the stop lambda. <pre><code>{\n    \"Condition\": \"ifRealmLambdasDeployed\",\n    \"Properties\": {\n        \"Action\": \"lambda:InvokeFunction\",\n        \"FunctionName\": {\n            \"Fn::ImportValue\": {\n                \"Fn::Sub\": \"lava:${realm}:lambdaLavaStop\"\n            }\n        },\n        \"Principal\": \"events.amazonaws.com\",\n        \"SourceArn\": {\n            \"Fn::GetAtt\": [\n                \"eventsWorkerTerminating\",\n                \"Arn\"\n            ]\n        }\n    },\n    \"Type\": \"AWS::Lambda::Permission\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#launchtemplatelavaworker","title":"launchtemplateLavaWorker","text":"DetailsSource Property Value Type AWS::EC2::LaunchTemplate Group EC2 Resources (EC2) Description Launch template for Lava worker. <pre><code>{\n    \"Condition\": \"ifCreateInstance\",\n    \"Properties\": {\n        \"LaunchTemplateData\": {\n            \"BlockDeviceMappings\": [\n                {\n                    \"Fn::If\": [\n                        \"ifSetSizeRootVol\",\n                        {\n                            \"DeviceName\": \"/dev/xvda\",\n                            \"Ebs\": {\n                                \"DeleteOnTermination\": \"true\",\n                                \"Encrypted\": \"true\",\n                                \"VolumeSize\": {\n                                    \"Ref\": \"rootVolumeSize\"\n                                }\n                            }\n                        },\n                        {\n                            \"Ref\": \"AWS::NoValue\"\n                        }\n                    ]\n                },\n                {\n                    \"Fn::If\": [\n                        \"ifCreateTmpVol\",\n                        {\n                            \"DeviceName\": \"/dev/xvdb\",\n                            \"Ebs\": {\n                                \"DeleteOnTermination\": \"true\",\n                                \"Encrypted\": \"true\",\n                                \"VolumeSize\": {\n                                    \"Ref\": \"tmpVolumeSize\"\n                                }\n                            }\n                        },\n                        {\n                            \"Ref\": \"AWS::NoValue\"\n                        }\n                    ]\n                },\n                {\n                    \"Fn::If\": [\n                        \"ifCreateDockerVol\",\n                        {\n                            \"DeviceName\": \"/dev/xvdc\",\n                            \"Ebs\": {\n                                \"DeleteOnTermination\": \"true\",\n                                \"Encrypted\": \"true\",\n                                \"VolumeSize\": {\n                                    \"Ref\": \"dockerVolumeSize\"\n                                }\n                            }\n                        },\n                        {\n                            \"Ref\": \"AWS::NoValue\"\n                        }\n                    ]\n                }\n            ],\n            \"IamInstanceProfile\": {\n                \"Name\": {\n                    \"Ref\": \"iamWorkerInstanceProfile\"\n                }\n            },\n            \"ImageId\": {\n                \"Ref\": \"amiId\"\n            },\n            \"InstanceType\": {\n                \"Fn::If\": [\n                    \"ifInstanceType\",\n                    {\n                        \"Ref\": \"workerInstanceType\"\n                    },\n                    {\n                        \"Ref\": \"AWS::NoValue\"\n                    }\n                ]\n            },\n            \"KeyName\": {\n                \"Fn::If\": [\n                    \"ifKeyPair\",\n                    {\n                        \"Ref\": \"keyPairName\"\n                    },\n                    {\n                        \"Ref\": \"AWS::NoValue\"\n                    }\n                ]\n            },\n            \"Monitoring\": {\n                \"Enabled\": true\n            },\n            \"NetworkInterfaces\": [\n                {\n                    \"AssociatePublicIpAddress\": {\n                        \"Ref\": \"workerPublicIp\"\n                    },\n                    \"Description\": {\n                        \"Fn::Sub\": \"lava-${realm}-${worker} eth0\"\n                    },\n                    \"DeviceIndex\": 0,\n                    \"Groups\": {\n                        \"Ref\": \"secGroups\"\n                    }\n                }\n            ],\n            \"UserData\": {\n                \"Fn::Base64\": {\n                    \"Fn::Join\": [\n                        \"\\n\",\n                        [\n                            \"{\",\n                            {\n                                \"Fn::Sub\": [\n                                    \"\\\"shell0\\\": \\\"/usr/local/bin/s3run s3://${s3CodeBucket}/${s3CodePrefix}/${realm}/${worker}/root.boot0.sh s3://${s3CodeBucket}/${s3CodePrefix} ${realm} ${worker}\\\"\",\n                                    {\n                                        \"s3CodeBucket\": {\n                                            \"Fn::ImportValue\": {\n                                                \"Fn::Sub\": \"lava:${realm}:s3CodeBucket\"\n                                            }\n                                        },\n                                        \"s3CodePrefix\": {\n                                            \"Fn::ImportValue\": {\n                                                \"Fn::Sub\": \"lava:${realm}:s3CodePrefix\"\n                                            }\n                                        }\n                                    }\n                                ]\n                            },\n                            \",\",\n                            {\n                                \"Fn::Sub\": [\n                                    \"\\\"shell\\\": \\\"/usr/local/bin/s3run s3://${s3CodeBucket}/${s3CodePrefix}/${realm}/${worker}/root.boot.sh s3://${s3CodeBucket}/${s3CodePrefix} ${realm} ${worker}\\\"\",\n                                    {\n                                        \"s3CodeBucket\": {\n                                            \"Fn::ImportValue\": {\n                                                \"Fn::Sub\": \"lava:${realm}:s3CodeBucket\"\n                                            }\n                                        },\n                                        \"s3CodePrefix\": {\n                                            \"Fn::ImportValue\": {\n                                                \"Fn::Sub\": \"lava:${realm}:s3CodePrefix\"\n                                            }\n                                        }\n                                    }\n                                ]\n                            },\n                            \",\",\n                            {\n                                \"Fn::Sub\": \"\\\"import-users\\\": { \\\"users\\\": \\\"lava-${realm}-admin\\\", \\\"sudoers\\\": \\\"lava-${realm}-admin\\\" }\"\n                            },\n                            \",\",\n                            {\n                                \"Fn::Sub\": \"\\\"swap\\\": { \\\"size\\\": ${swapSize} }\"\n                            },\n                            \"}\"\n                        ]\n                    ]\n                }\n            }\n        },\n        \"TagSpecifications\": [\n            {\n                \"ResourceType\": \"launch-template\",\n                \"Tags\": [\n                    {\n                        \"Key\": \"LavaRealm\",\n                        \"Value\": {\n                            \"Ref\": \"realm\"\n                        }\n                    },\n                    {\n                        \"Key\": \"LavaWorker\",\n                        \"Value\": {\n                            \"Ref\": \"worker\"\n                        }\n                    }\n                ]\n            }\n        ]\n    },\n    \"Type\": \"AWS::EC2::LaunchTemplate\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#logfilterworkerheartbeat","title":"logFilterWorkerHeartbeat","text":"DetailsSource Property Value Type AWS::Logs::MetricFilter Group Logs Resources (Logs) Description Metric filter on /var/log/lava/&lt;REALM&gt; log group to find heartbeat messages. <pre><code>{\n    \"Condition\": \"ifCreateHeatbeatAlarm\",\n    \"Properties\": {\n        \"FilterPattern\": {\n            \"Fn::Sub\": \"{$.event_source=\\\"lava-worker\\\" &amp;&amp; $.event_type=\\\"heartbeat\\\" &amp;&amp; $.realm=\\\"${realm}\\\" &amp;&amp; $.worker=\\\"${worker}\\\"}\"\n        },\n        \"LogGroupName\": {\n            \"Fn::Sub\": \"/var/log/lava/${realm}\"\n        },\n        \"MetricTransformations\": [\n            {\n                \"MetricName\": {\n                    \"Fn::Sub\": \"lava-${realm}-${worker}-heartbeat\"\n                },\n                \"MetricNamespace\": \"LogMetrics\",\n                \"MetricValue\": \"1\"\n            }\n        ]\n    },\n    \"Type\": \"AWS::Logs::MetricFilter\"\n}\n</code></pre>"},{"location":"19-cloudformation-templates.html#sqsworkerqueue","title":"sqsWorkerQueue","text":"DetailsSource Property Value Type AWS::SQS::Queue Group SQS Resources (SQS) Description SQS queue for the lava worker to receive dispatched jobs. <pre><code>{\n    \"Properties\": {\n        \"KmsMasterKeyId\": {\n            \"Fn::ImportValue\": {\n                \"Fn::Sub\": \"lava:${realm}:kmsUserKeyAlias\"\n            }\n        },\n        \"MessageRetentionPeriod\": {\n            \"Ref\": \"messageRetentionPeriod\"\n        },\n        \"QueueName\": {\n            \"Fn::Sub\": \"lava-${realm}-${worker}\"\n        },\n        \"Tags\": [\n            {\n                \"Key\": \"lava:function\",\n                \"Value\": \"worker.dispatch\"\n            }\n        ],\n        \"VisibilityTimeout\": {\n            \"Ref\": \"visibilityTimeout\"\n        }\n    },\n    \"Type\": \"AWS::SQS::Queue\"\n}\n</code></pre>"},{"location":"20-faq.html","title":"Frequently Asked Questions","text":""},{"location":"20-faq.html#scheduling-and-dispatch-faq","title":"Scheduling and Dispatch FAQ","text":""},{"location":"20-faq.html#scheduling","title":"Scheduling","text":""},{"location":"20-faq.html#can-a-job-be-scheduled-to-run-when-a-worker-reboots","title":"Can a job be scheduled to run when a worker reboots?","text":"<p>Yes \u2026 and no.</p> <p>The <code>@reboot</code> crontab specification can be used in a job schedule but what does that actually mean? Given that it will occur on the dispatcher node, any job scheduled on a reboot will only get dispatched when the dispatcher itself reboots. This may be useful, but it's not obvious how.</p>"},{"location":"20-faq.html#jobs-faq","title":"Jobs FAQ","text":"<p>This section covers questions related to jobs in general as well as those specific to particular job types.</p>"},{"location":"20-faq.html#general-questions","title":"General Questions","text":""},{"location":"20-faq.html#job-failed-errno-26-text-file-busy-tmplava","title":"Job failed: [Errno 26] Text file busy: '/tmp/lava/...'","text":"<p>This is the result of an S3 race condition. Essentially it's a lava internal error that should not happen.</p> <p>The PAYLOAD_SETTLING_TIME can help control this.</p>"},{"location":"20-faq.html#no-payload-files-downloaded-from-s3","title":"No payload files downloaded from S3","text":"<p>The payload prefix specified does not refer to any objects in S3. It generally means the payload value in the job specification is incorrect. Note that this value is always relative to the payloads area in S3, not absolute.</p>"},{"location":"20-faq.html#recursive-download-not-supported","title":"Recursive download not supported","text":"<p>This error can occur for multiple job types that involve downloading a payload from the payload area in S3.</p> <p>It occurs if the specified payload points to an S3 prefix that has nested folders underneath (in the sense that S3 fakes the notion of folder).</p>"},{"location":"20-faq.html#overlapping-runs-for-the-same-job","title":"Overlapping runs for the same job","text":"<p>When a lava job is dispatched, a run ID (UUID) is assigned at the time of dispatch and a dispatch message is placed onto the job SQS queue for the target worker.</p> <p>The lava worker must complete the job, successfully or otherwise, before the SQS queue message visibility timeout expires, otherwise the job will be re-queued by SQS for another run. The re-run will have the same run configuration, including run ID.</p> <p>If multiple instances of a job run are active at the same time, it is likely that the job is running longer than the SQS worker queue visibility timeout.</p> <p>Options in this situation are (in order of preference):</p> <ol> <li> <p>Redesign the job to run more efficiently so it completes within the message     visibility period.</p> </li> <li> <p>Use the <code>timeout</code> parameter on the job where available so that lava will     kill the job before the message visibility timeout expires.</p> </li> <li> <p>Set the max_tries field in the job     specification.</p> </li> <li> <p>Increase the visibility timeout on the worker SQS queue. This should be done     by updating the CloudFormation stack for the worker. SQS limits the     visibility timeout to a maximum of 12 hours.</p> </li> </ol>"},{"location":"20-faq.html#what-timezone-applies-when-a-job-is-run","title":"What timezone applies when a job is run?","text":"<p>Lava controls the timezone when a job is dispatched as it's obviously important that a job is run at the correct time.</p> <p>Lava does not control the timezone in which a job executes and lava makes no promises about the timezone of the execution environment.</p> <p>If a job runs natively on a lava worker, it will generally inherit the timezone of the host, which can be anything. If it runs as a docker job it may have a different timezone to the host.</p> <p>The bottom line is that jobs need to either:</p> <ol> <li> <p>Avoid any timezone dependency in the execution environment (e.g. by always     working with UTC or timezone aware timestamps); or</p> </li> <li> <p>Explicitly control the execution timezone. For cmd,     docker, exe and pkg     jobs, this can be done by specifying the <code>TZ</code> environment variable in the     <code>env</code> parameter of the job specification. e.g.</p> </li> </ol> <pre><code>{\n    \"type\": \"exe\",\n    \"payload\": \"...\",\n    \"env\": {\n        \"TZ\": \"Australia/Darwin\"\n    }\n}\n</code></pre> <p>Danger signs in a job payload are usage of the date(1) command or use of <code>datetime.datetime.now()</code> in a Python script.</p>"},{"location":"20-faq.html#why-are-timezones-all-over-the-place-in-event-records","title":"Why are timezones all over the place in event records?","text":"<p>Records in the events table contain a bunch of different timestamps that may appear to have a random selection of timezones associated with them.</p> <p>Timezone aware timestamps are not all rendered into UTC for two reasons:</p> <ol> <li> <p>The timezone information can sometimes be useful when investigating issues.</p> </li> <li> <p>There is no particular need to convert to UTC as the timestamp is     unambiguous.</p> </li> </ol> <p>This why they have the timezones that they do.</p> Field Timezone Explanation ts_event This timezone aware timestamp is generated by the job execution environment. For most jobs, the timezone is that of the host running the lava worker. For docker jobs, it is the timezone of the container, which may be different from the host. ts_dispatch This timezone aware timestamp is generated by the job dispatch environment. For scheduled jobs, this is the timezone of the host running the dispatch command via cron(8) and is unrelated to the timezone associated with the job schedule. For jobs dispatched by the dispatch helper or s3trigger, the dispatch event comes from an AWS lambda function and the associated timezone is UTC. For directly dispatched jobs, the timezone will be whatever timezone the dispatching entity happens to have. tu_event This is the timezone naive equivalent of <code>ts_event</code> in UTC. It's present purely as a convenience. ttl This is a lava internal field. Its the UNIX epoch timestamp used by DynamoDB to automatically expire and remove old event records. Don't touch it or bother trying to interpret it."},{"location":"20-faq.html#db_from_s3","title":"db_from_s3","text":""},{"location":"20-faq.html#error-invalid-byte-sequence-for-encoding-utf8","title":"Error: invalid byte sequence for encoding \u201cUTF8\u201d","text":"<p>This typically means either:</p> <ol> <li> <p>The source data contains non-UTF8 characters; or</p> </li> <li> <p>The source is actually a gzip file but the <code>GZIP</code> option has not been     specified in the job parameters.</p> </li> </ol>"},{"location":"20-faq.html#error-http-412-the-file-has-been-modified-since-the-import-call-started","title":"Error: HTTP 412. The file has been modified since the import call started","text":"<p>This is probably the result of an S3 race condition. It shouldn't happen. Report it if it does.</p>"},{"location":"20-faq.html#db_to_s3","title":"db_to_s3","text":""},{"location":"20-faq.html#there-is-no-db_to_s3-job-type-what-should-i-do-instead","title":"There is no db_to_s3 job type. What should I do instead?","text":"<p>Yes. Sorry about that.</p> <p>There are a number of alternatives.</p> <p>For Redshift, use redshift_unload which is optimised to extract data from Redshift efficiently.</p> <p>For other database types, the lava-sql utility can cover many use cases. It can extract data from all supported database types into CSV, JSONL and Parquet format. It can be called directly from within the payload of exe, pkg and docker jobs.</p> <p>It can also be used directly in a cmd job as a DIY version of a <code>db_to_s3</code> job. This is what such a job might look like in the YAML format supported by the lava job framework:</p> <pre><code>description: Ersatz db_to_s3 type of job\n\ntype: cmd\n\njob_id: &lt;{ prefix.job }&gt;/ersatz-db-to-s3\n\nworker: &lt;{ worker.main }&gt;\nenabled: true\nowner: &lt;{ owner }&gt;\n\npayload: &gt;-\n  /bin/sh -c\n  \"\n  echo 'SELECT a, b, c FROM my_schema.my_table' |\n  lava-sql\n  --format csv\n  --header\n  --conn-id '&lt;{ db.conn_id }&gt;'\n  --output '{{ realm.s3_temp }}/{{ job.job_id }}/output.csv'\n  -\n  \"\n\nevent_log: Output file is {{ realm.s3_temp }}/{{ job.job_id }}/output.csv\n</code></pre> <p>Note the final <code>-</code> argument to lava-sql tells it to read the query from stdin.</p> <p>We can also load the query from S3 instead of embedding it in the job, thus:</p> <pre><code>description: Ersatz db_to_s3 type of job\n\ntype: cmd\n\njob_id: &lt;{ prefix.job }&gt;/ersatz-db-to-s3\n\nworker: &lt;{ worker.main }&gt;\nenabled: true\nowner: &lt;{ owner }&gt;\n\npayload: &gt;-\n  /bin/sh -c\n  \"\n  lava-sql\n  --format csv\n  --header\n  --conn-id '&lt;{ db.conn_id }&gt;'\n  --output '{{ realm.s3_temp }}/{{ job.job_id }}/output.csv'\n  '{{ realm.s3_payloads }}/&lt;{ prefix.payload }&gt;/xyz.rsc/query.sql'\n  \"\n\nevent_log: Output file is {{ realm.s3_temp }}/{{ job.job_id }}/output.csv\n</code></pre>"},{"location":"20-faq.html#docker","title":"docker","text":""},{"location":"20-faq.html#you-must-specify-a-region","title":"You must specify a region","text":"<p>If a docker job requires access to AWS resources (e.g. S3, the lava connections manager etc.) it will be using the boto3 Python module to do so. This requires the AWS region in which it runs to be specified. Rather than hard-wire that into the container, the easiest way to set this is by adding the following environment variable to the parameters in the job specification:</p> <pre><code>{\n    \"parameters\": {\n        \"env\": {\n         \"AWS_DEFAULT_REGION\": \"ap-southeast-2\"\n        }\n    }\n}\n</code></pre> <p>Note that the IAM role for the worker should provide the authentication requirements for docker containers running on that worker.</p>"},{"location":"20-faq.html#exe","title":"exe","text":""},{"location":"20-faq.html#error-job-failed-errno-8-exec-format-error-tmplava","title":"Error: job ... (...) failed: [Errno 8] Exec format error: '/tmp/lava/...'","text":"<p>This error indicates that the operating system tried to run a script as a binary when it isn't. It generally means that the hashbang line is missing from the beginning of a script file indicating what interpreter to use. For a Python script it will look exactly like this:</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>This error can also occur when an executable has been edited on a DOS system and has acquired DOS CRLF line endings instead of UNIX LF line endings.</p>"},{"location":"20-faq.html#lavasched","title":"lavasched","text":""},{"location":"20-faq.html#failing-lavasched-jobs","title":"Failing lavasched jobs","text":"<p>This is the result of a bad crontab spec in a newly added job.</p> <p>Prior to version 5.0.0 (Cotopaxi), it could be tricky to localise the bad job specification. Since Cotopaxi, output from lavasched jobs now includes a context diff of the new crontab vs the old one to simplify tracking of changes and problem diagnosis.</p>"},{"location":"20-faq.html#pkg","title":"pkg","text":""},{"location":"20-faq.html#error-job-failed-errno-8-exec-format-error-tmplava_1","title":"Error: job ... (...) failed: [Errno 8] Exec format error: '/tmp/lava/...'","text":"<p>This error indicates that the operating system tried to run a script as a binary when it isn't. It generally means that the hashbang line is missing from the beginning of a script file indicating what interpreter to use. For a Python script it will look exactly like this:</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>This error can also occur when an executable has been edited on a DOS system and has acquired DOS CRLF line endings instead of UNIX LF line endings.</p>"},{"location":"20-faq.html#in-what-timezone-will-a-pkg-job-run","title":"In what timezone will a pkg job run?","text":"<p>By default, all jobs run in the timezone setting of the worker itself. This is unrelated to the dispatch timezone.</p> <p>For a pkg job, the timezone for the job run can be specified by adding the <code>TZ</code> environment variable to the job specification, thus:</p> <pre><code>{\n    \"type\": \"pkg\",\n    \"payload\": \"...\",\n    \"env\": {\n        \"TZ\": \"Australia/Darwin\"\n    }\n}\n</code></pre>"},{"location":"20-faq.html#sql","title":"sql","text":""},{"location":"20-faq.html#error-need-to-escape-but-no-escapechar-set","title":"Error: need to escape, but no escapechar set","text":"<p>When lava collects the output of a query from an sql job, it uses the standard Python CSV writer to format the data for output. If it detects that the data requires an escape character to be specified but this has not been done in the job specification, this error will result.</p>"},{"location":"20-faq.html#connectors-faq","title":"Connectors FAQ","text":"<p>This section covers questions related to connectors in general as well as those specific to particular job types.</p>"},{"location":"20-faq.html#general-questions_1","title":"General Questions","text":""},{"location":"20-faq.html#why-is-psycopg2-not-supported","title":"Why is psycopg2 not supported?","text":"<p>In a word, licensing. Psycopg2 is LGPL.</p>"},{"location":"20-faq.html#why-is-pygresql-not-supported","title":"Why is pygresql not supported?","text":"<p>As of v5.0.0 (Cotopaxi), pygresql can be specified as the driver to use for Postgres family connectors using the <code>subtype</code> field in the connection specification.</p> <p>It is strongly recommended to stick with the default of pg8000 for the db_from_s3 and redshift_unload job types.</p>"},{"location":"20-faq.html#why-would-i-use-pygresql-instead-of-pg8000","title":"Why would I use pygresql instead of pg8000?","text":"<p>Insert performance of pg8000 is poor. For Python based jobs inserting a lot of data, pygresql should give better performance.</p>"},{"location":"20-faq.html#is-sqlalchemy-supported","title":"Is SQLAlchemy supported?","text":"<p>As of v5.0.0 (Cotopaxi), the SQL based connectors provide native support for SQLAlchemy. An SQLAlchemy engine can be created using a lava connector to manage the underlying connection process.</p>"},{"location":"20-faq.html#ssh-scp-sftp","title":"ssh, scp, sftp","text":""},{"location":"20-faq.html#authentication-failures","title":"Authentication Failures","text":"<p>A possible cause of authentication failures is the default behaviour of ssh, scp and sftp in situations where it cannot verify the fingerprint of the remote host's public key. When this occurs, the client will drop into interactive mode to ask for confirmation to accept the key, like so:</p> <pre><code>The authenticity of host 'sftp.xyzzy.com (192.219.1.1)' can't be established.\nRSA key fingerprint is SHA256:TXYwAhoSEfm6Me6RtFHJRUEGL9lTuHqySI6GyxVe//M.\nRSA key fingerprint is MD5:52:b4:70:1d:c1:0e:aa:4d:32:8e:f8:7a:cb:f9:b8:7e.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>Lava cannot handle this.</p> <p>To avoid this problem, add <code>-o StrictHostKeyChecking=no</code> to the arguments when invoking the connector script provided by lava.</p>"},{"location":"20-faq.html#installation-and-operation-faq","title":"Installation and Operation FAQ","text":""},{"location":"20-faq.html#the-lava-worker","title":"The Lava Worker","text":""},{"location":"20-faq.html#modulenotfounderror-no-module-named-psutil","title":"ModuleNotFoundError: No module named 'psutil'","text":"<p>This error appears to be unique to the Amazon Linux 1 AMI due to a bug in the Python configuration on that AMI. Basically, binary modules, such as <code>psutil</code>, are getting installed in a <code>lib64</code> directory by pip instead of <code>lib</code> and Python is not looking in <code>lib64</code>.</p> <p>The fix is:</p> <p><pre><code>unset PYTHON_INSTALL_LAYOUT\npython3 -m pip install psutil --upgrade\n</code></pre> Note that this problem can also affect other modules including <code>jinja2</code> and <code>cx_Oracle</code>. The fix is the same in each case.</p>"},{"location":"20-faq.html#the-lava-docker-images","title":"The Lava Docker Images","text":""},{"location":"20-faq.html#docker-build-fails-error-internal-load-metadata-for-dockerio","title":"Docker build fails : ERROR [internal] load metadata for docker.io...","text":"<p>There is a bug in some versions of docker buildkit which seems to occur when connecting to docker hub via a proxy. If this message occurs, try building from location that does not require a proxy connection.</p> <p>Info</p> <p>As of v8.1.0 (K\u012blauea), lava docker images must be built with buildkit enabled.</p>"},{"location":"20-faq.html#the-lava-framework-faq","title":"The Lava Framework FAQ","text":""},{"location":"20-faq.html#general-questions_2","title":"General Questions","text":""},{"location":"20-faq.html#retrospectively-adding-jupyter-notebook-support","title":"Retrospectively adding Jupyter notebook support","text":"<p>If Jupyter support was not requested when the project was originally created using cookiecutter, it can be enabled afterwards using the following process:</p> <pre><code># Got to project root\ncd  &lt;PROJECT-ROOT&gt;\n\n# Add jupyter to requirements file\necho jupyter &gt;&gt; etc/requirements.txt\n\n# Install jupyter support\nmake init\n</code></pre>"},{"location":"20-faq.html#retrospectively-adding-lava-libraries","title":"Retrospectively adding lava libraries","text":"<p>If the lava libraries were not requested when the project was originally created using cookiecutter, they can be added afterwards using the following process:</p> <pre><code># Got to project root\ncd  &lt;PROJECT-ROOT&gt;\n\n# Add lava to requirements file\necho jinlava &gt;&gt; etc/requirements.txt\n\n# Install\nmake init\n</code></pre>"},{"location":"20-faq.html#jinja-rendering-faq","title":"Jinja Rendering FAQ","text":""},{"location":"20-faq.html#does-lava-support-recursive-rendering","title":"Does lava support recursive rendering?","text":"<p>No.</p> <p>For any job type that supports Jinja rendering, lava performs only a single rendering pass. So, for example, it is not possible to include Jinja syntax in a <code>vars</code> variable that references an element in <code>globals</code> as this would require two rendering passes to first inject the global into the var and then render the Jinja content of the var.</p> <p>You are in a maze of twisty little passages, all alike.</p>"},{"location":"21-release-notes.html","title":"Release Notes","text":"<p>Note</p> <p>The historical release notes have been purged, because, well, who cares? The headers have been retained for old times sake.</p>"},{"location":"21-release-notes.html#warnings","title":"Warnings","text":"<p>The following changes will occur in the next major release after Version 8.</p> <ul> <li> <p>The legacy <code>main</code> parameter will be removed from the pkg   job type. Use <code>command</code> instead.</p> </li> <li> <p>The legacy <code>key</code> parameter will be removed from the   redshift_unload job type. Use <code>prefix</code> instead.</p> </li> <li> <p>The lava worker will perform much more aggressive validation of DynamoDB   entries (jobs, actions, connections, etc.) as per the documentation. For   example, jobs that would previously have run with a malformed action   specification will be rejected prior to running.   See The Lava Schema Utility.</p> </li> <li> <p>Support for Python 3.9 will end. Seriously, just stop using it.</p> </li> </ul>"},{"location":"21-release-notes.html#version-8","title":"Version 8","text":""},{"location":"21-release-notes.html#version-820-kilauea","title":"Version 8.2.0 (K\u012blauea)","text":"<p>This version is functionally identical to v8.1, insofar as the main lava code is concerned, hence the appellation has been retained. A number of changes have been made to elements of packaging, deployment and documentation.</p> <p>Changes are:</p> <ul> <li> <p>Added the lava-new utility. This is the preferred way     of creating a new lava-job-framework project and     replaces the old cookiecutter bundle approach. The latter is still available     but is now deprecated.</p> </li> <li> <p>Some slight reordering and updating of questions has been done when creating     a new lava-job-framework project.</p> </li> <li> <p>A bunch of packaging stuff has been updated for the open source release.</p> </li> <li> <p>The user guide has been converted to     mkdocs material     and Sphinx has been replaced by     mkdocstrings     for API documentation. As a result, the other publication formats for the     user guide (DOCX, EPUB etc.) have been discontinued.</p> </li> <li> <p>A number of changes have been made to the management of the     lava docker images, none of which impact image     functionality. Probably.</p> <ul> <li> <p>The lava docker images are now created without attestation manifests. If     you don't know that means, you won't miss them. If you do know what that     means, you may wonder why they were there in the first place. \u00af\\(\u30c4)/\u00af</p> </li> <li> <p>Previous versions of lava used AWS ECR as a private registry for     publishing images internally. This is still possible. The default     registry for publication is now GitHub Container Registry (ghcr.io).</p> </li> <li> <p>The naming for publicly available lava docker images is:   <code>ghcr.io:jin-gizmo/lava/&lt;PLATFORM&gt;/&lt;TYPE&gt;</code>.       e.g:   <code>ghcr.io/jin-gizmo/lava/amzn2023/base</code> </p> </li> <li> <p>Images published privately to AWS ECR retain the following format for     compatibility with lava realm IAM structures:   <code>&lt;ECR&gt;:dist/lava/&lt;PLATFORM&gt;/&lt;TYPE&gt;</code>      e.g.   <code>123456789123.dkr.ecr.ap-southeast-2.amazonaws.com/dist/lava/amzn2023/base</code> </p> </li> <li> <p>The lava job framework defaults have been     updated to reference the public images, by default. This change will not     affect existing projects and can be altered on a per project basis, as     needed.</p> </li> <li> <p>The Rocky Linux lava image (<code>rocky9</code>) has been discontinued. As if      anyone will notice. The build code has moved into a legacy area in the     repo on the off-chance it is required but it is not maintained.</p> </li> </ul> </li> <li> <p>Lava has changed from semantic versioning to     PEP 440 versioning. You would have to     be doing something pretty unusual to notice the difference for main-line     releases. The change was made to simplify working with PyPI. I promise lava     will never have a version number like <code>1.0b2.post345.dev456</code>, although the     techno-masochists among you will be aquiver with the new-found possibility.     The semantic versioning support code has been left in (and repaired), just     in case, but lava itself no longer uses it.</p> </li> </ul>"},{"location":"21-release-notes.html#version-81-kilauea","title":"Version 8.1 (K\u012blauea)","text":""},{"location":"21-release-notes.html#version-80-incahuasi","title":"Version 8.0 (Incahuasi)","text":""},{"location":"21-release-notes.html#version-7","title":"Version 7","text":""},{"location":"21-release-notes.html#version-71-pichincha","title":"Version 7.1 (Pichincha)","text":""},{"location":"21-release-notes.html#version-70-tronador","title":"Version 7.0 (Tronador)","text":""},{"location":"21-release-notes.html#version-6","title":"Version 6","text":""},{"location":"21-release-notes.html#version-63-chimborazo","title":"Version 6.3 (Chimborazo)","text":""},{"location":"21-release-notes.html#version-62-reventador","title":"Version 6.2 (Reventador)","text":""},{"location":"21-release-notes.html#version-61-volcan-pinta","title":"Version 6.1 (Volc\u00e1n Pinta)","text":""},{"location":"21-release-notes.html#version-60-la-cumbre","title":"Version 6.0 (La Cumbre)","text":""},{"location":"21-release-notes.html#version-5","title":"Version 5","text":""},{"location":"21-release-notes.html#version-51-tungurahua","title":"Version 5.1 (Tungurahua)","text":""},{"location":"21-release-notes.html#version-50-cotopaxi","title":"Version 5.0 (Cotopaxi)","text":""},{"location":"21-release-notes.html#version-4","title":"Version 4","text":""},{"location":"21-release-notes.html#version-43-volcan-wolf","title":"Version 4.3 (Volc\u00e1n Wolf)","text":""},{"location":"21-release-notes.html#version-42-fernandina","title":"Version 4.2 (Fernandina)","text":""},{"location":"21-release-notes.html#version-41-sierra-negra","title":"Version 4.1 (Sierra Negra)","text":""},{"location":"22-credits.html","title":"Credits","text":"<p>Lava was written by Murray Andrews, Chris Donoghue and Alex Boul.</p> <p>Many, many people have contributed to the expansion of lava over recent years. Key contributors (in random order) include:</p> <ul> <li>Siok Yee</li> <li>Lance</li> <li>Harry</li> <li>Julianto</li> <li>Aaron</li> <li>Zack</li> <li>Shawn</li> <li>Andie.</li> </ul>"},{"location":"23-known-issues.html","title":"Known Issues","text":""},{"location":"23-known-issues.html#pg8000-type-error-with-sqlalchemy","title":"PG8000 type error with SQLAlchemy","text":"<p>PG8000 had a bug in its implementation that the SQLAlchemy driver patched over. A recent pg8000 release introduced a patch for this issue that broke the SQLAlchemy workaround.</p> <p>See the SQLAlchemy issue on GitHub.</p> <p>The workaround is to change this:</p> <pre><code>from lava.connection import get_sqlalchemy_engine\n\nprint('Connecting with sqlalchemy')\ne = get_sqlalchemy_engine(conn_id, realm)\n\nwith e.connect() as conn:\n    for row in conn.execute('SELECT whatever FROM wherever'):\n        print(row)\n</code></pre> <p>to this:</p> <pre><code>from lava.connection import get_sqlalchemy_engine\n\nprint('Connecting with sqlalchemy')\ne = get_sqlalchemy_engine(conn_id, realm)\n\n# This is the workaround\ne.dialect.description_encoding = None\n\nwith e.connect() as conn:\n    for row in conn.execute('SELECT whatever FROM wherever'):\n        print(row)\n</code></pre> <p>The workaround is benign for other driver types.</p> <p>If a code change is not possible, try switching from a pg8000 connector to a PyGreSQL based connector. This should work fine without a code change.</p>"},{"location":"23-known-issues.html#cant-load-plugin-sqlalchemydialectspostgresqlpygresql","title":"Can't load plugin: sqlalchemy.dialects:postgresql.pygresql","text":"<p>SQLAlchemy support is currently not working with the PyGreSQL driver. Use PG8000 instead.</p>"}]}